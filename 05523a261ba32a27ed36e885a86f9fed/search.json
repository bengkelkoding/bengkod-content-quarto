[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Associate Data Science",
    "section": "",
    "text": "Pengantar\nSelamat datang dalam modul pembelajaran online tentang Data Science. Modul ini dirancang untuk memberi Anda pemahaman yang komprehensif tentang konsep, metode, dan aplikasi yang terkait dengan Data Science. Dalam era digital yang terus berkembang, Data Science telah menjadi elemen kunci dalam pengambilan keputusan yang informasional dan cerdas di berbagai bidang, mulai dari bisnis hingga penelitian ilmiah.\nDiharapkan dengan mengikuti modul ini, Anda akan dapat memahami konsep dasar Data Science, menguasai metode analisis data, dan mengembangkan keterampilan pemrograman Python untuk mengolah data dan membuat model machine learning."
  },
  {
    "objectID": "konsep.html",
    "href": "konsep.html",
    "title": "Konsep",
    "section": "",
    "text": "Pada kursus Data Science berisi tentang modul-modul, sebagai berikut:\n\nData Science Tools\nTelaah Data\nLIbraries for Data Preprocessing\nValidasi dan Visualisasi Data\nMenentukan Objek atau Memilih Data\nCleaning Data\nTransformasi Data\nPemodelan Data Science\nEvaluasi Data\nDeployment"
  },
  {
    "objectID": "modul_1.html",
    "href": "modul_1.html",
    "title": "1  Data Science Tools",
    "section": "",
    "text": "2 Konsep Data Science\nData science adalah bidang yang berkaitan dengan ekstraksi pengetahuan dan wawasan dari data. Data science menggabungkan konsep dan metode dari berbagai disiplin ilmu seperti statistika, matematika, komputer, dan pemodelan untuk menganalisis dan menginterpretasikan data secara sistematis.\nTahukah anda bahwa 70% pengembangan software gagal? Data science adalah bagian dari pengembangan software yang mempunyai tingkat kegagalan paling tinggi di angka 80% tingkat kegagalan.\n\nBanyak sekali masalah yang dihadapi di dalam proses pengembangan software. Mulai dari perubahan requirement, maintenance yang kurang, hingga dokumentasi yang tidak lengkap. Oleh karena itu, pemahaman terhadap pengembangan model sangatlah penting, agar semua usaha kita untuk membuat sebuah aplikasi tidak sia sia, dan dapat membantu orang banyak."
  },
  {
    "objectID": "modul_1.html#python-versi-3.10.10",
    "href": "modul_1.html#python-versi-3.10.10",
    "title": "1  Data Science Tools",
    "section": "1.1 Python versi 3.10.10",
    "text": "1.1 Python versi 3.10.10\n\nBuka browser, kunjungi https://www.python.org/downloads/windows/ , cari bagian Python 3.10.10 - Feb 8, 2023, lalu pilih Download Windows Installer.\nJalankan file yang di download, tunggu sampai proses installing selesai.\nBuka Command Prompt, lalu jalankan command “python –version”\n\nJika hasil dari command tersebut adalah “3.10.10” maka instalasi python sudah selesai.\n\n\n\nGambar3. Python di cmd"
  },
  {
    "objectID": "modul_1.html#development-environment",
    "href": "modul_1.html#development-environment",
    "title": "1  Data Science Tools",
    "section": "1.2 Development Environment",
    "text": "1.2 Development Environment\nEnvironment adalah lingkungan yang kita tinggali selama proses koding. Di course ini kita bisa menggunakan salah satu dari tiga aplikasi, yaitu VSCode, Jupyter dan Google Colab. Cukup pilih salah satu dari ketiga opsi Kami merekomendasikan untuk menggunakan Jupyter notebook. Untuk alasannya, silahkan simak slide selanjutnya\n\n1.2.1 VSCode Notebook\nVSCode adalah sebuah code editor multifungsi yang cukup ringan dan handal. Kelebihan dari VSCode adalah fleksibilitas dari VSCode. Anda dapat mengkustomisasi tema, menambah ekstensi, dan mengubah semua settingan yang ada di VSCode. Anda mempunyai kendali penuh. Kekurangan dari VSCode adalah terkadang settingan VSCode terlalu kompleks untuk digunakan.\nBerikut langkah-langkah untuk menginstalasi VScode Notebook :\n\nBuka browser, kunjungi https://code.visualstudio.com/download# pilih salah satu link yang berada di System Installer.\n\nJalankan file yang di download, tunggu sampai proses installing selesai.\n\nSelanjutnya kita akan menginstall beberapa ekstensi.\n\nBuka VSCode, pergi ke bagian Extensions, (ctrl+shift+x) lalu ketik “Python”, lalu install ekstensi yang muncul di paling atas.\n\n\nMasih di tab Extensions, ketik “Jupyter”, lalu instal ekstensi yang muncul di paling atas\n\n\nReload VSCode\n\n\n\n1.2.2 Jupyter Notebook\nJupyter Notebook adalah aplikasi yang digunakan untuk memproses dan menampilkan teks, kode, dan data secara bersamaan. Jupyter sangat cocok untuk data science karena Jupyter sangat ringan, mudah di install, dan mudah untuk digunakan. Oleh karena itu, kita merekomendasikan anda untuk menggunakan Jupyter Notebook untuk pelatihan ini.\nBerikut langkah-langkah untuk menginstalasi Jupyter Notebook :\n- Buka Windows start menu, ketik “cmd”\n- Klik kanan pada item command prompt, lalu pilih run as administrator\n- Di dalam command prompt, ketik “pip install jupyterlab”\n- Tunggu sampai proses selesai\n- Ketik “jupyter-lab” untuk menjalankan aplikasi jupyter"
  },
  {
    "objectID": "modul_1.html#install-libraries",
    "href": "modul_1.html#install-libraries",
    "title": "1  Data Science Tools",
    "section": "1.3 Install Libraries",
    "text": "1.3 Install Libraries\n\nBuka terminal (ctrl+shift+~) lalu ketik “pip install [package name]” Contoh : “pip install pandas”, dimana pandas adalah sebuah nama dari package Untuk membuka terminal di Jupyter Notebook, buka launcher baru (ctrl+shift+L) lalu pilih terminal.\n\nPackage name diisi dengan list dibawah ini\n\nnumpy\nscipy\npandas\nmatplotlib\nseaborn\nscikit"
  },
  {
    "objectID": "modul_1.html#workflow",
    "href": "modul_1.html#workflow",
    "title": "1  Data Science Tools",
    "section": "2.1 Workflow",
    "text": "2.1 Workflow\nDi proses pekerjaan pengolahan data, Data Scientist bekerja untuk mengembangkan model terbaik dari data untuk menjawab permasalahan bisnis. Data Scientist mengambil data dengan bantuan Data Engineer dan DevOps Engineer, lalu menganalisa data tersebut. Hasil analisa data dikembalikan ke mereka atau dipresentasikan ke anggota lain.\n\n\n\nGambar1. Contoh workflow data scientist"
  },
  {
    "objectID": "modul_1.html#mengapa-data-scientist-dibutuhkan",
    "href": "modul_1.html#mengapa-data-scientist-dibutuhkan",
    "title": "1  Data Science Tools",
    "section": "2.2 Mengapa Data Scientist Dibutuhkan?",
    "text": "2.2 Mengapa Data Scientist Dibutuhkan?\nAplikasi Modern mempunyai skala yang jauh lebih besar dari aplikasi-aplikasi sebelumnya. Dalam satu waktu, jumlah user bisa mencapai ratusan ribu, bahkan sampai jutaan. Masing-masing user menghasilkan beberapa request ke server untuk mengambil data, dan menghasilkan data tersendiri. Hasilnya, data yang dibuat, disimpan, dan diproses sangatlah besar. Fenomena ini dinamakan: BIG DATA"
  },
  {
    "objectID": "modul_1.html#apa-yang-dimaksud-big-data",
    "href": "modul_1.html#apa-yang-dimaksud-big-data",
    "title": "1  Data Science Tools",
    "section": "2.3 Apa Yang Dimaksud Big Data?",
    "text": "2.3 Apa Yang Dimaksud Big Data?\nBig data mempunyai karakteristik yang dinamakan 5V"
  },
  {
    "objectID": "modul_1.html#tahap-pengembangan-sistem-ai-berbasis-big-data",
    "href": "modul_1.html#tahap-pengembangan-sistem-ai-berbasis-big-data",
    "title": "1  Data Science Tools",
    "section": "3.1 Tahap Pengembangan Sistem AI Berbasis Big Data:",
    "text": "3.1 Tahap Pengembangan Sistem AI Berbasis Big Data:\n\nPelatihan, Adalah tahap dimana kita membangun sebuah model dari nol. Proses pelatihan meliputi pemilihan dataset, pemilihan algoritma, penyetelan parameter, testing, dan lain lain\n\nPenggunaan, Ketika model sudah dibuat dan siap untuk digunakan, model akan di deploy di internet agar dapat digunakan oleh semua orang.\n\n\n3.1.1 Pelatihan\n\n\n\nGambar7. Flow dari proses training model machine learning\n\n\n\n\n3.1.2 Penggunaan\n\n\n\nGambar8. Flow dari proses penggunaan model machine learning"
  },
  {
    "objectID": "modul_1.html#software-development-life-cycle-sdlc",
    "href": "modul_1.html#software-development-life-cycle-sdlc",
    "title": "1  Data Science Tools",
    "section": "3.2 Software Development Life Cycle (SDLC)",
    "text": "3.2 Software Development Life Cycle (SDLC)\nMetode SDLC (Software Development Life Cycle) adalah proses pembuatan dan pengubahan sistem serta model dan metodologi yang digunakan untuk mengembangkan sistem rekayasa perangkat lunak.\n\nProses logika yang digunakan oleh seorang analis sistem untuk mengembangkan sebuah sistem informasi yang melibatkan requirements, validation, training dan pemilik sistem proses yang memproduksi sebuah software dengan kualitas setinggi-tingginya tetapi dengan biaya yang serendah-rendahnya (Stackify)"
  },
  {
    "objectID": "modul_1.html#jenis-jenis-sdlc",
    "href": "modul_1.html#jenis-jenis-sdlc",
    "title": "1  Data Science Tools",
    "section": "3.3 Jenis-Jenis SDLC",
    "text": "3.3 Jenis-Jenis SDLC\nBerikut adalah beberapa jenis dari SDLC:\n\n\n\nGambar10. Jenis jenis SDLC\n\n\n\n3.3.1 Agile\nMetode pengembangan Agile adalah pendekatan kolaboratif dan iteratif dalam pengembangan perangkat lunak. Dalam metode ini, pengembangan dilakukan dalam iterasi kecil yang disebut “sprints”\nKelebihan\n\nFleksibilitas tinggi\nKolaborasi erat antar tim\nAplikasi di-update secara rutin\nFokus kepada kebutuhan pengguna\n\nKekurangan\n\nSulitnya merencanakan anggaran dan timeline\nMemerlukan tingkat disiplin yang tinggi dan pengawasan konstan\nKurang efektif untuk project skala besar\n\n\n\n3.3.2 Waterfall\nMetode pengembangan Waterfall adalah pendekatan linier dan berurutan dalam pengembangan perangkat lunak. Dalam metode ini, setiap fase pengembangan (analisis kebutuhan, desain, implementasi, pengujian, dan pemeliharaan) dilakukan secara berurutan, di mana setiap fase harus selesai sebelum melanjutkan ke fase berikutnya.\nKelebihan\n\nStruktur yang jelas\n\nDokumentasi lengkap dan rapi\n\nCocok untuk proyek yang kebutuhannya tidak berubah-ubah\n\nKekurangan\n\nKurang fleksibel ketika menghadapi kebutuhan yang berubah\n\nKeterbatasan dalam kolaborasi dan feedback\n\nRisiko terlambatnya mendeteksi kesalahan/glitch\n\n\n\n3.3.3 Prototype\nMetode pengembangan prototyping adalah pendekatan dalam pengembangan perangkat lunak di mana prototipe perangkat lunak awal dibuat dan digunakan untuk mengumpulkan umpan balik dari pengguna dan pemangku kepentingan. Prototipe tersebut dapat berupa versi sederhana atau parsial dari perangkat lunak yang akhir.\nKelebihan\n\nFeedback didapatkan lebih awal\n\nKlarifikasi kebutuhan lebih jelas\n\nMeningkatkan kolaborasi tim\n\nKelemahan\n\nMembutuhkan banyak waktu dan sumber daya\n\nPotensi kesalahan di aspek desain dan efisiensi besar\n\nKesulitan dalam timing untuk berhenti mengembangkan prototipe"
  },
  {
    "objectID": "modul_1.html#rapid-application-development-rad",
    "href": "modul_1.html#rapid-application-development-rad",
    "title": "1  Data Science Tools",
    "section": "3.4 Rapid Application Development (RAD)",
    "text": "3.4 Rapid Application Development (RAD)\nMetode pengembangan Rapid Application Development (RAD) adalah pendekatan iteratif dan kolaboratif dalam pengembangan perangkat lunak yang menekankan kecepatan, fleksibilitas, dan keterlibatan pengguna. Dalam metode ini, pengembangan dilakukan dalam siklus pendek yang disebut “iterasi” dan melibatkan partisipasi aktif dari pengguna.\nKelebihan\n\nWaktu pengembangan cepat\n\nKeterlibatan pengguna secara sering dan langsung\n\nFleksibilitas tinggi\n\nTingginya kolaborasi terhadap developer dan pengguna\n\nKelemahan\n\nKetergantungan terhadap pengguna tinggi\n\nSulit untuk diskalakan\n\nKualitas dan efisiensi arsitektur rendah"
  },
  {
    "objectID": "modul_1.html#kesimpulan",
    "href": "modul_1.html#kesimpulan",
    "title": "1  Data Science Tools",
    "section": "3.5 Kesimpulan",
    "text": "3.5 Kesimpulan\nKesimpulannya, Di dunia software development, tidak ada metode yang lebih baik atau buruk, hanya ada pengorbanan atau ‘tradeoff’.\nUntuk melakukan proses development yang baik dan efisien, kita harus memahami kebutuhan, kondisi, dan sumber daya yang kita punya. Pemahaman yang baik memungkinkan kita untuk memilih tradeoff mana yang akan diambil. Setelah itu kita menyesuaikan tradeoff yang kita pilih dengan jenis SDLC yang sudah ada."
  },
  {
    "objectID": "modul_2.html",
    "href": "modul_2.html",
    "title": "2  Telaah Data",
    "section": "",
    "text": "3 Data Understanding\nDi dunia bisnis, proses data understanding dilakukan setelah problem bisnis didefinisikan. Tujuan utama dari data understanding adalah untuk memberikan gambaran utuh terhadap sebuah data. Setelah Data Understanding, kita dapat melanjutkan proses persiapan data.\nData mempunyai berbagai jenis, bentuk, ukuran dan nilai. Sebagai Data Scientist, kita harus mengetahui sifat, jenis, asal, dan bentuk dari data yang kita olah.\nProses pengambilan data, juga dikenal sebagai proses ekstraksi data, merujuk pada langkah-langkah yang dilakukan untuk mengumpulkan, mengakses, dan memperoleh data dari sumber yang relevan\nAda beberapa Cara untuk mengambil data, yaitu:"
  },
  {
    "objectID": "modul_2.html#mengapa-kita-perlu-data-understanding",
    "href": "modul_2.html#mengapa-kita-perlu-data-understanding",
    "title": "2  Telaah Data",
    "section": "3.1 Mengapa Kita Perlu Data Understanding",
    "text": "3.1 Mengapa Kita Perlu Data Understanding\nDi bidang machine learning, ada sebuah mantra yang semua data scientists harus ikuti; Garbage in, garbage out. Artinya, jika data yang dimasukkan ke model itu jelek, maka apapun yang terjadi, hasil dari model akan selalu jelek.\nSemua data yang kita dapatkan belum tentu bagus dan bisa kita olah lebih lanjut karena:\n\nMaksud dan tujuan data berbeda\n\nKeadaan data terlalu terpisah, atau terlalu terintegrasi\n\nKekayaan atau value data berbeda beda\n\nKeandalan atau reliability data berbeda beda"
  },
  {
    "objectID": "modul_2.html#data-understanding-documentation",
    "href": "modul_2.html#data-understanding-documentation",
    "title": "2  Telaah Data",
    "section": "3.2 Data Understanding Documentation",
    "text": "3.2 Data Understanding Documentation\nData Understanding Documentation adalah sebuah prosedur yang harus kita ikuti untuk mempelajari data yang akan kita gunakan dan menulis hasil pekerjaan kita secara terstruktur agar dapat dicerna oleh orang lain dengan mudah.\nData Understanding Documentation mempunyai 4 komponen, yaitu:\n\n\n\nGambar2. Komponen Data Understanding"
  },
  {
    "objectID": "modul_2.html#sumber-data",
    "href": "modul_2.html#sumber-data",
    "title": "2  Telaah Data",
    "section": "4.2 Sumber Data",
    "text": "4.2 Sumber Data\n\n4.2.1 Internal\nData internal merupakan data yang sumbernya berasal dari dalam pihak yang dijadikan objek penelitian. Sumber data internal biasanya dihasilkan oleh perusahaan sendiri atau oleh perusahaan lain yang terkait dengan perusahaan tersebut. Contoh data meliputi :\n\nSpreadsheet (excel, csv, json)\n\nDatabase (sql)\n\nTeks (txt, rtf)\n\nMedia (video, audio)\n\n\n\n4.2.2 External\nData eksternal merupakan data yang sumbernya berasal dari luar pihak objek yang diteliti. Sumber data eksernal biasanya didapatkan dari repositori data publik dan halaman web publik. Contoh data meliputi :\n\nWebsite repository data (Kaggle, sklearn)\n\nWeb page domain public (wikipedia, dbdata, data.go.id)\n\nPublic Dataset (worldbank, UNICEF, WHO)"
  },
  {
    "objectID": "modul_2.html#susunan-data",
    "href": "modul_2.html#susunan-data",
    "title": "2  Telaah Data",
    "section": "4.1 Susunan Data",
    "text": "4.1 Susunan Data\nSusunan data, juga dikenal sebagai struktur data, merujuk pada cara data disimpan, diatur, dan dihubungkan satu sama lain dalam suatu sistem komputasi. Ini melibatkan pemilihan format dan metode penyimpanan yang tepat agar data dapat diakses, dimanipulasi, dan dicari dengan efisien. Susunan data yang baik membantu meningkatkan efisiensi operasional, performa sistem, dan kemampuan pengambilan keputusan.\n\n\n\nGambar4. Struktur Data\n\n\nDatum adalah satuan terkecil, sebuah kumpulan teks dan angka. Pada bentuk ini, data tidak mempunyai nilai jual apapun.\nNamun ketika kita menyusun beberapa datum menjadi satu kolom atau baris, kita dapat mendeskripsikan sebuah objek atau makna tertentu.\nKumpulkan banyak data maka kita menciptakan sebuah Informasi, dataset, atau konteks yang mudah dicerna oleh manusia.\nJika kita analisa sebuah dataset, kita akan mendapatkan sebuah hipotesis yang dapat dikonversi menjadi fakta atau knowledge setelah diverifikasi.\nDari sekumpulan knowledge, kita dapat menyusun sebuah decision atau keputusan yang sangat berpengaruh di kehidupan kita sehari-hari\nSusunan Data dibagi menjadi 2, yaitu :\n\n4.1.1 Structured\nData terstruktur adalah jenis data yang memiliki format atau skema yang terorganisir dengan jelas. Setiap kolom dalam data terstruktur memiliki tipe data yang konsisten, dan setiap baris berisi entitas atau objek yang serupa.\nContoh data terstruktur meliputi :\n\nTabular Data\n\nObject Oriented Data\n\nTime-series data\n\n\n\n4.1.2 Unstructured\nData tidak terstruktur merujuk pada jenis data yang tidak memiliki format atau skema yang terorganisir dengan jelas. Data ini seringkali memiliki struktur yang tidak teratur atau tidak terprediksi, sehingga sulit untuk mengklasifikasikan, mengatur, atau memodelkannya secara tradisional.\nContoh data tidak terstruktur meliputi :\n\nVideo atau audio\n\nDokumen HTML\n\nTweet atau postingan sosial media"
  },
  {
    "objectID": "modul_2.html#tipe-data",
    "href": "modul_2.html#tipe-data",
    "title": "2  Telaah Data",
    "section": "4.4 Tipe Data",
    "text": "4.4 Tipe Data\n\n4.4.1 Tipe Data Berdasarkan Sifatnya\n\nData dikotomi, merupakan data yang bersifat pilah satu sama lain, misalnya suku, agama, jenis kelamin, pendidikan, dan lain sebagainya.\n\nData diskrit, merupakan data yang proses pengumpulan datanya dijalankan dengan cara menghitung atau membilang. Seperti, jumlah anak, jumlah penduduk, jumlah kematian dan sebagainya.\n\nData kontinum, merupakan data pengumpulan datanya didapatkan dengan cara mengukur dengan alat ukur yang memakai skala tertentu. Seperti misalnya, Suhu, berat, bakat, kecerdasan, dan lainnya.\n\n\n\n4.4.2 Tipe Data Berdasarkan Cara Pengumpulan\n\nData primer, merupakan data yang didapatkan dari sumber pertama, atau dapat dikatakan pengumpulannya dilakukan sendiri oleh si peneliti secara langsung, seperti hasil wawancara dan hasil pengisian kuesioner (angket).\n\nData sekunder, merupakan data yang didapatkan dari sumber kedua. Menurut Purwanto (2007), data sekunder yaitu data yang dikumpulkan oleh orang atau lembaga lain. Data sekunder adalah data yang digunakan atau diterbitkan oleh organisasi yang bukan pengolahnya (Soeratno dan Arsyad (2003;76).\n\n\n\n4.4.3 Tipe Data Berdasarkan Seri Waktu\n\nData Cross Section, Data cross-section adalah data yang menunjukkan titik waktu tertentu. Contohnya laporan keuangan per 31 Desember 2020, data pelanggan PT. Data Indah bulan mei 2004, dan lain sebagainya.\n\nData Time Series / Berkala, Data berkala adalah data yang datanya menggambarkan sesuatu dari waktu ke waktu atau periode secara historis. Contoh data time series adalah data perkembangan nilai tukar dollar amerika terhadap rupiah tahun 2016 - 2020."
  },
  {
    "objectID": "modul_2.html#web-scraping",
    "href": "modul_2.html#web-scraping",
    "title": "2  Telaah Data",
    "section": "5.1 Web Scraping",
    "text": "5.1 Web Scraping\nWeb scraping artinya mengekstraksi data secara langsung dari suatu halaman web.\nLangkah-langkah umum (contoh detil dapat dilihat di https://realpython.com/beautiful-soup-web-scraper-python/)\n\nTentukan URL halaman web (HTML) yang akan di-scrape.\n\nGunakan fungsi requests.get untuk mengakses URL tersebut. Teks HTML akan tersimpan pada atribut text dari object yang dikembalikan requests.get.\n\nLakukan parsing pada HTML dengan library beautifulsoup untuk memperoleh tabel data yang diinginkan (dengan mengekstraksi elemen-elemen HTML yang relevan).\nContoh pengambilan data melalui web scraping kometar video youtube"
  },
  {
    "objectID": "modul_2.html#application-program-interface-api",
    "href": "modul_2.html#application-program-interface-api",
    "title": "2  Telaah Data",
    "section": "5.2 Application Program Interface (API)",
    "text": "5.2 Application Program Interface (API)\nAPI adalah sebuah alat untuk memudahkan website dan pengguna saling bertukar informasi. API disediakan oleh berbagai website atau perusahaan seperti Kaggle dan Twitter. Biasanya API yang dimiliki perusahaan bersifat private, sehingga dibutuhkan sebuah token khusus untuk mengaksesnya. Namun ada beberapa API publik yang dapat diakses oleh siapa saja seperti PokeAPI.\n\n\n\nGambar6. Contoh Penggunaan API\n\n\n\nContoh melalui API Kaggle (pip install kaggle)(kaggle datasets download -d mauryansshivam/paytm-revenue-users-transactions)\n\nContoh melalui API Portal Data Bandung (http://data.bandung.go.id/index.php/portal/api/1db4a0cc-dc0f-4362-91f1-d0ad1b4bce98)\n\n\n\n    #| code-fold: true\n\n    # Jalankan instalasi library berikut jika belum terinstall\n    # pip install google-api-python-client\n\n    import csv\n    from googleapiclient.discovery import build\n\n    # Set up the API key and YouTube Data API service\n    ## Masukkan API key yang sudah dibuat di Google Cloud Platform\n    ## Contoh : AIzaSyCFjru8dOZbGtZUi_AQu1Cz1MLoANaY22k\n    API_KEY = \"\"\n    youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n\n    def scrape_comments(video_id):\n        # Get the video details\n        video_response = youtube.videos().list(\n            part=\"snippet\",\n            id=video_id\n        ).execute()\n\n        video_title = video_response['items'][0]['snippet']['title']\n        print(\"Scraping comments for video:\", video_title)\n\n        # Get the video comments\n        comments = []\n        next_page_token = None\n\n        while True:\n            comment_response = youtube.commentThreads().list(\n                part=\"snippet\",\n                videoId=video_id,\n                maxResults=100,\n                pageToken=next_page_token\n            ).execute()\n\n            for item in comment_response[\"items\"]:\n                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n                comments.append(comment)\n\n            next_page_token = comment_response.get(\"nextPageToken\")\n\n            if not next_page_token:\n                break\n\n        return comments\n\n    # Test the function\n    ## Masukkan ID video youtube yang ingin diambil komentarnya\n    ## Contoh : 5kAF9QV5nYQ\n    video_id = \"\"\n    comments = scrape_comments(video_id)\n\n    # Save comments to CSV file\n    filename = \"comments.csv\"\n    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Comment\"])\n        writer.writerows(zip(comments))\n\n    print(\"Comments saved to\", filename)"
  },
  {
    "objectID": "modul_2.html#manual-kaggle.com",
    "href": "modul_2.html#manual-kaggle.com",
    "title": "2  Telaah Data",
    "section": "5.3 Manual (Kaggle.com)",
    "text": "5.3 Manual (Kaggle.com)\n\nKita akan mengakses data dari “Goal Dataset – Top 5 European Leagues” dari Kaggle.\n\nKunjungi Kaggle.com dan login (buat akun jika perlu)\n\nLakukan pencarian “goal dataset top 5 European leagues”\n\nKlik “Goal Dataset – Top 5 European Leagues”\n\n\n\n\nGambar7. Contoh Pencarian Data di Kaggle"
  },
  {
    "objectID": "modul_2.html#direct-database",
    "href": "modul_2.html#direct-database",
    "title": "2  Telaah Data",
    "section": "5.4 Direct Database",
    "text": "5.4 Direct Database\nData juga dapat bersumber dari basis data relasional (RDB, Mysql, Postgres) organisasi. Berikut langkah-langkahnya :\n\nInstall sqlalchemy (pip install sqlalchemy), pandas (pip install pandas), dan mysql connector (pip install mysql-connector-python)\n\nImport semua library\n\nTentukan username (root), password (kosong), port (3306), dan nama dari database (emp), lalu masukkan ke variable url.\n\nBuat engine menggunakan function create_engine, masukkan url sebagai parameternya\n\nBuat query yang akan dijalankan di database, masukkan ke variable sql\n\nJalankan query menggunakan function execute\n\nTampung hasil query di dataframe\n\nLakukan operasi data di dataframe"
  },
  {
    "objectID": "modul_3.html#apa-itu-telaah-data",
    "href": "modul_3.html#apa-itu-telaah-data",
    "title": "3  Telaah Data",
    "section": "3.1 Apa itu telaah data?",
    "text": "3.1 Apa itu telaah data?\n\nTelaah data merujuk pada proses pengumpulan, pembersihan, eksplorasi, dan analisis data untuk mendapatkan pemahaman yang lebih yang terkandung dalam data tersebut. Tujuan dari telaah data adalah mendukung pengambilan keputusan berdasarkan bukti yang ditemukan dalam data."
  },
  {
    "objectID": "modul_3.html#mengapa-perlu-telaah-data",
    "href": "modul_3.html#mengapa-perlu-telaah-data",
    "title": "3  Telaah Data",
    "section": "3.2 Mengapa perlu telaah data?",
    "text": "3.2 Mengapa perlu telaah data?\n\n\nData dari masing-masing sumber belum tentu dapat langsung dipakai karena:\n\n\nmaksud dan tujuan data berbeda-beda\nkeadaan asal terpisah-pisah atau justru terintegrasi secara ketat.\ntingkat kekayaan (richness) berbeda-beda\ntingkat keandalan (reliability) berbeda-beda\n\n\nData understanding memberikan gambaran awal tentang:\n\n\nkekuatan data\nkekurangan dan batasan penggunaan data\ntingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan\nketersediaan data (terbuka/tertutup, biaya akses, dsb.)"
  },
  {
    "objectID": "modul_3.html#bentuk-data",
    "href": "modul_3.html#bentuk-data",
    "title": "3  Telaah Data",
    "section": "3.3 Bentuk Data",
    "text": "3.3 Bentuk Data\n\nData dapat memiliki berbagai bentuk, diantaranya:\n\nSpreadsheet(excel, csv, dll)\nDatabase(SQL, Oracle, dll)\nText file(txt, doc, pdf, dll)\nMultimedia documents(audio, video, gambar, dll)"
  },
  {
    "objectID": "modul_3.html#sumber-data",
    "href": "modul_3.html#sumber-data",
    "title": "3  Telaah Data",
    "section": "3.4 Sumber Data",
    "text": "3.4 Sumber Data\n\n\nSumber Internal\n\n\nData yang dihasilkan oleh perusahaan sendiri\nData yang dihasilkan oleh perusahaan lain yang terkait dengan perusahaan\n\n\nSumber Eksternal\n\n\nRepositori data publik\nHalaman web publik"
  },
  {
    "objectID": "modul_3.html#daftar-sumber-data-daring",
    "href": "modul_3.html#daftar-sumber-data-daring",
    "title": "3  Telaah Data",
    "section": "3.5 Daftar Sumber Data Daring",
    "text": "3.5 Daftar Sumber Data Daring\n\nPortal Satu Data Indonesia\nPortal Data Jakarta\nPortal Data Bandung\n\nBadan Pusat Statistik\n\nBadan Informasi Geospasial\n\nUCI Machine Learning repository\nKaggle\n\nWorld Bank Open Data"
  },
  {
    "objectID": "modul_3.html#tipe-data",
    "href": "modul_3.html#tipe-data",
    "title": "3  Telaah Data",
    "section": "3.5 Tipe Data",
    "text": "3.5 Tipe Data\n\n3.5.1 Bedasarkan sifat\n\nData dikotomi, merupakan data yang bersifat pilah satu sama lain, misalnya suku, agama, jenis kelamin, pendidikan, dan lain sebagainya.\nData diskrit, merupakan data yang proses pengumpulan datanya dijalankan dengan cara menghitung atau membilang. Seperti, jumlah anak, jumlah penduduk, jumlah kematian dan sebagainya.\nData kontinu, merupakan data pengumpulan datanya didapatkan dengan cara mengukur dengan alat ukur yang memakai skala tertentu. Seperti misalnya, Suhu, berat, bakat, kecerdasan, dan lainnya.\n\n\n\n3.5.2 Bedasarkan waktu\n\nData Cross Section, adalah data yang menunjukkan titik waktu tertentu. Contohnya laporan keuangan per 31 Desember 2020, data pelanggan PT.Data Indah bulan mei 2004, dan lain sebagainya.\nData Time Series / Berkala, adalah data yang datanya menggambarkan sesuatu dari waktu ke waktu atau periode secara historis. Contoh data time series adalah data perkembangan nilai tukar dollar amerika terhadap rupiah tahun 2016 - 2020"
  },
  {
    "objectID": "modul_3.html#pengambilan-data",
    "href": "modul_3.html#pengambilan-data",
    "title": "3  Telaah Data",
    "section": "3.6 Pengambilan Data",
    "text": "3.6 Pengambilan Data\n\nPengambilan data secara manual.\nPengambilan data melalui API\nPengambilan data melalui akses langsung ke basis data relasional yang ada.\nPengambilan data melalui web scraping."
  },
  {
    "objectID": "modul_3.html#library-pandas",
    "href": "modul_3.html#library-pandas",
    "title": "3  Libraries for Data Preprocessing",
    "section": "3.1 Library Pandas",
    "text": "3.1 Library Pandas\n\n\n\n\n\n\nPandas adalah library yang digunakan untuk melakukan manipulasi, analisis, dan visualisasi data.\nPandas menyediakan struktur data dan fungsi high-level untuk membuat pekerjaan dengan data terstruktur/tabular lebih cepat, mudah, dan ekspresif.\n\n\n3.1.1 Memuat data ke Pandas\n\nNyalakan Jupyter Notebook di folder kerja Anda.\nBuka atau buat baru suatu skrip ipynb (Python 3)\nImport pandas dan numpy. (Pastikan sudah terinstal sebelumnya).\nLoad file CSV yang sudah diunduh sebelumnya (Mengambil Data secara Manual) ke dalam sebuah DataFrame\nGunakan perintah read_csv(dataset)\n\nLink Dataset : titanic\nimport pandas as pd\n# Contoh csv\ndf = pd.read_csv('titanic_dataset.csv')\n\n# Contoh excel\ndf = pd.read_excel('titanic_dataset.xlsx')\n\n# Contoh sqlite\nimport sqlite3\nconn = sqlite3.connect('titanic_dataset.sqlite3')\ndf = pd.read_sql_query(\"SELECT * FROM titanic\", conn)\n\n\n3.1.2 Menampilkan Data\n\nMethod head() dan tail() pada DataFrame membantu kita menampilkan 5 beberapa baris pertama/terakhir dari data yang kita muat. Dengan memberikan argumen pada method tersebut kita juga bisa mengatur jumlah data yang ditampilkan\n# 5 baris pertama\ndf.head()\n\n# 5 baris terakhir\ndf.tail()\n\n# 10 baris pertama\ndf.head(10)\n\n\n3.1.3 Melihat Tipe Data\n\nMethod dtypes pada DataFrame membantu kita menampilkan tipe data dari setiap kolom pada data yang kita muat. Jika terdapat tipe data yang tidak tepat maka sebaiknya dilakukan pengecekan nialai pada kolom tersebut. Jika memang sudah benar, maka kita bisa mengubah tipe data tersebut menjadi tipe data yang tepat.\n# Melihat tipe data\ndf.dtypes\n\n# Mengubah tipe data menjadi int\ndf['nama_kolom'] = df['nama_kolom'].astype('int')\n\n\n3.1.4 Deskripsi statistik data\n\nMethod describe() pada DataFrame membantu kita menampilkan deskripsi statistik dari data yang kita muat. Deskripsi statistik yang ditampilkan adalah deskripsi statistik untuk kolom-kolom dengan tipe data numerik. Jika terdapat kolom dengan tipe data selain numerik, maka deskripsi statistik tersebut tidak akan ditampilkan. Untuk tetap menampilkan deskripsi statistik dari kolom non-numerik, kita bisa menambahkan argumen include='all' pada method describe().\n# Deskripsi statistik data\ndf.describe()\n\n# Deskripsi statistik data termasuk kolom non-numerik\ndf.describe(include='all')\n\n\n3.1.5 Fungsi statistik dalam Pandas\n\nPandas menyediakan fungsi statistik untuk menghitung nilai rata-rata, median, standar deviasi, dan variansi. Fungsi-fungsi tersebut adalah:\n\n\n\n\n\n\n\nFungsi\nKeterangan\n\n\n\n\ncount\nJumlah observasi non-NULL\n\n\nsum\nJumlah\n\n\nmean\nRata-rata\n\n\nmad\nDeviasi absolut rata-rata\n\n\nmedian\nNilai tengah\n\n\nmin\nNilai minimum\n\n\nmax\nNilai maksimum\n\n\nmode\nModus (nilai yang paling sering muncul)\n\n\nabs\nNilai absolut (nilai mutlak)\n\n\nprod\nHasil kali dari nilai-nilai\n\n\nquantile\nKuantil sampel (nilai pada persentil), kuartil pertama = quantile(0.25)\n\n\nstd\nStandar deviasi\n\n\nvar\nVarians\n\n\nsem\nStandar error mean (standar error dari rata-rata)\n\n\nskew\nSkewness (kecondongan distribusi data)\n\n\nkurt\nKurtosis (tingkat “tumpul” atau “tajam” distribusi data)\n\n\ncumsum\nAkumulasi jumlah\n\n\ncumprod\nAkumulasi hasil kali\n\n\ncummax\nNilai maksimum akumulasi\n\n\ncummin\nNilai minimum akumulasi\n\n\n\n\n\n3.1.6 Nilai Unik\n\nMethod unique() pada Pandas digunakan untuk mengetahui nilai unik dari suatu kolom. Method ini mengembalikan nilai unik dari suatu kolom dalam bentuk array. Method value_counts() pada Pandas digunakan untuk menghitung berapa kali suatu nilai muncul dalam suatu kolom. Method ini mengembalikan Series yang berisi frekuensi setiap nilai yang muncul dalam suatu kolom.\n# Penggunaan unique\ndf[\"nama_kolom\"].unique()\n\n# Penggunaan value_counts\ndf[\"nama_kolom\"].value_counts()\n\n\n3.1.7 Analisis dengan groupby\n\nMethod groupby() pada Pandas digunakan untuk melakukan grouping berdasarkan kolom tertentu. Method ini mengembalikan objek DataFrameGroupBy.\n# Penggunaan groupby dengan fungsi mean \ndf.groupby(\"kolom1\")[\"kolom2\"].mean()\n\n# menghitung statistik deskriptif dari setiap kelompok data\ndf.groupby(\"kolom1\")[\"kolom2\"].describe()\n\n# agregasi kustom dengan fungsi agg\ndf.groupby(\"kolom1\")[\"kolom2\"].agg([sum, min, max])\n\n#Menggunakan operasi agregasi kustom (menghitung rasio nilai maksimum dan minimum dalam setiap kelompok\ndf.groupby(\"kolom1\")[\"kolom2\"].agg(lambda x: x.max() / x.min())"
  },
  {
    "objectID": "modul_4.html#tugas-validasi-data",
    "href": "modul_4.html#tugas-validasi-data",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.1 Tugas Validasi Data",
    "text": "4.1 Tugas Validasi Data\n\nPeriksa/Nilai Kualitas Data\nPeriksa/Nilai Tingkat Kecukupan Data\nPeriksa/Nilai Kesesuaian Data\nPeriksa/Nilai Konsistensi Data"
  },
  {
    "objectID": "modul_4.html#manfaat-validasi-data",
    "href": "modul_4.html#manfaat-validasi-data",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.2 Manfaat Validasi Data",
    "text": "4.2 Manfaat Validasi Data\n\nTidak merusak perhitungan pada tahapan selanjutnya.\nMemvisualisasikan sebaran data, mendeteksi pola, dan mengidentifikasi anomali.\nMembantu menjelaskan dan mengkomunikasikan hasil analisis dengan lebih jelas."
  },
  {
    "objectID": "modul_4.html#laporan-dokumentasi-data-validasi",
    "href": "modul_4.html#laporan-dokumentasi-data-validasi",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.3 Laporan Dokumentasi Data Validasi",
    "text": "4.3 Laporan Dokumentasi Data Validasi\nLaporan dokumentasi data validasi, setidaknya memiliki parameter berikut:\n\nKebenaran, misal di Indonesia isian Gender yang diakui hanya 2 P/W; Agama hanya 6 (Islam, Protestan, Katholik, Hindu, Budha, Konghucu)\nKelengkapan, misal data provinsi seluruh Indonesia (34 prov), namun hanya sebagian yg ada.\nKonsistensi, misal penulisan STM atau SMK;"
  },
  {
    "objectID": "modul_4.html#validasi-vs-verifikasi",
    "href": "modul_4.html#validasi-vs-verifikasi",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.4 Validasi vs Verifikasi",
    "text": "4.4 Validasi vs Verifikasi\n\nValidasi: memastikan bahwa data yang diinputkan sesuai dengan ketentuan yang berlaku.\nVerifikasi: memastikan bahwa data yang diinputkan sesuai dengan data yang ada."
  },
  {
    "objectID": "modul_4.html#tahapan-kritikal-dalam-validasi",
    "href": "modul_4.html#tahapan-kritikal-dalam-validasi",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.5 Tahapan kritikal dalam validasi",
    "text": "4.5 Tahapan kritikal dalam validasi\n\nTipe Data (integer, float, string)\nEkspresi Konsisten (mis. Jalan, Jl., Jln.)\nFormat Data (mis. utk tgl “YYYY-MM-DD” vs “DD-MM-YYYY.”)\nNilai Null/Missing Values\nMisspelling/Type\nInvalid Data (gender: L/P: L; Laki-laki; P: Pria/Perempuan? )"
  },
  {
    "objectID": "modul_4.html#teknik-validasi-data",
    "href": "modul_4.html#teknik-validasi-data",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.6 Teknik Validasi Data",
    "text": "4.6 Teknik Validasi Data\n\nManual: melihat data secara langsung, misalnya melihat data di Excel.\nStatistik: menggunakan statistik deskriptif, misalnya melihat jumlah data, nilai maksimum, nilai minimum, dan lain-lain.\nVisualisasi: menggunakan visualisasi data, misalnya melihat sebaran data menggunakan histogram, boxplot, dan lain-lain."
  },
  {
    "objectID": "modul_4.html#validasi-dengan-pandas",
    "href": "modul_4.html#validasi-dengan-pandas",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.7 Validasi Dengan Pandas",
    "text": "4.7 Validasi Dengan Pandas\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Buat DataFrame\ndf = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\ndf['target'] = y\n\n# rename dataframe column\ndf = df.rename(columns={'feature_0':'Sepal Length','feature_1':'Sepal Width','feature_2':'Petal Length','feature_3':'Petal Width'})\n\n# untuk menampilkan 5 data teratas\nprint(df.head())\n\n# untuk menampilkan informasi dataset\nprint(df.info())\n\n# untuk menampilkan statistik deskriptif dataset\nprint(df.describe())\n\n# untuk menampilkan jumlah data null pada dataset\nprint(df.isnull().sum())\n\nMethod head() dapat digunakan untuk melihat tampilan 5 data teratas dalam dataset.\nMethod info() dapat digunakan untuk melihat informasi data frame, seperti jumlah baris, kolom, nilai non-NULL, tipe data, dan total penggunaan memori. Method ini sangat berguna dalam melakukan validasi tahap awal pada data.\nMethod describe() digunakan untuk menampilkan statistik deskriptif dari dataset seperti max , min , count , std dan lain lain.\nMethod isnull().sum() berfungsi untuk menghitung jumlah data kosong dalam sebuah dataset."
  },
  {
    "objectID": "modul_4.html#visualisasi-data",
    "href": "modul_4.html#visualisasi-data",
    "title": "4  Validasi dan Visualisasi Data",
    "section": "4.8 Visualisasi Data",
    "text": "4.8 Visualisasi Data\n\nVisualisasi data dapat membantu dalam memvalidasi data.\nMemvisualisasikan sebaran data, mendeteksi pola, dan mengidentifikasi anomali.\nVisualisasi yang baik dapat menceritakan sebuah cerita tentang data Anda dengan cara yang tidak dapat dilakukan oleh sebuah kalimat.\n\n\n4.8.1 Library Visualisasi Data\nTerdapat dua library populer untuk visualisasi data di Python, yaitu:\n\nMatplotlib: library yang paling populer untuk visualisasi data di Python. Umumnya diberi alias plt.\n\n\n\n\n\n\n\nSeaborn: library yang dibangun di atas Matplotlib, sehingga memiliki sintaks yang mirip. Umumnya diberi alias sns.\n\n\n\n\n\n\n\n\n4.8.2 Jenis Visualisasi Data\n\nPerbandingan (Comparison)\n\nBar Chart\n\nDeskripsi: Bar chart menunjukkan data dalam bentuk batang vertikal atau horizontal, di mana panjang batang mewakili nilai data.\nPenggunaan: Cocok untuk membandingkan nilai antara kategori atau menunjukkan perubahan nilai dari waktu ke waktu.\n\nLine Chart\n\nDeskripsi: Line chart menunjukkan data sebagai garis yang menghubungkan titik data. Digunakan untuk menyoroti tren atau perubahan sepanjang waktu.\nPenggunaan: Ideal untuk melihat perubahan nilai seiring waktu atau mengidentifikasi pola tren.\n\nCombo Chart\n\nDeskripsi: Combo chart menggabungkan dua jenis chart atau lebih dalam satu tampilan, seperti bar chart dan line chart.\nPenggunaan: Berguna untuk menyajikan data yang memiliki skala atau satuan yang berbeda dalam satu tampilan. Misalnya, pendapatan (bar) dan persentase pertumbuhan (line).\n\nHands On Coding \n\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    # Load dataset Iris\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Buat DataFrame\n    df = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n    df['target'] = y\n\n    # Rename DataFrame Column \n    df = df.rename(columns={'feature_0':'Sepal Length','feature_1':'Sepal Width','feature_2':'Petal Length','feature_3':'Petal Width'})\n\n    # Bar Chart untuk nilai rata-rata setiap fitur berdasarkan target\n    mean_values = df.groupby('target').mean()\n    bar_chart = mean_values.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n    bar_chart.set_ylabel('Rata-rata Nilai Fitur')\n    bar_chart.set_xlabel('Kategori Target')\n    bar_chart.set_title('Bar Chart: Rata-rata Nilai Fitur berdasarkan Target')\n    plt.show()\n\n    # Line Chart untuk nilai rata-rata setiap fitur\n    line_chart = mean_values.T.plot(kind='line', marker='o', figsize=(10, 6), colormap='viridis')\n    line_chart.set_ylabel('Rata-rata Nilai Fitur')\n    line_chart.set_xlabel('Fitur')\n    line_chart.set_title('Line Chart: Rata-rata Nilai Fitur')\n    plt.show()\n\n    # Combo Chart: Kombinasi Bar dan Line Chart\n    fig, ax1 = plt.subplots(figsize=(8, 6))\n\n    # Bar Chart pada sumbu kiri\n    color = 'tab:blue'\n    ax1.set_xlabel('Kategori Target')\n    ax1.set_ylabel('Rata-rata Nilai Fitur', color=color)\n    bar_chart_combo = mean_values.plot(kind='bar', ax=ax1, colormap='viridis')\n    ax1.tick_params(axis='y', labelcolor=color)\n\n    # Line Chart pada sumbu kanan\n    ax2 = ax1.twinx()\n    color = 'tab:red'\n    ax2.set_ylabel('Rata-rata Nilai Fitur', color=color)\n    line_chart_combo = mean_values.T.plot(kind='line', marker='o', ax=ax2, color=color)\n    ax2.tick_params(axis='y', labelcolor=color)\n\n    # Judul dan tampilan grafik\n    fig.suptitle('Combo Chart: Kombinasi Bar dan Line Chart', fontsize=16)\n    plt.show()\n  \n\nKomposisi (Composition)\n\nPie Chart\n\nDeskripsi: Pie chart menggambarkan data dalam bentuk lingkaran yang terbagi menjadi sejumlah “potongan” (slices) yang mewakili proporsi relatif dari keseluruhan.\nPenggunaan: Cocok untuk menunjukkan perbandingan proporsi atau persentase dari satu keseluruhan. Contohnya, pangsa pasar dari beberapa produk.\n\nStacked Bar Chart\n\nDeskripsi: Stacked bar chart menunjukkan data dalam bentuk batang vertikal atau horizontal yang bertumpuk, dengan setiap tumpukan mewakili kategori atau sub-kategori.\nPenggunaan: Berguna untuk menunjukkan kontribusi masing-masing bagian terhadap keseluruhan. Contohnya, penjualan bulanan per produk yang dibagi berdasarkan wilayah.\n\nTreemap\n\nDeskripsi: Treemap menggambarkan data hierarkis dalam bentuk kotak-kotak yang berukuran berdasarkan nilai atau ukuran masing-masing kategori.\nPenggunaan: Ideal untuk menunjukkan struktur hierarki dan membandingkan proporsi relatif di dalamnya. Misalnya, pengeluaran anggaran departemen dalam sebuah perusahaan.\n\nWaterfall Chart\n\nDeskripsi: Waterfall chart digunakan untuk mengilustrasikan perubahan kumulatif dari suatu nilai, dengan batang yang naik atau turun mewakili kontribusi positif atau negatif.\nPenggunaan: Berguna untuk melihat kontribusi sepanjang suatu proses atau mengidentifikasi faktor-faktor yang mempengaruhi perubahan nilai. Contohnya, perubahan laba bersih dari satu periode ke periode berikutnya.\n\nHands On Coding \n\n    pip install squarify\n    import matplotlib.pyplot as plt\n    import squarify\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    # Load dataset Iris\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Buat DataFrame\n    df = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n    df['target'] = y\n\n    # Rename columns\n    df = df.rename(columns={'feature_0':'Sepal Length','feature_1':'Sepal Width','feature_2':'Petal Length','feature_3':'Petal Width'})\n\n    # Pie Chart untuk proporsi kategori target\n    target_counts = df['target'].value_counts()\n    labels = [f'Kategori {i}' for i in target_counts.index]\n    colors = ['gold', 'lightcoral', 'lightskyblue']\n\n    plt.figure(figsize=(5, 5))\n    plt.pie(target_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n    plt.title('Pie Chart: Proporsi Kategori Target')\n    plt.show()\n\n    # Stacked Bar Chart untuk distribusi nilai fitur berdasarkan kategori target\n    stacked_bar_chart = df.groupby('target').mean().T.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n    stacked_bar_chart.set_ylabel('Nilai Fitur')\n    stacked_bar_chart.set_xlabel('Fitur')\n    stacked_bar_chart.set_title('Stacked Bar Chart: Distribusi Nilai Fitur berdasarkan Kategori Target')\n    plt.show()\n\n    # Treemap untuk proporsi kategori target\n    plt.figure(figsize=(10, 6))\n    squarify.plot(sizes=target_counts, label=labels, color=colors, alpha=0.7)\n    plt.title('Treemap: Proporsi Kategori Target')\n    plt.axis('off')\n    plt.show()\n\n    # Waterfall Chart untuk melihat kontribusi setiap fitur terhadap total\n    waterfall_data = df.drop('target', axis=1).sum()\n    waterfall_chart = waterfall_data.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n    waterfall_chart.set_ylabel('Nilai Fitur')\n    waterfall_chart.set_xlabel('Fitur')\n    waterfall_chart.set_title('Waterfall Chart: Kontribusi Fitur terhadap Total')\n    plt.show()\n   \n\nDistribusi (Distribution)\n\nHistogram\n\nDeskripsi: Histogram adalah grafik batang yang menunjukkan distribusi frekuensi dari suatu data numerik. Batang-batang tersebut mewakili rentang nilai dan tingginya sesuai dengan frekuensinya.\nPenggunaan: Cocok untuk menunjukkan distribusi dan pola data. Biasanya digunakan untuk melihat apakah data memiliki kecenderungan tertentu, seperti distribusi normal atau skewness.\n\nBox Plot\n\nDeskripsi: Box plot menggambarkan statistik ringkasan seperti kuartil, median, dan rentang dalam bentuk diagram. Garis di dalam kotak mewakili median, dan batas kotak menunjukkan kuartil pertama (Q1) dan kuartil ketiga (Q3).\nPenggunaan: Berguna untuk melihat distribusi dan variabilitas data. Box plot membantu mengidentifikasi pencilan (outliers) dan memberikan gambaran ringkas tentang statistik deskriptif.\n\nViolin Plot\n\nDeskripsi: Violin plot adalah kombinasi antara box plot dan kernel density plot. Ini menunjukkan distribusi data dengan cara yang mirip dengan histogram, tetapi dalam bentuk lebih halus.\nPenggunaan: Ideal untuk melihat distribusi dan kepadatan data. Violin plot dapat memberikan informasi lebih rinci tentang bentuk distribusi, serta menunjukkan kepadatan pada berbagai nilai.\n\nHands On Coding \n\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    # Load dataset Iris\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Buat DataFrame\n    df = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n    df['target'] = y\n\n    # Rename columns\n    df = df.rename(columns={'feature_0':'Sepal Length','feature_1':'Sepal Width','feature_2':'Petal Length','feature_3':'Petal Width'})\n\n    # Histogram untuk distribusi nilai fitur\n    plt.figure(figsize=(12, 6))\n    for i, feature in enumerate(df.columns[:-1]):\n        plt.subplot(2, 2, i + 1)\n        sns.histplot(df[feature], kde=True, color='skyblue')\n        plt.title(f'Histogram: {feature}')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Box Plot untuk melihat distribusi dan outlier\n    plt.figure(figsize=(12, 6))\n    for i, feature in enumerate(df.columns[:-1]):\n        plt.subplot(2, 2, i + 1)\n        sns.boxplot(x='target', y=feature, data=df, palette='viridis')\n        plt.title(f'Box Plot: {feature}')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Violin Plot untuk kombinasi histogram dan kernel density estimate (KDE)\n    plt.figure(figsize=(12, 6))\n    for i, feature in enumerate(df.columns[:-1]):\n        plt.subplot(2, 2, i + 1)\n        sns.violinplot(x='target', y=feature, data=df, palette='viridis', inner='quartile')\n        plt.title(f'Violin Plot: {feature}')\n\n    plt.tight_layout()\n    plt.show()\n  \n\nHubungan (Relationship)\n\nScatter Plot\n\nDeskripsi: Scatter plot menunjukkan hubungan antara dua variabel numerik dengan menempatkan titik-titik pada bidang kartesian. Setiap titik mewakili satu pengamatan atau data point.\nPenggunaan: Cocok untuk menemukan pola atau hubungan antara dua variabel. Misalnya, hubungan antara waktu studi dan nilai ujian.\n\nBubble Chart\n\nDeskripsi: Sama seperti scatter plot, tetapi dengan tambahan dimensi ketiga yang direpresentasikan oleh ukuran gelembung (bubble). Ukuran gelembung mencerminkan nilai dari variabel ketiga.\nPenggunaan: Berguna untuk menunjukkan hubungan tiga variabel sekaligus. Contohnya, scatter plot yang menunjukkan hubungan antara waktu studi dan nilai ujian, dengan ukuran gelembung mencerminkan jumlah jam belajar.\n\nHeatmap\n\nDeskripsi: Heatmap adalah representasi visual dari data dalam bentuk matriks di mana warna mewakili nilai masing-masing sel. Digunakan terutama untuk menunjukkan pola dalam data besar.\nPenggunaan: Cocok untuk menemukan pola atau relasi dalam data matriks. Misalnya, heatmap dapat digunakan untuk menunjukkan korelasi antara berbagai variabel atau untuk visualisasi data spasial.\n\nHands On Coding \n\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    # Load dataset Iris\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Buat DataFrame\n    df = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n    df['target'] = y\n\n    # Rename columns\n    df = df.rename(columns={'feature_0':'Sepal Length','feature_1':'Sepal Width','feature_2':'Petal Length','feature_3':'Petal Width'})\n\n    # Scatter Plot untuk melihat hubungan antara dua fitur\n    plt.figure(figsize=(12, 6))\n    sns.scatterplot(x='Sepal Length', y='Sepal Width', hue='target', data=df, palette='viridis', s=80)\n    plt.title('Scatter Plot: Hubungan antara Sepal Length dan Sepal Width')\n    plt.show()\n\n    # Bubble Chart untuk menambah dimensi ketiga (ukuran marker) pada scatter plot\n    plt.figure(figsize=(12, 6))\n    sns.scatterplot(x='Sepal Length', y='Sepal Width', hue='target', size='Petal Length', sizes=(20, 200), data=df, palette='viridis')\n    plt.title('Bubble Chart: Hubungan antara Sepal Length, Sepal Width, dan Petal Length')\n    plt.show()\n\n    # Heatmap untuk melihat korelasi antara seluruh fitur\n    correlation_matrix = df.drop('target', axis=1).corr()\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=.5)\n    plt.title('Heatmap: Korelasi antara Seluruh Fitur')\n    plt.show()"
  },
  {
    "objectID": "modul_5.html#menentukan-sumber-data",
    "href": "modul_5.html#menentukan-sumber-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.1 Menentukan Sumber Data",
    "text": "5.1 Menentukan Sumber Data\nSumber data merupakan tempat atau lokasi sebuah data disimpan dan atau diakses untuk dapat dilakukan analisis serta penggunaan lainnya. Pemilihan sumber data harus mempertimbangkan sebuah kendala, akurasi, ketersediaan, serta relvansi data dengan tujuan analisis atau model dari machine learning yang akan digunakan atau di bangun."
  },
  {
    "objectID": "modul_5.html#menelaah-susunan-data",
    "href": "modul_5.html#menelaah-susunan-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.2 Menelaah Susunan Data",
    "text": "5.2 Menelaah Susunan Data\n\nMelakukan pemeriksaan struktur data untuk memahami bagaimana data diorganisir.\nMelihat dimensi data, jumlah atribut/kolom, dan jumlah sampel/baris.\nMenilai apakah data terstruktur (misalnya, data tabel) atau tidak terstruktur (misalnya, data teks atau gambar)."
  },
  {
    "objectID": "modul_5.html#menentukan-tipe-dan-model-data-yang-dimiliki",
    "href": "modul_5.html#menentukan-tipe-dan-model-data-yang-dimiliki",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.3 Menentukan Tipe dan Model Data yang dimiliki",
    "text": "5.3 Menentukan Tipe dan Model Data yang dimiliki\n\nIdentifikasi tipe data untuk setiap atribut (numerik, kategorikal, teks, tanggal, dll.).\nPenentuan model data yang sesuai untuk analisis atau model machine learning berdasarkan tipe data.\nPemahaman tentang apakah data bersifat kontinu, diskret, ordinal, atau nominal."
  },
  {
    "objectID": "modul_5.html#mengambil-data",
    "href": "modul_5.html#mengambil-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.4 Mengambil Data",
    "text": "5.4 Mengambil Data\n\nProses pengambilan data dari sumber data ke lingkungan analisis atau pengembangan model machine learning.\nMenggunakan berbagai metode seperti mengimpor file data (misalnya CSV, Excel), mengakses database, atau menggunakan API untuk mengambil data dari sumber online."
  },
  {
    "objectID": "modul_5.html#menelaah-data-dalam-machine-learning",
    "href": "modul_5.html#menelaah-data-dalam-machine-learning",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.5 Menelaah Data dalam Machine Learning",
    "text": "5.5 Menelaah Data dalam Machine Learning\n\nMelakukan eksplorasi data (data exploration) untuk memahami karakteristik data secara lebih mendalam.\nVisualisasi data dengan grafik atau plot untuk memahami pola, distribusi, dan korelasi antara atribut.\nIdentifikasi missing value, outlier, dan data yang tidak konsisten untuk diatasi sebelum analisis atau pemodelan."
  },
  {
    "objectID": "modul_5.html#contoh-source-code",
    "href": "modul_5.html#contoh-source-code",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.6 Contoh Source Code",
    "text": "5.6 Contoh Source Code\nBerikut adalah contoh sederhana menggunakan Python untuk mengambil data dari file CSV, menelaah susunan data, menentukan tipe data, dan melakukan eksplorasi data menggunakan pandas dan matplotlib.\nSilahkan Unduh dataset berikut Iris_Dataset. Pastikan dataset disimpan dalam file CSV dengan nama “iris_dataset.csv” dalam folder yang sama dengan script Python.\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Mengambil data dari file CSV\ndata_path = 'iris_dataset.csv'\ndf = pd.read_csv(data_path)\n\n# Menelaah susunan data\nprint(\"Dimensi data:\", df.shape)\nprint(\"Info data:\")\nprint(df.info())\nprint(\"Sepuluh data pertama:\")\nprint(df.head(10))\n\n# Menentukan tipe data dan model data yang dimiliki\nprint(\"Tipe data untuk setiap atribut:\")\nprint(df.dtypes)\n\n# Preprocessing data (jika diperlukan)\n# Tidak ada preprocessing yang diperlukan dalam contoh ini\n\n# Mengambil statistik deskriptif untuk data numerik\nprint(\"Statistik deskriptif:\")\nprint(df.describe())\n\n# Eksplorasi data dengan visualisasi\nplt.figure(figsize=(10, 6))\nplt.scatter(df['sepal_length'], df['sepal_width'])\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('Scatter Plot: Sepal Length vs. Sepal Width')\nplt.show()\n\nplt.figure(figsize=(8, 6))\ndf['species'].value_counts().plot(kind='bar')\nplt.xlabel('Species')\nplt.ylabel('Count')\nplt.title('Bar Plot: Species Distribution')\nplt.show()\nPastikan bahwa dataset “Iris” (iris_dataset.csv) berisi kolom dengan nama “sepal_length”, “sepal_width”, “petal_length”, “petal_width”, dan “species”. Jika dataset tersebut memiliki atribut lain atau format yang berbeda, Anda perlu menyesuaikan kode di atas sesuai dengan dataset yang digunakan.\nContoh di atas akan membaca dataset Iris, menampilkan informasi dasar tentang dataset, menampilkan sepuluh baris pertama dari data, menentukan tipe data untuk setiap atribut, menampilkan statistik deskriptif untuk data numerik, dan melakukan eksplorasi data dengan visualisasi menggunakan scatter plot untuk melihat hubungan antara panjang dan lebar kelopak bunga serta bar plot untuk melihat distribusi spesies bunga.\nHarap dicatat bahwa contoh di atas hanya merupakan contoh sederhana, dan dalam penerapan sebenarnya, langkah-langkah analisis dan eksplorasi data bisa lebih canggih dan lengkap sesuai dengan kompleksitas dan karakteristik data.\nPemahaman mendalam tentang sumber data, susunan data, tipe data, dan karakteristik data adalah langkah kritis sebelum membangun model machine learning yang efektif."
  },
  {
    "objectID": "modul_6.html#data-cleaning",
    "href": "modul_6.html#data-cleaning",
    "title": "6  Cleaning Data",
    "section": "6.1 Data Cleaning",
    "text": "6.1 Data Cleaning\nPada proses machine learning, terdapat tahapan preprocessing. Preprocessing merupakan tahapan yang penting dalam machine learning. Hasil dari preprocessing dapat mempengaruhi nilai akurasi dari sebuah model. Tujuan dari Preprocessing adalah untuk memastikan data siap untuk diproses dan atau digunakan pada machine learning. Dalam melakukan preprocessing memiliki beberapa tantangan, diantara adalah missing value, outlier, format tidak konsisten, dan malformed record"
  },
  {
    "objectID": "modul_6.html#cara-mengatasi-missing-value",
    "href": "modul_6.html#cara-mengatasi-missing-value",
    "title": "6  Cleaning Data",
    "section": "6.2 Cara Mengatasi Missing Value",
    "text": "6.2 Cara Mengatasi Missing Value\n\nMising Value merupakan data yang kosong atau tidak lengkap.\nTantangan dalam mengatasi missing value adalah bagaimana mengisi nilai kosong tersebut.\nStrategi yang dapat dilakukan diantara lain :\n\nMenghapus baris atau kolom yang memiliki missing value\nMengisi nilai kosong dengan nilai rata-rata atau median\nMengisi nilai kosong dengan nilai yang sering muncul\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis\n\n6.2.1 Menghapus baris atau kolom yang memiliki missing value\n\n6.2.1.1 Tanpa Menggunakan Dataset\nimport pandas as pd\n\n# Contoh dataset dengan nilai yang hilang\ndata = {\n    'A': [1, 2, None, 4],\n    'B': [5, None, 7, 8],\n    'C': [9, 10, 11, None]\n}\ndf = pd.DataFrame(data)\n\n# Tampilkan dataset sebelum penghapusan\nprint(\"Dataset sebelum penghapusan:\")\nprint(df)\n\n# Hapus baris yang memiliki nilai yang hilang\ndf_cleaned_rows = df.dropna()\nprint(\"\\nDataset setelah menghapus baris yang memiliki nilai yang hilang:\")\nprint(df_cleaned_rows)\n\n# Hapus kolom yang memiliki nilai yang hilang\ndf_cleaned_cols = df.dropna(axis=1)\nprint(\"\\nDataset setelah menghapus kolom yang memiliki nilai yang hilang:\")\nprint(df_cleaned_cols)\n\n\n6.2.1.2 Menggunakan Dataset\nuntuk mengunduh dataset agar dapat digunakan pada source code disini\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom\nprint(\"Informasi tentang dataset Titanic:\")\nprint(titanic_df.info())\n\n# Hapus baris yang memiliki nilai yang hilang\ntitanic_cleaned_rows = titanic_df.dropna()\nprint(\"\\nDataset Titanic setelah menghapus baris yang memiliki nilai yang hilang:\")\nprint(titanic_cleaned_rows)\n\n# Hapus kolom yang memiliki nilai yang hilang\ntitanic_cleaned_cols = titanic_df.dropna(axis=1)\nprint(\"\\nDataset Titanic setelah menghapus kolom yang memiliki nilai yang hilang:\")\nprint(titanic_cleaned_cols)\n\n\n\n6.2.2 Mengisi nilai kosong dengan nilai rata-rata atau median\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom\nprint(\"Informasi tentang dataset Titanic sebelum pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n# Mengisi nilai kosong pada kolom 'Age' dengan nilai median dari kolom tersebut\nage_median = titanic_df['Age'].median()\ntitanic_df['Age'].fillna(age_median, inplace=True)\n\n# Mengisi nilai kosong pada kolom 'Fare' dengan nilai rata-rata dari kolom tersebut\nfare_mean = titanic_df['Fare'].mean()\ntitanic_df['Fare'].fillna(fare_mean, inplace=True)\n\n# Tampilkan informasi tentang dataset setelah pengisian nilai kosong\nprint(\"\\nInformasi tentang dataset Titanic setelah pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n\n6.2.3 Mengisi nilai kosong dengan nilai yang sering muncul\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom sebelum pengisian\nprint(\"Informasi tentang dataset Titanic sebelum pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n# Mengisi nilai kosong pada kolom 'Embarked' dengan nilai yang sering muncul (mode) dari kolom tersebut\nembarked_mode = titanic_df['Embarked'].mode()[0]\ntitanic_df['Embarked'].fillna(embarked_mode, inplace=True)\n\n# Tampilkan informasi tentang dataset setelah pengisian nilai kosong\nprint(\"\\nInformasi tentang dataset Titanic setelah pengisian nilai kosong:\")\nprint(titanic_df.info())"
  },
  {
    "objectID": "modul_6.html#menangani-outlier",
    "href": "modul_6.html#menangani-outlier",
    "title": "6  Cleaning Data",
    "section": "6.3 Menangani Outlier",
    "text": "6.3 Menangani Outlier\nOutlier merupakan data yang jauh dari nilai rata-rata atau nilai normal. Outlier dapat mempengaruhi hasil analisis dan machine learning.\nOutlier dapat terjadi karena berbagai alasan, seperti kesalahan pengukuran, kesalahan entri data, variasi alami, atau peristiwa langka. Outlier dapat memiliki dampak signifikan pada analisis statistik, model pembelajaran mesin, dan interpretasi data, yang dapat mengarah pada hasil yang bias atau kesimpulan yang tidak akurat.\n\nStrategi untuk mengatasi Outlier :\n\nMenghapus outlier.\nMenggantikan nilai outlier dengan nilai lain seperti nilai rata-rata atau median.\nMenggunakan teknik scaling atau normalisasi.\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis.\n\n- Menghapus Outlier\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom scipy import stats\n\n# Load dataset Iris dari scikit-learn\ndata = load_iris()\niris_df = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Tampilkan informasi tentang dataset Iris sebelum penghapusan outlier\nprint(\"Informasi tentang dataset Iris sebelum penghapusan outlier:\")\nprint(iris_df.describe())\n\n# Definisikan fungsi untuk menghapus outlier berdasarkan z-score\ndef remove_outliers_zscore(df, z_threshold=3):\n    z_scores = stats.zscore(df)\n    return df[(z_scores &lt; z_threshold).all(axis=1)]\n\n# Hapus outlier dari dataset Iris berdasarkan z-score\niris_cleaned = remove_outliers_zscore(iris_df)\n\n# Tampilkan informasi tentang dataset Iris setelah penghapusan outlier\nprint(\"\\nInformasi tentang dataset Iris setelah penghapusan outlier:\")\nprint(iris_cleaned.describe())\nPada contoh di atas, digunakan metode z-score untuk mendeteksi outlier dalam dataset Iris. Outlier adalah data yang memiliki z-score lebih besar dari z_threshold yang telah ditentukan (z_threshold=3). Fungsi remove_outliers_zscore digunakan untuk menghapus baris yang mengandung outlier berdasarkan z-score, yaitu baris yang memiliki setidaknya satu fitur (kolom) dengan z-score melebihi z_threshold. Penting untuk berhati-hati karena penghapusan outlier dapat mempengaruhi hasil analisis atau model machine learning. Selain z-score, ada banyak teknik deteksi outlier lainnya seperti IQR dan pendekatan berbasis model machine learning atau domain knowledge.\n\n\n- Menggantikan nilai outlier dengan nilai lain seperti nilai rata-rata atau median.\nimport pandas as pd\n\n# membaca dataset publik\ndata = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n\n# menentukan batas outlier\nq1 = data[0].quantile(0.25)\nq3 = data[0].quantile(0.75)\niqr = q3 - q1\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n\n# menggantikan outlier dengan nilai rata-rata\ndata[0] = data[0].apply(lambda x: data[0].mean() if x &lt; lower_bound or x &gt; upper_bound else x)\n\n# menggantikan outlier dengan nilai median\ndata[0] = data[0].apply(lambda x: data[0].median() if x &lt; lower_bound or x &gt; upper_bound else x)\n\n# menampilkan dataset yang telah diubah\nprint(data)\nPada contoh kode di atas, dataset publik yang digunakan adalah Iris Dataset yang tersedia di UCI Machine Learning Repository. Pertama-tama, kita menentukan batas outlier dengan menghitung kuartil pertama (Q1), kuartil ketiga (Q3), dan rentang antar kuartil (IQR). Kemudian, kita menentukan batas bawah dan batas atas untuk menentukan nilai outlier.\nKemudian, kita menggantikan nilai outlier dengan nilai rata-rata atau median. Untuk menggantikan dengan nilai rata-rata, kita menggunakan fungsi apply() pada kolom yang menghasilkan nilai rata-rata jika nilai kurang dari batas bawah atau lebih dari batas atas, sedangkan untuk menggantikan dengan nilai median, kita juga menggunakan fungsi apply() pada kolom yang menghasilkan nilai median jika nilai kurang dari batas bawah atau lebih dari batas atas.\nTerakhir, kita menampilkan dataset yang telah diubah dengan fungsi print().\n\n\n- Mengisi nilai kosong dengan nilai yang sering muncul\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom sebelum pengisian\nprint(\"Informasi tentang dataset Titanic sebelum pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n# Mengisi nilai kosong pada kolom 'Embarked' dengan nilai yang sering muncul (mode) dari kolom tersebut\nembarked_mode = titanic_df['Embarked'].mode()[0]\ntitanic_df['Embarked'].fillna(embarked_mode, inplace=True)\n\n# Tampilkan informasi tentang dataset setelah pengisian nilai kosong\nprint(\"\\nInformasi tentang dataset Titanic setelah pengisian nilai kosong:\")\nprint(titanic_df.info())\nPada contoh di atas, menggunakan metode mode() dari pandas untuk menghitung nilai yang sering muncul (mode) pada kolom ‘Embarked’, lalu mengisi nilai kosong pada kolom tersebut dengan nilai mode yang dihitung. Penggunaan parameter inplace=True memastikan perubahan dilakukan langsung pada DataFrame asli. Setelah mengisi nilai kosong, hasilnya dapat diperiksa untuk memastikan tidak ada lagi nilai yang hilang pada kolom ‘Embarked’.\nPerlu diingat, pengisian nilai kosong dengan nilai yang sering muncul merupakan salah satu teknik imputasi yang umum. Teknik lainnya termasuk pengisian dengan nilai rata-rata, nilai median, atau menggunakan model machine learning untuk memprediksi nilai yang hilang berdasarkan data lainnya. Pilihan teknik imputasi tergantung pada karakteristik dataset dan tujuan analisis atau model machine learning yang ingin diimplementasikan."
  },
  {
    "objectID": "modul_6.html#menangani-format-yang-tidak-konsisten",
    "href": "modul_6.html#menangani-format-yang-tidak-konsisten",
    "title": "6  Cleaning Data",
    "section": "6.4 Menangani Format yang Tidak Konsisten",
    "text": "6.4 Menangani Format yang Tidak Konsisten\nFormat Tidak Konsisten terjadi ketika data memiliki format yang sama atau tidak sesuai dengan format yang diharapkan. Sering terjadi pada pemformatan tanggal, bulan, dan tahun.\n\nStrategi dalam mengatasi Format Tidak Konsisten\n\nMengubah format data menjadi format yang konsisten seperti mengubah format tanggal menjadi format yang sama\nMemperbaiki data yang salah ketik atau typo\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis\n\n6.4.1 Mengubah format data menjadi format yang konsisten seperti mengubah format tanggal menjadi format yang sama\nimport pandas as pd\n\n# Contoh dataset dengan kolom tanggal dalam format yang berbeda\ndata = {\n    'Tanggal': ['2021-08-01', '02-08-2021', '2021/08/03', '20210804']\n}\ndf = pd.DataFrame(data)\n\n# Tampilkan dataset sebelum perubahan format\nprint(\"Dataset sebelum perubahan format:\")\nprint(df)\n\n# Ubah format tanggal menjadi format yang sama (YYYY-MM-DD)\ndf['Tanggal'] = pd.to_datetime(df['Tanggal'], errors='coerce').dt.strftime('%Y-%m-%d')\n\n# Tampilkan dataset setelah perubahan format\nprint(\"\\nDataset setelah perubahan format:\")\nprint(df)\n\n\n6.4.2 - Memperbaiki data yang salah ketik atau typo\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load dataset Iris dari scikit-learn\ndata = load_iris()\niris_df = pd.DataFrame(data.data, columns=data.feature_names)\niris_df['species'] = data.target_names[data.target]\n\n# Contoh data dengan salah ketik atau typo\niris_df.iloc[0, 0] = 5.1\niris_df.iloc[1, 1] = 3.6\n\n# Tampilkan dataset sebelum pembersihan data\nprint(\"Dataset sebelum pembersihan data:\")\nprint(iris_df)\n\n# Koreksi data salah ketik atau typo\n# Misalnya, jika nilai 3.6 pada kolom 'sepal width (cm)' seharusnya 3.0\niris_df.loc[iris_df['sepal width (cm)'] == 3.6, 'sepal width (cm)'] = 3.0\n\n# Tampilkan dataset setelah pembersihan data\nprint(\"\\nDataset setelah pembersihan data:\")\nprint(iris_df)"
  },
  {
    "objectID": "modul_6.html#menangani-malformed-record",
    "href": "modul_6.html#menangani-malformed-record",
    "title": "6  Cleaning Data",
    "section": "6.5 Menangani Malformed Record",
    "text": "6.5 Menangani Malformed Record\nMalformed Record terjadi saat ketika data tidak memenuhi format atau struktur yang diharapkan.\n\nStrategi yang dapat dilakukan diantara lain :\n\nMenghapus record yang tidak sesuai dengan format atau struktur yang diharapkan\nMengubah record yang tidak sesuai dengan format atau struktur yang diharapkan\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis.\n\n6.5.1 - Menghapus record yang tidak sesuai dengan format atau struktur yang diharapkan\nimport pandas as pd\n\n# membaca dataset\ndata = pd.read_csv(\"nama_file.csv\")\n\n# mengecek dan menghapus record yang tidak sesuai dengan format atau struktur yang diharapkan\nfor i, row in data.iterrows():\n    if not format_check(row):\n        data.drop(i, inplace=True)\n\n# menampilkan dataset yang telah diubah\nprint(data)\nKode di atas menggunakan library pandas untuk membaca dataset dari file csv dan melakukan pengecekan format atau struktur pada setiap record dalam dataset dengan fungsi format_check(). Jika record tidak sesuai dengan format atau struktur yang diharapkan, maka record tersebut dihapus dari dataset menggunakan fungsi drop(). Fungsi iterrows() digunakan untuk mengiterasi setiap record dalam dataset. Setelah proses penghapusan selesai, dataset yang telah diubah ditampilkan menggunakan fungsi print(). Penting untuk menyesuaikan kode dengan format atau struktur dataset yang digunakan dan memastikan menggunakan fungsi format_check() yang sesuai.\n\n\n6.5.2 - Mengubah record yang tidak sesuai dengan format atau struktur yang diharapkan\n#| code-fold: true\nimport pandas as pd\n\n# membaca dataset dari file csv\ndata = pd.read_csv('nama_file.csv')\n\n# fungsi untuk melakukan pengecekan format atau struktur pada setiap record dalam dataset\ndef format_check(record):\n    # implementasi pengecekan format atau struktur pada satu record\n    # return True jika format atau struktur sesuai, False jika tidak sesuai\n\n# melakukan iterasi pada setiap record dalam dataset\nfor index, row in data.iterrows():\n    # cek apakah format atau struktur record sesuai dengan yang diharapkan\n    if not format_check(row):\n        # jika tidak sesuai, hapus record dari dataset\n        data = data.drop(index)\n\n# menampilkan dataset yang telah diubah\nprint(data)\nPada contoh di atas, dataset dibaca dari file csv menggunakan fungsi read_csv() dari library pandas. Kemudian, dilakukan iterasi pada setiap record dalam dataset menggunakan fungsi iterrows(). Pada setiap iterasi, record dicek dengan fungsi format_check() untuk memastikan bahwa format atau struktur record sesuai dengan yang diharapkan. Jika tidak sesuai, record tersebut dihapus dari dataset menggunakan fungsi drop(). Setelah proses penghapusan selesai, dataset yang telah diubah ditampilkan menggunakan fungsi print(). Harap diingat bahwa fungsi format_check() harus disesuaikan dengan format atau struktur yang diharapkan pada dataset yang digunakan."
  },
  {
    "objectID": "modul_6.html#kesimpulan",
    "href": "modul_6.html#kesimpulan",
    "title": "6  Cleaning Data",
    "section": "6.6 Kesimpulan",
    "text": "6.6 Kesimpulan\nPreprocessing merupakan tahapan penting dalam proses machine learning. Tantangan seperti contoh diatas dapat diatasi dengan berbagai strategi, dengan memilih strategi yang tepat dan pemahaman tipe atau jenis data yang ada akan makin memudahkan dalam melakukan preprocessing."
  },
  {
    "objectID": "modul_7.html",
    "href": "modul_7.html",
    "title": "7  Transformasi Data",
    "section": "",
    "text": "8 Pelabelan Data\nPelabelan data (data labeling) adalah proses menandai atau memberi label pada data dengan kelas atau kategori yang sesuai.\nKuantitas & kualitas data pelatihan yang secara langsung menentukan keberhasilan suatu algoritma AI sehingga tidak mengherankan jika rata-rata 80% waktu yang dihabiskan untuk proyek AI membahas data pelatihan yang mencakup proses pelabelan data.\nKeakuratan model AI Anda berkorelasi langsung dengan kualitas data yang digunakan untuk melatihnya. Hal ini menjadi satu alasan mengapa proses pelabelan data merupakan bagian integral dari alur kerja persiapan data dalam membangun model AI yang handal."
  },
  {
    "objectID": "modul_7.html#rekayasafitur",
    "href": "modul_7.html#rekayasafitur",
    "title": "7  Transformasi Data",
    "section": "7.1 RekayasaFitur",
    "text": "7.1 RekayasaFitur\nRekayasa fitur adalah proses penambahan atau modifikasi fitur dengan mengaplikasikan penghitungan matematik, statistika, atau pengetahuan terhadap fitur.\nSebagai contoh, anda dapat membuat fitur baru bernama average atau rata rata yang mengambil nilai dari fitur-fitur lain. Atau anda dapat membuat fitur kategori baru dengan mengolah data dari fitur-fitur lain.\nDiharapkan fitur-fitur yang di modifikasi atau ditambah dapat menambah kualitas dataset sehingga dapat menghasilkan model yang lebih akurat dan efisien.\n\n7.1.1 Hands On Coding\nKita akan melakukan proses feature enginnering ke sebuah dataset dibawah ini\nJangan lupa untuk menginstall library pandas menggunakan command pip install pandas\n\nfrom spacy import displacy\nimport spacy\nfrom matplotlib import pyplot as plt\nimport cv2\nfrom sklearn.preprocessing import MinMaxScaler\nimport time\nimport random\nimport plotly.express as px\nfrom sklearn.impute import KNNImputer\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom scipy.stats import pearsonr\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.DataFrame({'Country': ['Afghanistan', 'Cameroon', 'Indonesia', 'Guatemala'], 'UrbanPopulation': [10142913, 15248270, 153983073,\n                  8738685], 'RuralPopulation': [28829316, 11242817, 117874900, 8119648], 'SlumPopulation': [7434756, 4981883, 29889391, 3285745]})\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nRuralPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n10142913\n28829316\n7434756\n\n\n1\nCameroon\n15248270\n11242817\n4981883\n\n\n2\nIndonesia\n153983073\n117874900\n29889391\n\n\n3\nGuatemala\n8738685\n8119648\n3285745\n\n\n\n\n\n\n\nDataset ini mempunyai 4 fitur:\n\nCountry, Nama negara\n\nUrbanPopulation, populasi manusia yang hidup di daerah urban (perumahan kota)\n\nRuralPopulation, populasi manusia yang hidup di daerah rural (pinggiran kota)\n\nSlumPopulation, populasi manusia yang hidup di daerah slum (pemukiman kumuh)\n\nPertama kita akan menghitung presentase jumlah populasi yang hidup di slum (pemukiman kumuh) dari populasi yang hidup di urban (perumahan kota). Nilai ini didapat menggunakan rumus:\n\\[\nSlumPrecentage = \\dfrac{SlumPopulation}{UrbanPopulation} * 100\n\\]\n\ndf['SlumPopulation'] = round(\n    (df['SlumPopulation']/(df['UrbanPopulation']))*100, 2)\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nRuralPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n10142913\n28829316\n73.30\n\n\n1\nCameroon\n15248270\n11242817\n32.67\n\n\n2\nIndonesia\n153983073\n117874900\n19.41\n\n\n3\nGuatemala\n8738685\n8119648\n37.60\n\n\n\n\n\n\n\nSelanjutnya, kita menggabungkan dua fitur (urban dan rural) menjadi satu, dan mengubah nilainya dari jumlah penduduk ke presentase penduduk. Rumusnya cukup simple:\n\\[\nUrbanPrecentage = \\dfrac{UrbanPopulation}{UrbanPopulation+RuralPopulation} * 100\n\\]\n\ndf['UrbanPopulation'] = round(\n    (df['UrbanPopulation']/(df['UrbanPopulation']+df['RuralPopulation']))*100, 2)\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nRuralPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n26.03\n28829316\n73.30\n\n\n1\nCameroon\n57.56\n11242817\n32.67\n\n\n2\nIndonesia\n56.64\n117874900\n19.41\n\n\n3\nGuatemala\n51.84\n8119648\n37.60\n\n\n\n\n\n\n\nDan untuk sentuhan akhir, kita akan hapus kolom SlumPopulation karena nilainya sudah ter-representasikan di kolom UrbanPopulation\n\ndf = df.drop(columns=['RuralPopulation'])\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n26.03\n73.30\n\n\n1\nCameroon\n57.56\n32.67\n\n\n2\nIndonesia\n56.64\n19.41\n\n\n3\nGuatemala\n51.84\n37.60\n\n\n\n\n\n\n\nHasilnya, nilai atau value di dataset lebih mudah dibaca, dan dapat direpresentasikan menggunakan fitur yang lebih sedikit. Dataset sudah siap untuk diproses lebih lanjut."
  },
  {
    "objectID": "modul_7.html#imputasi",
    "href": "modul_7.html#imputasi",
    "title": "7  Transformasi Data",
    "section": "7.2 Imputasi",
    "text": "7.2 Imputasi\nImputasi adalah proses penggantian nilai data yang hilang dengan data yang baru. Seperti contoh rekayasa fitur sebelumnya, nilai NaN termasuk data yang perlu kita olah.\nDalam bab ini kita akan mempelajari beberapa hal terkait imputasi antara lain:\n\nJenis-jenis imputasi\n\nTeknik imputasi\n\n\n7.2.1 Jenis-jenis Imputasi\nJenis data yang hilang dikelompokkan menjadi 3, yaitu\n\nMissing completely at random (MCAR)\nMissing at random (MAR)\nMissing not at random (MNAR)\n\n\n\n\nGambar1. Jenis-jenis Imputasi\n\n\n\n7.2.1.1 Missing Completely At Random (MCAR)\nJika probabilitas hilangnya data dalam suatu fitur sama antara satu data dengan yang lain. Asumsi ini dapat diuji dengan memisahkan data yang hilang dan yang lengkap serta memeriksa karakteristik data. Jika karakteristik data tidak sama untuk kedua fitur, asumsi MCAR tidak berlaku\n\n\n\nGambar2. Contoh MCAR\n\n\n\n\n7.2.1.2 Missing At Random (MAR)\nKemungkinan data yang hilang dipengaruhi oleh variabel lain, namun tidak dipengaruhi oleh variabel yang hilang. Sebagai contoh, untuk data di samping, hanya peserta dengan umur yang dibawah 31 yang nilainya hilang. Berarti fitur age mempengaruhi probabilitas missing data IQ score.\n\n\n\nGambar3. Contoh MAR\n\n\n\n\n\n7.2.2 Missing Not At Random\nKemungkinan data yang hilang tidak dipengaruhi oleh fitur lain, namun dipengaruhi oleh fitur pada data yang hilang. Sebagai contoh, untuk data di samping, ada kemungkinan bahwa data IQ score yang hilang hanya data yang nilainya dibawah 110. Sedangkan variabel age tidak berpengaruh atas hilangnya data IQ score karena age yang kecil dan besar sama sama mempunyai data yang hilang\n\n\n\nGambar4. Contoh MNAR\n\n\n\n\n7.2.3 Teknik-Teknik Imputasi\nPerlu diingat bahwa jika 70% data hilang/missing, maka semua fitur(kolom) dan data (row) harus dihapus.\n\n\n\nGambar5. Flow Proses Imputasi\n\n\nData yang hilang harus dimutasikan berdasarkan jenis datanya, yaitu :\nNumerik\n\nMean/Median\n\nArbitrary\n\nEnd of tail\n\nRegresi\n\nKNN\n\nKategorik\n\nFrequent/Modus\n\nKNN\n\n\n7.2.3.1 Mean\nDataset imputasi mean adalah metode untuk mengisi nilai yang hilang dalam dataset dengan menggunakan nilai rata-rata (mean) dari variabel yang bersangkutan. Ketika ada nilai yang hilang dalam suatu variabel dalam dataset, imputasi mean menggantikan nilai-nilai yang hilang tersebut dengan nilai rata-rata dari seluruh nilai yang ada dalam variabel tersebut.\nKelebihan\n\nMudah dan cepat.\n\nBekerja efektif untuk dataset numerik berukuran kecil.\n\nCocok untuk variabel numerik.\n\nCocok untuk data missing completely at random (MCAR).\n\nDapat digunakan dalam produksi (mis. dalam model deployment)\n\nKekurangan\n\nTidak memperhitungkan korelasi antar fitur, berfungsi pada tingkat kolom.\n\nKurang akurat.\n\nTidak memperhitungkan probabilitas/ketidakpastian.\n\nTidak cocok utk &gt;5% missing data.\n\n\n7.2.3.1.1 Hands On Coding\nPertama kita akan buat sebuah dataset menggunakan pandas dataframe\n\n# buat dataset dengan format dataframe\ndf = pd.DataFrame({'age': [25, 26, 29, 30, 30, 31, 44, 46],\n                   'IQ': [np.NaN, 121, 91, np.NaN, 110, np.NaN, 118, 93]})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nNaN\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\nNaN\n\n\n4\n30\n110.0\n\n\n5\n31\nNaN\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\nLalu kita akan hitung mean atau rata rata dari dataset\n\nmean = df['IQ'].mean()\nprint(f'Mean: {mean}, dibulatkan menjadi {round(mean)}')\ndisplay(df)\n\nMean: 106.6, dibulatkan menjadi 107\n\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nNaN\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\nNaN\n\n\n4\n30\n110.0\n\n\n5\n31\nNaN\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\nLalu kita masukkan nilai mean tersebut ke data yang kosong\n\n# masukkan nilai mean ke missing value\ndf['IQ'] = df['IQ'].fillna(round(mean))\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n107.0\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\n107.0\n\n\n4\n30\n110.0\n\n\n5\n31\n107.0\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\n\n\n\n7.2.3.2 Arbiter\nTeknik imputasi arbiter (arbiter imputation) adalah metode untuk mengisi nilai yang hilang dalam dataset dengan menggunakan hasil gabungan dari beberapa metode imputasi yang berbeda. Dalam metode ini, beberapa teknik imputasi yang berbeda diterapkan pada dataset yang sama, dan hasil dari setiap teknik imputasi digabungkan menjadi satu nilai yang digunakan untuk mengisi nilai yang hilang.\nKelebihan\n\nSangat mudah dan cepat\n\nCocok untuk missing dataset dengan asumsi tidak missing at random\n\nKekurangan\n\nMengganggu variansi dan distribusi variable original\n\nDapat membentuk outlier\n\nSemakin besar nilai arbitrary, maka semakin besar distorsi\n\n\n7.2.3.2.1 Hands On Coding\nKita akan gunakan dataset yang sama seperti sebelumnya\n\n# buat dataset dengan format dataframe\ndf = pd.DataFrame({'age': [25, 26, 29, 30, 30, 31, 44, 46],\n                   'IQ': [np.NaN, 121, 91, np.NaN, 110, np.NaN, 118, 93]})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nNaN\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\nNaN\n\n\n4\n30\n110.0\n\n\n5\n31\nNaN\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\nCukup masukkan nilai arbiter ke data yang kosong. Dalam koding ini, nilai arbiter adalah 130.\n\ndf['IQ'] = df['IQ'].fillna(130)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n130.0\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\n130.0\n\n\n4\n30\n110.0\n\n\n5\n31\n130.0\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\n\n\n\n7.2.3.3 End Of Tail\nTeknik imputasi end of tail adalah metode untuk mengisi nilai yang hilang dalam dataset dengan menggunakan nilai ekstrem (tail) dari distribusi data yang ada. Metode ini didasarkan pada asumsi bahwa nilai yang hilang cenderung berada di ekor distribusi data. Namun untuk teknik ini kita harus mengikuti sebuah ketentuan khusus, yaitu:\nJika distribusi data bersifat normal, maka gunakan rumus :\n\\[\n{\\sigma = \\sqrt{\\dfrac{\\Sigma |x-\\mu|^2}{N}}}\n\\]\nJika distribusi data bersifat skewed, maka gunakan rumus IQR proximity\n\\[\n{IQR = Q_{3} - Q_{1}}\n\\]\n\n7.2.3.3.1 Distribusi Normal\nData yang terdistribusi secara normal, juga dikenal sebagai distribusi Gaussian atau kurva lonceng, mengacu pada distribusi statistik di mana titik-titik data secara simetris terdistribusi di sekitar nilai rata-rata, menciptakan kurva berbentuk lonceng yang khas.\nUntuk menghitung data yg mempunyai distribusi normal, kita harus mengetahui standar deviasinya. Berikut rumus untuk menghitung standar deviasi :\n\\[\n{\\mu + 3 * \\sigma}\n\\] Keterangan:\nσ = Standar Deviasi\nΣ = jumlah\nx = data yang dihitung\nμ = Mean/rata rata\nN = Jumlah data\n\n\n7.2.3.3.2 Distribusi Normal - Hands On Coding\nPertama kita akan buat sebuah dataset yang cukup besar jika dibandingkan dengan dataset yang diatas.\nJangan lupa untuk menginstall library yang dibutuhkan :\n\npip install numpy\n\npip install pandas\n\npip install matplotlib\n\npip install keras\npip install tensorflow\n\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [85, 90, 95, 95, 100, np.nan, 100, 110, 105, 105, 110,\n        np.nan, 110, 110, 115, 115, 115, 120, np.nan, 125, 130]\n\n# make a dataframe\ndf = pd.DataFrame({'age': age, 'IQ': data})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n85.0\n\n\n1\n26\n90.0\n\n\n2\n29\n95.0\n\n\n3\n30\n95.0\n\n\n4\n30\n100.0\n\n\n5\n31\nNaN\n\n\n6\n44\n100.0\n\n\n7\n46\n110.0\n\n\n8\n22\n105.0\n\n\n9\n33\n105.0\n\n\n10\n35\n110.0\n\n\n11\n27\nNaN\n\n\n12\n21\n110.0\n\n\n13\n23\n110.0\n\n\n14\n45\n115.0\n\n\n15\n47\n115.0\n\n\n16\n41\n115.0\n\n\n17\n38\n120.0\n\n\n18\n37\nNaN\n\n\n19\n21\n125.0\n\n\n20\n24\n130.0\n\n\n\n\n\n\n\nSelanjutnya kita akan mengecek jenis distribusi data menggunakan library matplotlib\n\ndata = [85, 90, 95, 95, 100, np.nan, 100, 110, 105, 105, 110,\n        np.nan, 110, 110, 115, 115, 115, 120, np.nan, 125, 130]\n\nplt.hist(data, bins=5)\nplt.xlabel('Data')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\nDapat dilihat bahwa grafik histogram membentuk seperti gunung atau lonceng, dengan puncak tepat di tengah-tengah grafik. Inilah salah satu karakteristik dataset dengan distribusi normal.\nSelanjutnya, kita akan mencari mean dari dataset\n\n# nilai kosong dihilangkan dari data terlebih dahulu\ndata = [85, 90, 95, 95, 100, 100, 110, 105, 105,\n        110, 110, 110, 115, 115, 115, 120, 125, 130]\nmean = np.mean(data)\nprint(f'Mean dari dataset adalah: ', mean)\n\nMean dari dataset adalah:  107.5\n\n\nLalu, kita akan menghitung jarak antara nilai x dan mean\n\ntotal = 0\nfor i in data:\n    calc = (107.5-i)**2\n    total = total + calc\n\nprint(f'jarak antara nilai x dan mean adalah ', total)\n\njarak antara nilai x dan mean adalah  2412.5\n\n\nMari kita hitung standar deviasi nya\n\n# math.sqrt adalah fungsi untuk melakukan operasi akar pangkat\n# round adalah fungsi untuk membulatkan hasil operasi)\nstddev = round(math.sqrt(total/len(data)), 2)\nprint(f'nilai standar deviasi adalah ', stddev)\n\nnilai standar deviasi adalah  11.58\n\n\nHitung nilai imputasi End of Tail\n\nimp = round(mean + 3 * stddev, 1)\nprint(f'nilai imputasi end of tail adalah ', imp)\n\nnilai imputasi end of tail adalah  142.2\n\n\n\nDari kalkulasi nilai imputasi end of tail, diperoleh nilai 142.2.\nKita akan memasukkan nilai ini ke dalam dataset\n\ndf['IQ'] = df['IQ'].fillna(imp)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n85.0\n\n\n1\n26\n90.0\n\n\n2\n29\n95.0\n\n\n3\n30\n95.0\n\n\n4\n30\n100.0\n\n\n5\n31\n142.2\n\n\n6\n44\n100.0\n\n\n7\n46\n110.0\n\n\n8\n22\n105.0\n\n\n9\n33\n105.0\n\n\n10\n35\n110.0\n\n\n11\n27\n142.2\n\n\n12\n21\n110.0\n\n\n13\n23\n110.0\n\n\n14\n45\n115.0\n\n\n15\n47\n115.0\n\n\n16\n41\n115.0\n\n\n17\n38\n120.0\n\n\n18\n37\n142.2\n\n\n19\n21\n125.0\n\n\n20\n24\n130.0\n\n\n\n\n\n\n\n\n\n7.2.3.3.3 Distribusi Skewed\nDistribusi skew atau skewness mengacu pada karakteristik asimetri dalam distribusi data. Dalam distribusi skew, ekor distribusi data cenderung condong ke salah satu sisi, baik ke kanan (positif) atau ke kiri (negatif), dibandingkan dengan pusat distribusi.\nDalam distribusi skew positif, ekor distribusi condong ke kanan, sementara nilai-nilai yang lebih kecil cenderung berada di sebelah kiri. Ini menghasilkan ekor yang panjang di sisi kanan distribusi.\n\nDalam distribusi skew negatif, ekor distribusi condong ke kiri, dengan nilai-nilai yang lebih besar cenderung berada di sebelah kiri. Ini menghasilkan ekor yang panjang di sisi kiri distribusi. Nilai rata-rata akan lebih kecil daripada median dalam distribusi ini.\n\n\n\n7.2.3.3.4 Distribusi Skewed - Hands On Coding\nPertama-tama mari kita buat data baru yang terdiri dari 21 data dengan 3 missing value. Tugas kita adalah mengisi missing value dengan imputasi end of tail.\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [125, 130, 125, 95, 115, np.nan, 100, np.nan, 130, 110,\n        90, 110, 120, 115, 105, 85, 115, 110, 120, 100, np.nan]\n\ndf = pd.DataFrame({'age': age, 'IQ': data})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n125.0\n\n\n1\n26\n130.0\n\n\n2\n29\n125.0\n\n\n3\n30\n95.0\n\n\n4\n30\n115.0\n\n\n5\n31\nNaN\n\n\n6\n44\n100.0\n\n\n7\n46\nNaN\n\n\n8\n22\n130.0\n\n\n9\n33\n110.0\n\n\n10\n35\n90.0\n\n\n11\n27\n110.0\n\n\n12\n21\n120.0\n\n\n13\n23\n115.0\n\n\n14\n45\n105.0\n\n\n15\n47\n85.0\n\n\n16\n41\n115.0\n\n\n17\n38\n110.0\n\n\n18\n37\n120.0\n\n\n19\n21\n100.0\n\n\n20\n24\nNaN\n\n\n\n\n\n\n\nSelanjutnya kita akan mengecek jenis distribusi data menggunakan library matplotlib\n\ndata = [125, 130, 125, 95, 115, 100, 130, 110, 90,\n        110, 120, 115, 105, 85, 115, 110, 120, 100]\n\nfig = px.histogram(df, x='IQ')\nfig.show()\n\n\n                                                \n\n\nDistribusi data adalah skew negatif karena puncak dari data berada di sebelah kanan titik tengah. Mari kita hitung nilai imputasi menggunakan rumus IQR.\nInter-Quartile-Range (IQR) adalah sebuah nilai yang digunakan untuk mengukur sebaran data dalam sebuah distribusi.\n\\({IQR = Q_{3} - Q_{1}}\\)\n\\({IQR_{max} = Q_{3} + 3 * IQR}\\)\n\\({IQR_{min} = Q_{1} + 3 * IQR}\\)\nPertama-tama, kita akan hitung nilai precentile dari dataset, yang dapat kita kalkulasi dengan mudah menggunakan fungsi np.precentile dari library numpy.\n\nmedian = np.median(data)\nq1 = np.percentile(data, 25)\nq3 = np.percentile(data, 75)\n\nprint(f'Median: {median}')\nprint(f'Q1: {q1}')\nprint(f'Q3: {q3}')\n\nMedian: 112.5\nQ1: 101.25\nQ3: 120.0\n\n\nLalu kita akan menghitung nilai IQR, IQRmin, dan IQRmax\n\niqr = q3 - q1\niqrmin = q1 + 3 * iqr\niqrmax = q3 + 3 * iqr\n\nprint(f'IQR: {iqr}')\nprint(f'IQRmin: {iqrmin}')\nprint(f'IQRmax: {iqrmax}')\n\nIQR: 18.75\nIQRmin: 157.5\nIQRmax: 176.25\n\n\nAnda boleh memilih nilai diantara IQRmin dan IQRmax. Namun untuk contoh ini, kita akan ambil nilai IQRmax saja, lalu kita masukkan ke dataset.\n\ndf['IQ'] = df['IQ'].fillna(iqrmax)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n125.00\n\n\n1\n26\n130.00\n\n\n2\n29\n125.00\n\n\n3\n30\n95.00\n\n\n4\n30\n115.00\n\n\n5\n31\n176.25\n\n\n6\n44\n100.00\n\n\n7\n46\n176.25\n\n\n8\n22\n130.00\n\n\n9\n33\n110.00\n\n\n10\n35\n90.00\n\n\n11\n27\n110.00\n\n\n12\n21\n120.00\n\n\n13\n23\n115.00\n\n\n14\n45\n105.00\n\n\n15\n47\n85.00\n\n\n16\n41\n115.00\n\n\n17\n38\n110.00\n\n\n18\n37\n120.00\n\n\n19\n21\n100.00\n\n\n20\n24\n176.25\n\n\n\n\n\n\n\n\n\n\n7.2.3.4 Regresi Linier\nTeknik imputasi regresi adalah metode untuk mengisi nilai kosong menggunakan algoritma regresi. Algoritma regresi akan memprediksi nilai kosong berdasarkan hubungan fitur nilai kosong dengan fitur lainnya.\nKelebihan\n\nSederhana dan mudah dipahami\n\nMenggabungkan hubungan antar variabel\n\nCocok untuk data yang besar dan bersifat numerik\n\nKekurangan\n\nHanya berlaku untuk data yang linear\n\nSensitif terhadap outlier\n\nTidak dapat menangani data non-numerik\n\nBergantung kepada dua fitur\n\n\n7.2.3.4.1 Regresi Linier - Hands On Coding\nKelemahan utama dari teknik regresi ini adalah dataset harus mempunyai distribusi linear. Mengambil contoh dataset age (umur) dan IQ, Jika dataset tersebut adalah linear, semakin tinggi umur seseorang, maka semakin tinggi IQ orang tersebut.\nMaka dari itu, kita harus mengecek jenis data yang akan kita kerjakan, apakah data tersebut bersifat linear atau tidak. Untuk mengecek dataset, kita gunakan grafik scatter plot.\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [125, 130, 125, 95, 115, np.nan, 100, np.nan, 130, 110,\n        90, 110, 120, 115, 105, 85, 115, 110, 120, 100, np.nan]\n\ndf = pd.DataFrame({'age': age, 'IQ': data})\n\n# hapus data NaN\ndf = df.dropna()\n\n# buat grafik scatter\nfig = px.scatter(df, x='age', y='IQ',\n                 color='age', hover_data=['age', 'IQ'])\nfig.show()\n\n\n                                                \n\n\nSecara garis besar, scatter plot membentuk sebuah garis diagonal dari pojok kiri atas ke pojok kiri bawah.\n\n\n\nGambar9. Data bersifat Linear\n\n\nUntuk menambah keyakinan kita bahwa dataset bersifat linear, kita dapat melakukan pengecekan dataset menggunakan pearson correllation. Kita harus menginstall library scipy terlebih dahulu dengan menjalankan perintah pip install scipy di terminal atau command prompt.\nPearson correlation adalah sebuah rumus yang berfungsi untuk menghitung kekuatan dan arah hubungan antara dua variable. nilai -1 mengindikasikan bahwa korelasi bersifat negatif, semakin kecil nilai variable x, maka nilai variable y akan semakin besar. nilai 0 mengindikasikan bahwa tidak ada korelasi linear antara variable. nilai 1 mengindikasikan bahwa korelasi bersifat positif.\n\ncorr, _ = pearsonr(df['age'], df['IQ'])\nprint(f'Pearsons correlation: {round(corr,2)}')\n\nPearsons correlation: -0.53\n\n\nnilai pearson -0.53 menandakan bahwa hubungan antara variabel umur dan iq adalah linear negatif yang mempunyai intensitas lemah.\nSelanjutnya, kita akan membuat model regresi. Sebelumnya, kita harus menginstall library keras dengan cara menjalankan perintah pip install keras di terminal atau command prompt.\n\nmodel = Sequential()\nmodel.add(Dense(1, input_shape=(1,)))\nmodel.compile(Adam(learning_rate=0.8), 'mean_squared_error')\n\nmodel.fit(df['age'], df['IQ'], epochs=1000, verbose=0)\n\npred = model.predict(df['age'])\n\nplt.scatter(df['age'], df['IQ'])\nplt.plot(df['age'], pred, color='red')\nplt.xlabel('Age')\nplt.ylabel('IQ')\nplt.show()\n\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000002807CDE8C20&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 26ms/step\n\n\n\n\n\nLalu buat hasil prediksi dari model regresi. Nilai yang kosong adalah IQ dengan umur 27, 31, dan 37. Maka kita masukkan ketiga nilai tersebut ke model untuk di prediksi nilai IQ nya.\n\nhasilprediksi = model.predict([27, 31, 37]).round(0).astype(int)\nprint(hasilprediksi)\n\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000002807CDE8C20&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 43ms/step\n\n\n[[115]\n [112]\n [107]]\n\n\nHasil prediksi menunjukkan angka 115, 112, dan 107. Maka kita akan masukkan nilai IQ tersebut kedalam dataset\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [125, 130, 125, 95, 115, np.nan, 100, np.nan, 130, 110,\n        90, 110, 120, 115, 105, 85, 115, 110, 120, 100, np.nan]\n\ndata[5] = hasilprediksi[0][0]\ndata[7] = hasilprediksi[1][0]\ndata[20] = hasilprediksi[2][0]\ndf = pd.DataFrame({'age': age, 'IQ': data})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n125\n\n\n1\n26\n130\n\n\n2\n29\n125\n\n\n3\n30\n95\n\n\n4\n30\n115\n\n\n5\n31\n115\n\n\n6\n44\n100\n\n\n7\n46\n112\n\n\n8\n22\n130\n\n\n9\n33\n110\n\n\n10\n35\n90\n\n\n11\n27\n110\n\n\n12\n21\n120\n\n\n13\n23\n115\n\n\n14\n45\n105\n\n\n15\n47\n85\n\n\n16\n41\n115\n\n\n17\n38\n110\n\n\n18\n37\n120\n\n\n19\n21\n100\n\n\n20\n24\n107\n\n\n\n\n\n\n\n\n\n\n7.2.3.5 Frequent\nImputasi frequent adalah teknik imputasi yang hanya bisa digunakan di jenis data kategorik. Kita mengambil nilai kategorik yang paling sering muncul, dan memasukkannya ke data yang kosong.\nKelebihan\n\nCocok untuk data dengan missing at random.\n\nMudah dan cepat diterapkan.\n\nCocok utk data yang memiliki skew\n\nDapat digunakan dalam produksi (mis. dalam model deployment).\n\nKelemahan\n\nMendistorsi relasi label dengan frekuensi tertinggi vs variabel lain.\n\nMenghasilkan over-representation jika banyak data yang missing.\n\n\n7.2.3.5.1 Frequent - Hands On Coding\nLangkah pertama adalah memuat dataset yang mempunyai jenis data kategorik. Oleh karena itu, fitur IQ kita ganti oleh nilai ‘rendah’, ‘sedang’, dan ‘tinggi’\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\nIQ = ['rendah', 'sedang', 'sedang', np.nan, 'sedang', 'rendah', np.nan, 'tinggi', 'sedang', 'sedang',\n      'rendah', 'sedang', 'tinggi', 'sedang', np.nan, 'rendah', 'tinggi', 'sedang', 'tinggi', 'rendah', 'tinggi']\n\ndf = pd.DataFrame({'age': age, 'IQ': IQ})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nrendah\n\n\n1\n26\nsedang\n\n\n2\n29\nsedang\n\n\n3\n30\nNaN\n\n\n4\n30\nsedang\n\n\n5\n31\nrendah\n\n\n6\n44\nNaN\n\n\n7\n46\ntinggi\n\n\n8\n22\nsedang\n\n\n9\n33\nsedang\n\n\n10\n35\nrendah\n\n\n11\n27\nsedang\n\n\n12\n21\ntinggi\n\n\n13\n23\nsedang\n\n\n14\n45\nNaN\n\n\n15\n47\nrendah\n\n\n16\n41\ntinggi\n\n\n17\n38\nsedang\n\n\n18\n37\ntinggi\n\n\n19\n21\nrendah\n\n\n20\n24\ntinggi\n\n\n\n\n\n\n\nLalu kita hitung frekuensi dari data kategorik\n\nfreq = df['IQ'].value_counts()\nprint(freq)\n\nIQ\nsedang    8\nrendah    5\ntinggi    5\nName: count, dtype: int64\n\n\nKategori sedang mempunyai frekuensi paling tinggi dengan nilai 8. Jadi, nilai imputasi yang akan kita gunakan adalah ‘sedang’\nSelanjutnya, kita masukkan data ‘sedang’ ke dataset.\n\ndf['IQ'] = df['IQ'].fillna('sedang')\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nrendah\n\n\n1\n26\nsedang\n\n\n2\n29\nsedang\n\n\n3\n30\nsedang\n\n\n4\n30\nsedang\n\n\n5\n31\nrendah\n\n\n6\n44\nsedang\n\n\n7\n46\ntinggi\n\n\n8\n22\nsedang\n\n\n9\n33\nsedang\n\n\n10\n35\nrendah\n\n\n11\n27\nsedang\n\n\n12\n21\ntinggi\n\n\n13\n23\nsedang\n\n\n14\n45\nsedang\n\n\n15\n47\nrendah\n\n\n16\n41\ntinggi\n\n\n17\n38\nsedang\n\n\n18\n37\ntinggi\n\n\n19\n21\nrendah\n\n\n20\n24\ntinggi\n\n\n\n\n\n\n\n\n\n\n7.2.3.6 K-Nearest Neighbor (KNN)\nImputasi menggunakan K-Nearest Neighbors (KNN) adalah sebuah metode untuk mengisi data kosong dengan mempertimbangkan nilai terdekat dari fitur lain di kategori yang sama.\nKelebihan\n\nLebih akurat vs mean/median/most frequent.\n\nKekurangan\n\nBiaya komputasi mahal (karena KNN bekerja dengan menyimpan seluruh dataset pelatihan dalam memori).\n-Sensitif terhadap outlier dalam data (tidak seperti SVM).\n\n\n7.2.3.6.1 K-Nearest Neighbor (KNN) - Hands On Coding\nDataset yang kita gunakan sama dengan contoh diatas\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\nIQ = ['rendah', 'sedang', 'sedang', np.nan, 'sedang', 'rendah', np.nan, 'tinggi', 'sedang', 'sedang',\n      'rendah', 'sedang', 'tinggi', 'sedang', np.nan, 'rendah', 'tinggi', 'sedang', 'tinggi', 'rendah', 'tinggi']\n\ndf = pd.DataFrame({'age': age, 'IQ': IQ})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nrendah\n\n\n1\n26\nsedang\n\n\n2\n29\nsedang\n\n\n3\n30\nNaN\n\n\n4\n30\nsedang\n\n\n5\n31\nrendah\n\n\n6\n44\nNaN\n\n\n7\n46\ntinggi\n\n\n8\n22\nsedang\n\n\n9\n33\nsedang\n\n\n10\n35\nrendah\n\n\n11\n27\nsedang\n\n\n12\n21\ntinggi\n\n\n13\n23\nsedang\n\n\n14\n45\nNaN\n\n\n15\n47\nrendah\n\n\n16\n41\ntinggi\n\n\n17\n38\nsedang\n\n\n18\n37\ntinggi\n\n\n19\n21\nrendah\n\n\n20\n24\ntinggi\n\n\n\n\n\n\n\nAlgoritma KNN tidak bisa menghitung data berjenis string, maka kita harus konversi dari string (huruf/kata) ke integer (angka).\n\ndf['IQ'] = df['IQ'].map({'rendah': 1, 'sedang': 2, 'tinggi': 3})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n1.0\n\n\n1\n26\n2.0\n\n\n2\n29\n2.0\n\n\n3\n30\nNaN\n\n\n4\n30\n2.0\n\n\n5\n31\n1.0\n\n\n6\n44\nNaN\n\n\n7\n46\n3.0\n\n\n8\n22\n2.0\n\n\n9\n33\n2.0\n\n\n10\n35\n1.0\n\n\n11\n27\n2.0\n\n\n12\n21\n3.0\n\n\n13\n23\n2.0\n\n\n14\n45\nNaN\n\n\n15\n47\n1.0\n\n\n16\n41\n3.0\n\n\n17\n38\n2.0\n\n\n18\n37\n3.0\n\n\n19\n21\n1.0\n\n\n20\n24\n3.0\n\n\n\n\n\n\n\nLalu kita membuat model KNN. Hasil dari KNN bisa langsung dimasukkan ke dataset.\n\nimputer = KNNImputer(n_neighbors=3)\n# isi missing value dengan KNN, lalu dibulatkan\ndf = pd.DataFrame(np.round(imputer.fit_transform(df)), columns=df.columns)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25.0\n1.0\n\n\n1\n26.0\n2.0\n\n\n2\n29.0\n2.0\n\n\n3\n30.0\n2.0\n\n\n4\n30.0\n2.0\n\n\n5\n31.0\n1.0\n\n\n6\n44.0\n2.0\n\n\n7\n46.0\n3.0\n\n\n8\n22.0\n2.0\n\n\n9\n33.0\n2.0\n\n\n10\n35.0\n1.0\n\n\n11\n27.0\n2.0\n\n\n12\n21.0\n3.0\n\n\n13\n23.0\n2.0\n\n\n14\n45.0\n2.0\n\n\n15\n47.0\n1.0\n\n\n16\n41.0\n3.0\n\n\n17\n38.0\n2.0\n\n\n18\n37.0\n3.0\n\n\n19\n21.0\n1.0\n\n\n20\n24.0\n3.0\n\n\n\n\n\n\n\n\n\n\n7.2.3.7 Kesimpulan\n\nSebelum melakukan imputasi kita harus mengetahui jenis missing data, tipe data, dan distribusi data\n\nTidak ada metode imputasi yang sempurna, masing masing teknik mempunyai kelebihan dan kelemahan yang unik\n\nDistribusi data sangat berpengaruh terhadap efisiensi imputasi\n\nImputasi dapat dilakukan menggunakan function dari library sklearn, feature_engine, dan keras. Namun akan jauh lebih baik jika kita menghitung/coding secara manual untuk mengetahui cara kerja algoritma tsb sebelum menggunakan function dari library."
  },
  {
    "objectID": "modul_7.html#handling-outlier",
    "href": "modul_7.html#handling-outlier",
    "title": "7  Transformasi Data",
    "section": "7.3 Handling Outlier",
    "text": "7.3 Handling Outlier\nOutlier adalah sebuah data yang mempunyai pola atau letak yang menyimpang sangat jauh dari rata rata dataset atau pola yang diharapkan dari sebuah dataset.\nOutlier dapat terjadi karena berbagai alasan, seperti kesalahan pengukuran, kesalahan entri data, variasi alami, atau peristiwa langka. Outlier dapat memiliki dampak signifikan pada analisis statistik, model pembelajaran mesin, dan interpretasi data, yang dapat mengarah pada hasil yang bias atau kesimpulan yang tidak akurat.\nOutlier harus kita atasi agar data bisa kita proses secara efisien.\n\n7.3.1 Deteksi Outlier\nOutlier dapat dideteksi menggunakan beberapa metode, antara lain\n\n7.3.1.1 Visualisasi\nAdalah sebuah teknik untuk memvisualisasikan sebuah data menjadi suatu bentuk yang dapat dilihat secara menyeluruh sehingga kita dapat menganalisa bentuk data, ukuran data, data point dan mendeteksi outlier.\nBeberapa bentuk visualisasi yang sering digunakan untuk mendeteksi outlier yaitu - Histogram\n- Scatter plot\n- Box plot\nKita akan menggunakan library plotly express untuk melakukan visualisasi, jadi jangan lupa untuk menginstall plotly dengan menjalankan pip install plotly di terminal atau cmd\nDataset yang kita gunakan adalah dataset lagu yang diambil dari spotify. Sumber dari dataset adalah https://www.kaggle.com/datasets/vatsalmavani/spotify-dataset\nNamun karena dataset ukurannya sangat besar, maka kita akan gunakan subset dataset berjumlah 100 data yang diambil secara random. Dataset versi ini dapat di download di https://github.com/rif42/AssociateDataScientist/blob/master/Module7-AssociateDataScientist/data_sampled_100.csv\nJika sudah di download, masukkan data ke dalam directory lalu muat dataset tersebut.\ndata = pd.read_csv('data_sampled_100.csv')\ndata.head()\nDataset ini berisi lagu-lagu yang ada di Spotify. Setiap lagu mempunyai fitur yang unik seperti popularitas, durasi, loudness, acousticness, speechiness, danceability, dll. Anda bisa melihat deskripsi data secara detail di https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features\nSelanjutnya kita akan lakukan proses visualisasi data menggunakan plot histogram dengan library plotly. Fitur yang kita visualisasikan adalah loudness atau volume lagu terhadap popularitas dari lagu yang ada di dataset.\nBerikut diagram scatter plot dari dataset:\nfig = px.scatter(data, x='loudness', y='popularity', color='loudness',\n                 hover_data=['artists', 'name', 'loudness', 'popularity'])\nfig.show()\nBerikut diagram histogram dari dataset:\nfig = px.histogram(data, x='loudness')\nfig.show()\nBerikut diagram box plot dari dataset:\nfig = px.box(data, x='loudness')\nfig.show()\nAda beberapa data yang bisa disebut outlier, salah satunya adalah lagu Thursday Afternoon - 2005 digital remaster, oleh Brian Eno yang mempunyai nilai loudness -31.8808. Alasannya adalah tingkat loudness nya jauh lebih kecil daripada yang lain.\n\n\n\n7.3.2 Kategori Outlier\nVariate dan univariate adalah dua jenis data dalam statistik. Outlier adalah observasi atau nilai yang secara signifikan berbeda dari pola atau pola umum data yang lain.\n\n7.3.2.1 Variate Data\nVariate data merujuk pada set data yang terdiri dari beberapa variabel atau fitur. Contoh umum variate data adalah dataset yang terdiri dari beberapa kolom, di mana setiap kolom mewakili variabel yang berbeda. Misalnya, jika kita memiliki dataset tentang mahasiswa yang mencakup variabel seperti tinggi, berat badan, dan usia, maka kita memiliki variate data. Outlier dalam variate data merujuk pada observasi atau nilai yang di luar kisaran yang diharapkan dalam setidaknya satu variabel.\n\n\n7.3.2.2 Univariate Data\nUnivariate data merujuk pada set data yang hanya memiliki satu variabel atau fitur. Contoh umum univariate data adalah dataset yang hanya terdiri dari satu kolom, seperti data tinggi badan seseorang. Outlier dalam univariate data merujuk pada observasi atau nilai yang sangat ekstrem atau jauh dari rentang nilai yang diharapkan.\ndf = pd.DataFrame({'age': [25, 26, 29, 30, 30, 31, 44, 46],\n                   'IQ': [np.NaN, 121, 91, np.NaN, 110, np.NaN, 118, 93]})\ndisplay(df)\nMasih ingat contoh data di imputasi data diatas? Data berisi fitur umur dan nilai IQ. Masing masing fitur hanya mempunyai 1 buah nilai. Inilah yang dimaksud dengan univariate data; data di dalam fitur hanya mempunyai satu jenis nilai.\n\n\n\n7.3.3 Mengatasi Outlier\nTidak semua outlier harus dihapus atau dihilangkan. Pada contoh data lagu di spotify, meskipun letaknya jauh dari yang lain,data ini masih valid sebagai lagu. Karena adalah lagu Thursday Afternoon - 2005 digital remaster, oleh Brian Eno mempunyai genre instrumental/ambient. Umumnya, tujuan utama genre ini adalah sebagai music background, sehingga tidak diperlukan vokal atau melodi yang keras.\nNamun terkadang outlier harus dihilangkan atau dihapus karena nilainya terlalu jauh dengan yang lain, yang dapat merubah value dari dataset secara keseluruhan, lalu dapat merubah hasil dari training data.\n\n7.3.3.1 Discretization/Binning\nDiscretization atau binning adalah proses mengubah data kontinu menjadi data diskrit dengan cara membagi rentang nilai kontinu menjadi beberapa interval atau kelompok yang disebut “bin” atau “bucket”. Tujuan utama dari discretization adalah mengurangi kompleksitas data kontinu dengan mengelompokkan nilainya ke dalam kategori atau range tertentu.\n\n\n\nGambar10. Grafik Proses Binning Dataset\n\n\nKelebihan - Dapat diterapkan pada data kategorik dan numerik.\n- Model lebih robust dan mencegah overfitting.\nKekurangan - Meningkatnya biaya kinerja perhitungan.\n- Mengorbankan informasi.\n- Untuk kolom data numerik, dapat menyebabkan redudansi untuk beberapa algoritma.\n- Untuk kolom data kategorik, label dengan frekuensi rendah berdampak negatif pada robustness model statistik.\n\n7.3.3.1.1 Discretization/Binning - Hands On Coding\nKita akan menggunakan dataset lagu spotify dengan fitur loudness untuk melakukan proses binning. Binning dilakukan menggunakan fungsi pd.cut. Fungsi ini akan membagi semua nilai yang ada di dalam sebuah fitur menjadi 3 (atau angka lain yang anda inginkan)\n# muat data\ndf = pd.read_csv('./data_sampled_100.csv')\n\n# binning data menjadi 5 kategori\ndf['loudness'] = pd.cut(df['loudness'], bins=3, labels=[\n                        'sunyi', 'standar', 'bising'])\n\n# urutkan data berdasarkan fitur loudness\ndf_sorted = df.sort_values(by='loudness')\n\n# visualisasikan data menggunakan histogram\nfig = px.histogram(df_sorted, x='loudness')\nfig.show()\n\n\n\n7.3.3.2 Trimming\nTrimming adalah proses penghapusan data yang dianggap sebagai outlier. Trimming biasanya dilakukan berdasarkan presentase data yang akan di trim, contohnya 5%.\n\nKelebihan\n- Cepat dan mudah\n- Dapat memperbaiki rata-rata data\nKekurangan\n- Hilangnya data yang dapat mengandung informasi\n- Dapat menyebabkan bias terhadap variansi data\n\n7.3.3.2.1 Trimming - Hands On Coding\nSangat penting untuk melakukan proses trimming hanya pada data tidak dibutuhkan/invalid. Oleh karena itu, proses trimming seharusnya dimulai dari angka yang sangat kecil, semisal 0.0001%. Jika outlier masih nampak, maka kita tambah presentasenya sedikit demi sedikit.\nDataset yang kita gunakan masih sama, yaitu lagu spotify dengan fitur loudness. Namun karena ukuran dataset kita cukup kecil, maka kita bisa gunakan presentase trimming 1%.\nPertama kita muat data, lalu kita sortir dataset.\ndf = pd.read_csv('./data_sampled_100.csv')\ndf_sorted = df.sort_values(by='loudness')\ndf.head()\nSelanjutnya, kita ambil 1% dari data tersebut, hanya dari tail atau ujung belakang dari dataset. Nilai ini nantinya akan menjadi batas dari trimming kita.\nq = df['loudness'].sort_values().quantile(0.01)\nprint(q)\nJadi batas dari trimming kita adalah -27.3332. Selanjutnya kita akan hilangkan data yang melebihi nilai ini.\ndf = df[df['loudness'] &gt; q]\nfig = px.scatter(df, x='loudness', y='popularity', color='loudness',\n                 hover_data=['artists', 'name', 'loudness', 'popularity'])\nfig.show()\nBisa dilihat bahwa lagu Thursday Afternoon - 2005 digital remaster, oleh Brian Eno yang mempunyai loudness kurang dari batas trimming sudah hilang.\n\n\n\n7.3.3.3 Winsorizing\nWinsorizing adalah proses penggantian data outlier dengan nilai-nilai yang berada dalam distribusi yang ditentukan.\nKelebihan\n- Mempertahankan informasi\n- Mengurangi efek dari outlier\n- Mudah diimplementasikan\nKekurangan\n- Dapat menghasilkan bias\n- Pemilihan presentil dapat mempengaruhi hasil analisis data\n\n7.3.3.3.1 Winsorizing - Hands On Coding\nKita akan menggunakan dataset dan fitur yang sama, yaitu lagu spotify dengan fitur loudness. Pertama kita akan muat data, lalu tentukan batas data yang akan kita winsorize. Kita akan menggunakan nilai yang sama dengan metode sebelumnya(trimming) yaitu 1% dan batas trimming -27.3332.\ndf = pd.read_csv('./data_sampled_100.csv')\ndf_sorted = df.sort_values(by='loudness')\ndf.head()\nPerbedaan utama antara trimming dan winsorizing adalah, di proses trimming, nilai dibawah batas dihilangkan, namun di proses winsorizing, nilai dibawah batas digantikan dengan nilai diantara quartil 1 dan quartil 3.\nUntuk itu, kita akan menggunakan box plot karena box plot sudah memberikan nilai high fence dan low fence secara langsung.\nfig = px.box(df, x='loudness')\nfig.show()\nSelanjutnya, kita akan menghitung berapa banyak data yang ada dibawah batas trimming\nq = df['loudness'].sort_values().quantile(0.01)\noutlier = df[df['loudness'] &lt; q]\noutlier\nDiketahui ada 1 buah outlier. Selanjutnya kita akan menentukan nilai q1 dan q3 sebagai batasan nilai winsorizing kita.\nq1 = np.percentile(df['loudness'], 25)\nq3 = np.percentile(df['loudness'], 75)\nprint(f'q1 = ', q1)\nprint(f'q3 = ', q3)\nNilai q1 dan q3 ini akan kita gunakan untuk memberi batasan terhadap angka random yang akan kita generate sebagai nilai winsorizing kita.\n\n# nilai n menyesuaikan jumlah data outlier\noutliercount = len(outlier)\n\n# generate random number\nrandom.seed(time.time_ns())\nwinsorized_outlier = [random.randint(int(q1), int(q3))\n                      for i in range(outliercount)]\nprint(winsorized_outlier)\nIngat, nilai ini di generate secara random, jadi nilai akan berubah setiap code di run.\nSelanjutnya, kita akan memasukkan nilai winsorizing ke outlier, lalu masukkan outlier ke dataset kita.\nfor i in range(len(winsorized_outlier)):\n    outlier.iloc[i, 12] = winsorized_outlier[i]\n\n# masukkan outlier ke dataset induk\nfor i in range(len(outlier)):\n    id = outlier.iloc[i, 8]  # ambil ID dari data outlier\n    # cari index dari data outlier di dataset\n    index = df[df['id'] == id].index[0]\n    df.iloc[index, 12] = outlier.iloc[i, 12]  # gantikan dataset dengan outlier\n\ndf = df.sort_values(by='loudness')\ndisplay(df)\n\n\n\n7.3.3.4 Imputing\nImputing adalah proses penggantian data outlier dengan nilai-nilai yang diprediksi atau diestimasi berdasarkan karakteristik data. Teknik imputasi sudah di bahas secara detail di bab sebelumnya\nKelebihan\n- Mempertahankan informasi dan ukuran sampel\n- Meningkatkan akurasi analisis\nKekurangan\n- Pemilihan metode dan implementasi bisa cukup sulit\n- Berpotensi merusak distribusi data\n\n\n7.3.3.5 Normalization\nAdalah metode untuk mengubah skala nilai dalam dataset sehingga nilainya berkisar antara 0 dan 1. Untuk melakukan normalisasi data, kita membagi data berdasarkan nilai minimum dan maksimum dari data. Proses normalisasi baik digunakan untuk dataset yang mempunyai distribusi data non-normal atau tidak beraturan.\nRumus dari normalisasi adalah:\n\\[\nnormalized_x = \\dfrac{x - min(x)}{max(x)-min(x)}\n\\]\nKelebihan\n- Mempertahankan informasi dan ukuran sampel\n- Meningkatkan akurasi analisis\n- Mudah diimplementasikan\nKekurangan\n- Metode harus sesuai dengan karakteristik data\n- Tidak menghilangkan outlier secara langsung, hanya mengurangi efek dari outlier\n\n7.3.3.5.1 Normalization - Hands On Coding\nKita akan menggunakan dataset lagu spotify dengan fitur loudness, mirip seperti bab-bab sebelumnya.\nPertama kita cek batasan-batasan data dari fitur loudness\ndf = pd.read_csv('./data_sampled_100.csv')\ndf['loudness'].describe()\nDiketahui bahwa batas minimum data adalah 31.808000, dan batas maksimum data adalah -2.478000\nBatas-batas data ini akan kita ubah menjadi 0 - 1 menggunakan metode normalization. Sebelumnya, kita cek distribusi dari data menggunakan plot histogram.\nfig = px.histogram(df, x='loudness')\nfig.show()\nLalu kita akan gunakan function minmax scaler untuk melakukan proses normalisasi terhadap data loudness.\n\n# buat objek scaler\nscaler = MinMaxScaler()\n\n# transformasi data tempo menggunakan objek scaler\ndf['loudness'] = scaler.fit_transform(df[['loudness']])\n\n# grafik histogram untuk fitur tempo\nfig = px.histogram(df, x='loudness')\nfig.show()\nSelanjutnya, kita cek batas-batas data dari data yang sudah di normalisasi.\ndf['loudness'].describe()\nData telah ter-normalisasi! Namun, apa efek dari normalisasi selain mengubah rentang data? Efek yang paling umum adalah jarak data dari kedua ujung menjadi lebih sama-rata. Hal ini dapat menambah akurasi model machine learning.\n\n\n\n7.3.3.6 Standarization / Z-Score\nStandarisasi data adalah suatu proses dalam analisis data yang mengubah variabel-variabel menjadi memiliki rata-rata nol dan standar deviasi satu. Dalam standarisasi data, setiap nilai data dikurangi dengan rata-rata dari seluruh data, kemudian hasilnya dibagi dengan standar deviasi data. Dengan melakukan hal ini, nilai-nilai data akan berada pada skala yang relatif terhadap variabilitas data. Proses standarisasi menggunakan rumus sebagai berikut:\n\\[\nstandardized_x = \\dfrac{x - mean(x)}{standard deviation(x)}\n\\]\nKelebihan\n- Mean dan standar deviasi tidak berubah\n- Tidak sensitif terhadap outlier\nKekurangan\n- Tidak dapat menentukan batasan data\n- Hasil dapat berupa angka negatif\n\n7.3.3.6.1 Standardization / Z-score - Hands On Coding\nKita akan menggunakan dataset lagu spotify dengan fitur loudness, mirip seperti bab-bab sebelumnya.\nPertama kita cek batasan-batasan data dari fitur loudness\ndf = pd.read_csv('./data_sampled_100.csv')\ndf['loudness'].describe()\nfig = px.histogram(df, x='loudness')\nfig.show()\nLalu kita akan gunakan function z-score scaler untuk melakukan proses normalisasi terhadap data loudness.\nmean = round(np.mean(df['loudness']), 2)\nstd = round(np.std(df['loudness']), 2)\nnormalized = round((df['loudness']-mean)/std, 2)\n\nprint(f'mean = ', mean)\nprint(f'std deviation = ', std)\nnormalized.describe()\nSelanjutnya kita buat diagram histogramnya untuk mengecek perubahan distribusi data.\nfig = px.histogram(normalized, x=\"loudness\")\nfig.show()\nJika kita bandingkan histogram data asli dengan data yang di standarisasi, distribusi data tidak berbeda jauh. Selain itu, batas data minimal mengecil dari -31 menjadi -3, dan batas data maksimal membesar dari -2 menjadi 1."
  },
  {
    "objectID": "modul_7.html#dokumentasi-fitur",
    "href": "modul_7.html#dokumentasi-fitur",
    "title": "7  Transformasi Data",
    "section": "7.3 Dokumentasi Fitur",
    "text": "7.3 Dokumentasi Fitur\nDokumentasi data dapat menjembatani kesenjangan antara transaksi (pembuatan data) dan analisis (konsumsi data). Dokumentasi data yang baik memungkinkan pengguna, ataupun rekan tim untuk memahami siapa/apa/kapan/di mana/bagaimana/mengapa data tersebut dibentuk ataupun dikonsumsi.\nSecara umum, dokumentasi fitur dari sebuah dataset mempunyai beberapa poin, yaitu :\n\nNama fitur\n\nDefinisi\n\nTipe data\n\nSkala\n\nSumber data\n\nKeterangan\n\nMari kita lakukan sebuah dokumentasi fitur dari dataset spotify yang kita gunakan di slide sebelumnya. Dengan menggunakan code data.info() kita bisa mengetahui banyak hal tentang dataset kita\nLink Dataset : Here\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 19 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   valence           100 non-null    float64\n 1   year              100 non-null    int64  \n 2   acousticness      100 non-null    float64\n 3   artists           100 non-null    object \n 4   danceability      100 non-null    float64\n 5   duration_ms       100 non-null    int64  \n 6   energy            100 non-null    float64\n 7   explicit          100 non-null    int64  \n 8   id                100 non-null    object \n 9   instrumentalness  100 non-null    float64\n 10  key               100 non-null    int64  \n 11  liveness          100 non-null    float64\n 12  loudness          100 non-null    float64\n 13  mode              100 non-null    int64  \n 14  name              100 non-null    object \n 15  popularity        100 non-null    int64  \n 16  release_date      100 non-null    object \n 17  speechiness       100 non-null    float64\n 18  tempo             100 non-null    float64\ndtypes: float64(9), int64(6), object(4)\nmemory usage: 15.0+ KB\n\n\n\n7.3.1 Nama Fitur\nFitur atau kolom adalah satuan data yang mendeskripsikan aspek tertentu dalam data. Di dalam dataset ini, kita mempunyai beberapa fitur, antara lain :\n\nValence\n\nYear\n\nAcousticness\n\nArtists\n\nDanceability\n\nDuration_ms\n\nEnergy\n\nExplicit\n\nID\n\nInstrumentalness\n\nKey\n\nLiveness\n\nLoudness\n\nMode\n\nName\n\nPopularity\n\nRelease_date\n\nSpeechiness\n\nTempo\n\n\n\n7.3.2 Definisi Fitur\nSetiap fitur mempunyai definisi atau penjelasan makna tertentu. Definisi yang ada di bawah hanya sebagian dari dataset saja. Untuk definisi keseluruhan data bisa dilihat di https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features.\nBeriku definisi fitur dari dataset :\n\nname, Nama lagu\n\nartists, Nama artis pembuat lagu\n\nyear, Tahun rilis lagu\n\npopularity, Tingkat popularitas lagu. Nilai popularitas adalah antara 0 dan 100, dengan 100 menjadi nilai maksimal.\n\nkey, adalah kunci lagu dari lagu tersebut. kunci lagu direpresentasikan menggunakan angka menurut Pitch Class Notation Standard. Misalnya, 0 = C, 1 = C♯/D♭, 2 = D, dan seterusnya. Jika tidak ada kunci, maka nilai -1.\n\nmode, Mode dari lagu tersebut. 1 = Major, 0 = Minor.\n\n\n\n7.3.3 Tipe Data\nTipe data sangatlah penting karena tipe data menentukan operasi atau function apa yang bisa dilakukan terhadap data tersebut. Sebagai contoh, untuk menentukan nilai random, kita tidak bisa menggunakan angka float (pecahan), kita harus menggunakan angka integer (bulat).\nKita dapat mengecek tipe data dari sebuah dataset menggunakan function data.info()\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 19 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   valence           100 non-null    float64\n 1   year              100 non-null    int64  \n 2   acousticness      100 non-null    float64\n 3   artists           100 non-null    object \n 4   danceability      100 non-null    float64\n 5   duration_ms       100 non-null    int64  \n 6   energy            100 non-null    float64\n 7   explicit          100 non-null    int64  \n 8   id                100 non-null    object \n 9   instrumentalness  100 non-null    float64\n 10  key               100 non-null    int64  \n 11  liveness          100 non-null    float64\n 12  loudness          100 non-null    float64\n 13  mode              100 non-null    int64  \n 14  name              100 non-null    object \n 15  popularity        100 non-null    int64  \n 16  release_date      100 non-null    object \n 17  speechiness       100 non-null    float64\n 18  tempo             100 non-null    float64\ndtypes: float64(9), int64(6), object(4)\nmemory usage: 15.0+ KB\n\n\nDefinisi dari tipe data di dataset :\n- float, angka pecahan\n- int, angka bulat\n- object, kombinasi antara teks dan angka\n\n\n7.3.4 Skala Data\nSkala atau rentang data adalah batasan-batasan data yang ada di sebuah dataset. Skala data bisa diperoleh menggunakan kode data.describe()\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\ndanceability\nduration_ms\nenergy\nexplicit\ninstrumentalness\nkey\nliveness\nloudness\nmode\npopularity\nspeechiness\ntempo\n\n\n\n\ncount\n100.000000\n100.000000\n100.000000\n100.000000\n1.000000e+02\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n\n\nmean\n0.506688\n1979.040000\n0.522918\n0.537518\n2.762373e+05\n0.461174\n0.120000\n0.164812\n4.940000\n0.180223\n-11.769480\n0.730000\n34.540000\n0.110832\n119.779170\n\n\nstd\n0.284391\n25.147613\n0.354783\n0.185604\n3.724344e+05\n0.253916\n0.326599\n0.302744\n3.716792\n0.155208\n5.685826\n0.446196\n21.180046\n0.194693\n31.438102\n\n\nmin\n0.029800\n1926.000000\n0.000175\n0.091800\n6.240000e+04\n0.005740\n0.000000\n0.000000\n0.000000\n0.040300\n-31.808000\n0.000000\n0.000000\n0.024500\n62.106000\n\n\n25%\n0.291500\n1960.750000\n0.207750\n0.399500\n1.671165e+05\n0.254000\n0.000000\n0.000000\n1.000000\n0.093325\n-15.430750\n0.000000\n19.000000\n0.033700\n94.762500\n\n\n50%\n0.521000\n1981.500000\n0.596000\n0.551000\n2.015920e+05\n0.465000\n0.000000\n0.000137\n5.000000\n0.124500\n-11.280000\n1.000000\n36.500000\n0.042450\n115.329000\n\n\n75%\n0.758250\n1999.500000\n0.821000\n0.681000\n2.804202e+05\n0.631500\n0.000000\n0.141500\n8.000000\n0.199750\n-7.177250\n1.000000\n48.250000\n0.066475\n137.057250\n\n\nmax\n0.977000\n2019.000000\n0.993000\n0.880000\n3.650800e+06\n0.994000\n1.000000\n0.921000\n11.000000\n0.967000\n-2.478000\n1.000000\n73.000000\n0.957000\n205.917000\n\n\n\n\n\n\n\n\n\n7.3.5 Sumber Data\nData diperoleh dari kaggle dengan link https://www.kaggle.com/datasets/vatsalmavani/spotify-dataset\nKaggle adalah platform dataset open source. Namun dataset spotify ini dapat di scrape secara mandiri dengan mendaftar sebagai developer di spotify dan mengambil data menggunakan API spotify.\nSetelah di download data diambil 100 baris secara random untuk meningkatkan performa dari coding.\n\n\n7.3.6 Keterangan\nData mempunyai beberapa lagu yang tidak valid, jadi harus dilakukan proses data cleaning."
  },
  {
    "objectID": "modul_7.html#data-training",
    "href": "modul_7.html#data-training",
    "title": "7  Transformasi Data",
    "section": "8.1 Data Training",
    "text": "8.1 Data Training\nMachine learning dibagi menjadi dua, yaitu supervised dan unsupervised learning.\nUnsupervised learning menggunakan dataset tanpa label untuk menemukan sebuah pola, struktur atau hubungan antar fitur untuk melatih algoritma machine learning.\nSupervised learning menggunakan dataset yang mempunyai label untuk melatih algoritma machine learning memprediksi nilai atau mengklasifikasikan sesuatu.\nNamun, tidak semua dataset mempunyai label. Dan tidak semua dataset yang tidak mempunyai label dapat di training menggunakan unsupervised learning. Jadi, kita harus melakukan proses labeling dataset.\n\n\n\nGambar12. Pelabelan data gambar\n\n\nKetika sebuah dataset diberi sebuah label, maka label tersebut sebagai dasar kebenaran atau ground truth.\nPelabelan dataset sangatlah penting untuk proses machine learning. Proses labeling data dapat mempengaruhi kualitas dan akurasi dari sebuah model.\nSelain itu, pelabelan dataset dapat meningkatkan kualitas dari dataset, sehingga dataset dapat diproses oleh lebih banyak algoritma machine learning.\nContoh lain dari pelabelan dataset deteksi email spam :\n\nKesimpulannya, jika pelabelan dataset tidak akurat, maka hasil prediksi yang dibuat oleh algoritma machine learning tidak akan akurat"
  },
  {
    "objectID": "modul_7.html#metode-pelabelan-data",
    "href": "modul_7.html#metode-pelabelan-data",
    "title": "7  Transformasi Data",
    "section": "8.2 Metode Pelabelan Data",
    "text": "8.2 Metode Pelabelan Data\nData labeling adalah langkah yang sangat penting dalam proses pengembangan model machine learning. Proses ini terlihat simpel, namun sebenarnya tidak semudah itu untuk diimplementasikan. Sebagai data scientist/data engineer, kita harus mempertimbangkan semua faktor dan metode yang ada untuk menentukan implementasi yang terbaik. Seperti semua hal di dunia programming, setiap metode pelabelan data mempunyai kelebihan dan kekurangan. Berikut beberapa metode pelabelan data yang sering digunakan :\n\n8.2.1 Internal Labeling\nProses labeling data oleh data scientist, data engineer, atau staf expert lain yang bekerja di perusahaan.\nKekuatan\n- Kualitas labeling tinggi\n- Akurasi labeling tinggi\n- Kemanan data terkontrol\nKelemahan\n- Membutuhkan waktu dan tenaga yang banyak\n- Cukup mahal\n\n\n8.2.2 Synthetic labeling\nProses labeling data yang dilakukan dengan men-generate data baru (dengan pattern yang mirip) dari data yang sudah ada. Contohnya adalah imagen (image generator) dimana sebuah gambar diputar dan digeser sehingga menghasilkan data baru. Proses ini biasanya dilakukan untuk melatih sebuah model machine learning\nKekuatan\n- Cepat\n- Tidak membutuhkan tenaga yang banyak\nKelemahan\n- Tidak semua algoritma bisa menerima semua jenis generated data\n\n\n8.2.3 Programmatic labeling\nProses labeling data yang dilakukan oleh algoritma atau mesin, umumnya algoritma machine learning berbentuk klasifikasi, clustering, atau regresi.\nKekuatan\n- Proses labeling cepat dan dapat diskalakan dengan mudah\n- Tidak membutuhkan tenaga yang banyak\nKelemahan\n- Kualitas labeling tergantung pada kualitas dataset dan penerapan algoritma machine learning\n- Cukup mahal tergantung dengan skala dan penggunaan hardware\n- Proses training model bisa lama\n\n\n8.2.4 Outsourcing\nOutsourcing adalah kegiatan merekrut perusahaan atau organisasi untuk melakukan proses data labeling. Salah satu pertimbangan utama adalah spesialisasi dan kompetensi dari sebuah perusahaan atau organisasi tersebut.\nKekuatan\n\nSimple\n\nHasil labeling akurat\n\nKelemahan\n\nMahal\n\nMemakan waktu lama\n\n\n\n8.2.5 Crowdsourcing\nCrowdsourcing adalah proses dimana pekerjaan data labeling didistribusikan ke khalayak publik, umumnya freelancer melalui platform website. Contohnya adalah project ReCaptcha dimana manusia diminta untuk mengisi captcha, lalu hasilnya dimasukkan ke algoritma machine learning untuk memprediksi captcha.\nKekuatan\n\nPaling efisien, dapat mengolah banyak data dalam waktu singkat\n\nKelemahan\n\nPerlu mengembangkan workflow yang cocok untuk freelancer\n\nAkurasi tergantung dengan QA dan workflow\n\nPotensi kebocoran data tinggi"
  },
  {
    "objectID": "modul_7.html#penggunaan-data-labeling-dalam-pengolahan-citra",
    "href": "modul_7.html#penggunaan-data-labeling-dalam-pengolahan-citra",
    "title": "7  Transformasi Data",
    "section": "8.3 Penggunaan Data Labeling dalam Pengolahan Citra",
    "text": "8.3 Penggunaan Data Labeling dalam Pengolahan Citra\nPengolahan citra adalah kegiatan memanipulasi data yang berbentuk gambar atau video. Di bidang machine learning, pengolahan citra adalah salah satu bidang yang paling penting.\nModel machine learning pengolah citra yang sering dibuat meliputi: klasifikasi, segmentasi, deteksi objek, dan estimasi pose. Semua proses pembuatan model machine learning sangat erat kaitannya dengan pelabelan data yang digunakan di data training.\nTujuan akhir dari semua model yang telah disebutkan adalah melabeli sebuah gambar atau video dengan sesuatu.\n\n8.3.1 Image Classification\n\nImage classification model adalah jenis model dalam bidang pengolahan citra yang digunakan untuk mengklasifikasikan gambar ke dalam berbagai kategori atau kelas yang telah ditentukan sebelumnya. Tujuan dari model ini adalah untuk mengidentifikasi dan memprediksi kelas atau label yang sesuai dengan gambar yang diberikan.\n\n\n8.3.2 Image Segmentation\n\nImage segmentation adalah proses dalam pengolahan citra yang membagi atau memisahkan gambar menjadi beberapa bagian/segmen yang lebih kecil. Setiap segmen mewakili area yang memiliki karakteristik visual yang serupa, seperti warna, tekstur, atau bentuk. Tujuan dari image segmentation adalah untuk memahami struktur internal gambar dan mengidentifikasi objek yang ada di dalamnya.\nKemiripan antara segmentasi dan klasifikasi adalah objek atau hasil prediksi hanya mempunyai satu nilai\n\n\n8.3.3 Object Detection\n\nDeteksi objek adalah proses dalam pengolahan citra yang bertujuan untuk mengidentifikasi dan memetakan objek-objek tertentu dalam sebuah gambar atau video. Tujuan utama dari deteksi objek adalah untuk menemukan dan menandai lokasi serta batas-batas objek yang ada dalam sebuah gambar.\nPerbedaan object detection dengan metode-metode sebelumnya yaitu objek atau hasil prediksi mempunyai lebih dari satu hasil\n\n\n8.3.4 Pengolahan Citra - Hands On Coding\nKita akan membuat model object detection menggunakan library OpenCV. Kita juga akan menggunakan algoritma classifier bernama Haar Cascade. Untuk file lengkapnya bisa dilihat di sini link paper.\nAlgoritma ini menggunakan sebuah sistem machine learning dimana kita memberikan gambar positif dan negatif. Gambar positif adalah gambar objek yang kita ingin klasifikasikan, sedangkan gambar negatif adalah gambar objek yang tidak ingin kita klasifikasikan.\nKarena keterbatasan waktu, kita akan menggunakan model yang sudah jadi untuk melakukan deteksi objek pada gambar yang kita inginkan. Pada kasus ini, kita akan mendeteksi rambu lalu lintas stop.\n\n8.3.4.1 Download Library\nLibrary bisa di download menggunakan pip install opencv-python dan pip install matplotlib\n\n\n8.3.4.2 Import Library\nbuat file python dan import package-package sebagai berikut:\n\nimport cv2\n\nimport matplotlib.pyplot as plt\n\n\n\n8.3.4.3 Download Model dan Gambar Contoh\nDownload pre-trained model yang mempunyai format .xml di link ini. Jangan lupa download gambar stop sign.\n\n\n\nGambar17. Contoh gambar stop sign\n\n\n\n\n8.3.4.4 Ekstrak Model dan Gambar ke Directory\n\n\n8.3.4.5 Tampilkan Gambar Menggunakan OpenCV dan Matplotlib\n\n# buka gambar menggunakan opencv\nimg = cv2.imread(\"image.jpg\")\n\n# OpenCV membuka gambar menggunakan metode BRG (blue red green)\n# namun kita ingin membuka dengan metoded RGB (red green blue)\n# kita harus konversi dari BRG ke RGB\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# kita juga membutuhkan gambar versi grayscale (hitam putih)\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# menampilkan gambar menggunakan matplotlib\nplt.subplot(1, 1, 1)\nplt.imshow(img_rgb)\nplt.show()\n\n\n\n\n\n\n8.3.4.6 Buat Algoritma Deteksi Objek\n\n# load model yang sudah dilatih untuk mendeteksi stop sign\n# model berbentuk file xml\nstop_data = cv2.CascadeClassifier('stop_data.xml')\n\n# buat ukuran kotak minimum agar ukuran kotak yang terdeteksi tidak terlalu kecil\nfound = stop_data.detectMultiScale(img_gray,\n                                   minSize=(5, 5))\n\n# hitung jumlah objek yang ditemukan.\namount_found = len(found)\n\n# jika objek tidak ditemukan maka tidak dilakukan apa-apa\nif amount_found != 0:\n\n    # jika objek yang ditemukan lebih dari satu, maka :\n    for (x, y, width, height) in found:\n\n        # kita gambar sebuah kotak hijau di objek yang ditemukan\n        cv2.rectangle(img_rgb, (x, y),\n                      (x + height, y + width),\n                      (0, 255, 0), 5)\n\n# tampilkan hasil citra menggunakan plt\nplt.subplot(1, 1, 1)\nplt.imshow(img_rgb)\nplt.show()\n\n\n\n\n\n\n8.3.4.7 Eksperimen!\nCari gambar rambu lalu lintas stop di internet atau ambil foto secara langsung, lalu masukkan gambar tersebut ke coding. Terapkan algoritma dan lihat apakah algoritma deteksi objek berjalan dengan sempurna.\nPetunjuk : edit line cv2.imread(“[nama file foto]”) untuk menggunakan gambar atau foto anda sendiri"
  },
  {
    "objectID": "modul_7.html#penggunaan-data-labeling-dalam-natural-language-processing",
    "href": "modul_7.html#penggunaan-data-labeling-dalam-natural-language-processing",
    "title": "7  Transformasi Data",
    "section": "8.4 Penggunaan Data Labeling dalam Natural Language Processing",
    "text": "8.4 Penggunaan Data Labeling dalam Natural Language Processing\nNatural Language Processing atau NLP mengacu pada analisis bahasa manusia dan bentuknya selama interaksi baik dengan manusia lain maupun dengan mesin. Menjadi bagian dari linguistik komputasi awalnya, NLP telah berkembang lebih lanjut dengan bantuan Artificial Intelligence dan Deep Learning.\n\n8.4.1 Sentiment Analysis\n\nAdalah sebuah proses pemberian label sentimen terhadap sebuah teks (kata, kalimat, atau paragraf) berdasarkan ekspresi perasaan manusia. Tujuan utamanya adalah untuk mengkategorikan teks berdasarkan sentimen yang diekspresikan oleh teks.\nTeks dianalisa dengan mempertimbangkan beberapa faktor, antara lain : pemilihan kata, konteks, subjektivitas, dan beberapa faktor lain. Lalu sebuah sentimen berupa perasaan (marah, sedih, bahagia, bingung) dilabelkan ke teks tersebut.\n\n\n8.4.2 Named Entity Recognition (NER)\n\nAdalah sebuah proses pemberian label entitas dalam sebuah teks. Entitas adalah sebuah kategori luas yang dapat diatur oleh pembuat algoritma. Contoh dari entitas antara lain, nama seseorang, lokasi, negara, organisasi, besaran, jumlah uang, dan lain lain.\nTujuan utama dari NER adalah mengekstrak informasi entitas dari sebuah teks. Dalam proses labeling, algoritma harus memperhatikan konteks, struktur kata, dan arti tersirat dalam sebuah teks untuk mengidentifikasi sebuah entitas secara akurat.\n\n\n8.4.3 Part of Speech Tagging (POS-tagging)\n\nAdalah sebuah proses pemberian label kategori tata-bahasa (grammar) dalam sebuah teks. Pada umumnya sebuah kalimat terdiri dari beberapa kata yang termasuk dalam kategori grammar kata benda (noun), kata sifat (adjective), keterangan (adverb), kata ganti (pronoun), dan lain-lain.\nTujuan utama POS-tagging adalah untuk menentukan kategori grammar dari sebuah kata di dalam sebuah konteks yang ada di teks. POS-tagging sangatlah penting karena grammar atau tata-bahasa dapat mengubah makna dari sebuah kalimat secara keseluruhan.\n\n\n8.4.4 Hands On Coding\nDalam praktikum ini kita akan membuat algoritma Named Entity Recognition menggunakan library spaCy dan sebuah artikel.\n\n8.4.4.1 Download Library dan Dataset yang Dibutuhkan\n\npython -m pip install -U pip setuptools wheel\n\npip install -U spacy\n\npython -m spacy download en_core_web_sm\n\n\n\n8.4.4.2 Import library\n\nimport spacy\n\nfrom spacy import displacy\n\n\n\n8.4.4.3 Masukkan Model NER SpaCy\n\nNER = spacy.load(\"en_core_web_sm\")\n\n\n\n8.4.4.4 Tentukan Artikel yang Akan di Proses. Artikel Harus Berbahasa Inggris.\n\nrawtext = \"Asus Zenfone 10 will please everyone who finds the current flagship cellphones too bulky: the compact and relatively light smartphone comes with a lot of memory on request, which also works as fast as an arrow. It scores with very high system performance, long software updates, fast WiFi 7 and a stylish, IP-certified case that is also available in brighter color variants. At a price of US$749, however, the smartphone competes with the Samsung Galaxy S23 or the Apple iPhone 13 mini and you have to accept a few weaknesses. For example, compared to Samsung's flagship, the camera lacks the optical zoom capability, which reduces the flexibility of the camera setup. Compared to the iPhone, the images appear less sharp. The screen could also be a little brighter, the strong heating of the phone under high load is annoying and the battery life is average. The fact that Asus installs a USB 2.0 port in its Zenfone 10 is also no reason for joy. But users who want a 3.5mm port combined with high performance will hardly get past the Zenfone 10.\"\n\n\n\n8.4.4.5 Masukkan Artikel ke Fungsi NER, Lalu tampilkan hasilnya\n\ntext1 = NER(rawtext)\n\nfor word in text1.ents:\n    print(word.text, '-', word.label_)\n\nAsus Zenfone - PERSON\nWiFi - PERSON\n7 - CARDINAL\nIP - ORG\n749 - MONEY\nSamsung - ORG\nApple - ORG\n13 - CARDINAL\nSamsung - ORG\niPhone - ORG\nAsus - PERSON\n2.0 - CARDINAL\nZenfone 10 - LAW\n3.5mm - QUANTITY\n\n\n\n\n8.4.4.6 Kesimpulan\nHasil dari NER adalah sebuah kata yang dideteksi dan kategori dari kata tersebut. Namun bisa dilihat bahwa hasil dari algoritma tidak 100% akurat. Ada beberapa hasil yang labelnya tidak sesuai, contohnya “Asus - PERSON”. Algoritma melabeli kata Asus sebagai sebuah nama orang, padahal Asus seharusnya adalah sebuah organisasi (ORG).\n\n\n8.4.4.7 Eksperimen!\nCari teks dari artikel, wikipedia, berita, lalu masukkan teks tersebut ke algoritma NER dan analisa hasil dan akurasi dari algoritma.\nPetunjuk : masukkan teks ke dalam variable rawtext sebagai string (“”)"
  },
  {
    "objectID": "modul_7.html#analisa-kualitas-dan-akurasi-data-untuk-pelabelan",
    "href": "modul_7.html#analisa-kualitas-dan-akurasi-data-untuk-pelabelan",
    "title": "7  Transformasi Data",
    "section": "8.5 Analisa Kualitas dan Akurasi Data untuk Pelabelan",
    "text": "8.5 Analisa Kualitas dan Akurasi Data untuk Pelabelan\nAkurasi dalam pelabelan data mengukur seberapa dekat pelabelan dengan ground truth, atau seberapa baik fitur berlabel dalam data set konsisten dengan kondisi dunia nyata. Dalam model pemrosesan bahasa alami (NLP) contohnya adalah seberapa akurat model memberikan label sentimen terhadap sebuah teks.\nKualitas dalam pelabelan data adalah tentang akurasi dataset secara keseluruhan. Apakah pekerjaan semua pemberi label terlihat sama? Apakah pelabelan secara konsisten akurat di seluruh data set?\n\n8.5.1 Definisi Data yang Berkualitas\nProses pelabelan data selalu bertumpu pada kualitas data. Trash in, trash out adalah sebuah mantra yang populer di dunia pengolahan data. Artinya, data yang tidak berkualitas tidak dapat menghasilkan produk yang berkualitas.\nMaka dari itu, data harus kita tentukan kualitas dan akurasinya agar kita dapat menentukan ekspektasi terhadap hasil dari pelabelan data yang akan kita lakukan. Pada umumnya data yang berkualitas mempunyai :\n\nTidak ada nilai kosong/korup\n\nNilai unik tiap entry\n\nVariasi seimbang\n\nMetode pengambilan data yang tangguh\n\nDokumentasi lengkap\n\nFormat yang rapi\n\n\n\n8.5.2 Faktor-faktor yang mempengaruhi kualitas proses pelabelan data\n\n8.5.2.1 Pemahaman Data\nData mempunyai banyak fitur dan value. Pengetahuan tentang fitur data dan nilai-nilai yang ada didalamnya akan sangat membantu untuk menentukan proses pelabelan data.\n\n\n8.5.2.2 Kompetensi Developer/Trainer\nDeveloper harus mempunyai pengetahuan yang luas di bidang machine learning atau bidang yang relevan. Developer harus bisa menentukan kualitas dataset, langkah-langkah data preprocessing, algoritma yang sesuai, dan cara untuk mengukur akurasi pelabelan.\n\n\n8.5.2.3 Ketangguhan Workflow\nProses pelabelan data pasti mempunyai tahap-tahap yang ditetapkan oleh kepemimpinan. Proses atau workflow tersebut harus tahan terhadap gangguan-gangguan seperti human error, machine error, perubahan requirements, ambiguitas, sehingga proses dapat berjalan lancar selamanya. Salah satu proses terpenting yaitu QA, yang akan kita bahas di slide selanjutnya\n\n\n\n8.5.3 Metode QA untuk mengukur kualitas data\n\n8.5.3.1 Consensus Algorithm\nAdalah sebuah metodologi untuk mencapai sebuah keputusan mengenai kualitas data dengan cara mengumpulkan persetujuan (consensus) dari semua atau sebagian orang yang melakukan proses data labeling.\n\n\n8.5.3.2 Benchmarking dan Gold Standard\nAdalah proses membandingkan hasil data labeling dari beberapa model dengan mengaplikasikan model tersebut ke dataset yang sering digunakan. Tujuan utamanya dalah untuk memperoleh batas bawah (baseline) atau reference point untuk mengevaluasi kualitas dari model. Gold standard adalah sebuah hasil pelabelan dari sebuah model berkualitas tinggi terhadap dataset berkualitas tinggi. Gold standard merepresentasikan teknologi terbaik, aplikasi terakurat, dan dataset berkualitas paling tinggi, hasilnya adalah nilai akurasi yang paling tinggi diantara riset atau percobaan lain.\n\n\n8.5.3.3 Cronbach Alpha Test\nAdalah sebuah metode untuk mengukur tingkat konsistensi dari beberapa variabel yang mempunyai variabel laten yang sama. Variabel laten adalah variabel yang tidak mempunyai metrik atau ukuran secara tersendiri. Untuk mengukur variabel laten, diperlukan beberapa variabel lain yang saling berhubungan dan mempunyai konsistensi tinggi. Cronbach Alpha berfungsi untuk mengukur variabel lain ini.\nContohnya adalah kita kana mengukur tingkat ekstroversi seseorang. Tingkat ekstroversi ini adalah variabel laten karena kita tidak bisa mengukur tingkat ekstroversi dengan sendirinya. Maka diperlukan kuesioner dengan 5 pertanyaan. Hasil dari 5 pertanyaan inilah yang akan kita tes dengan Cronbach Alpha Test untuk menentukan apakah mereka merepresentasikan tingkat ekstroversi seseorang.\n\n\n\n8.5.4 Hands On Coding - Cronbach Alpha Test\nRumus dari Cronbach Alpha test adalah:\n\\[\n\\alpha = \\dfrac{k}{k-1}(1-\\dfrac{\\Sigma s^2_i}{s^2_X})\n\\]\n\\(\\alpha\\) = koefisien reliabilitas \\(k\\) = jumlah item set \\(s^2_i\\) = nilai variance setiap item i dimana i = 1, 2, …, k, \\(s^2_X\\) = nilai variance dari semua item\nNilai α diatas 0.7 termasuk cukup bagus untuk sebuah paper.\nSebagai contoh, kita akan membuat sebuah sistem untuk mengecek kepuasan pelanggan terhadap 5 produk baru kita. Pelanggan diberikan sebuah kuesioner dan pelanggan akan memberi rating dari 1-5 terhadap 5 produk baru kita.\n\n8.5.4.1 Install dan Import Library\n\npip install numpy\n\npip install scipy\n\n\n\n8.5.4.2 Buat Data Menggunakan NumPy\nAnda dapat menambahkan data baru\n\ndata = np.array([[4,3,5,2,4], [5,4,4,3,5], [3,2,3,4,3], [4,4,5,5,4]])\nprint(data)\n\n[[4 3 5 2 4]\n [5 4 4 3 5]\n [3 2 3 4 3]\n [4 4 5 5 4]]\n\n\n\n\n8.5.4.3 Hitung Rata-Rata dari Setiap Item/Fitur\n\nitem_means = np.mean(data,axis=0)  \nprint(item_means)\n\n[4.   3.25 4.25 3.5  4.  ]\n\n\n\n\n8.5.4.4 Hitung Variansi dari Total Skor dan Skor Setiap Item\n\ntotal_var = np.var(np.sum(data, axis=1))\nitem_var = np.var(data, axis=0, ddof=1)\nprint(f'total variance : ',total_var)\nprint(f'item variance : ',item_var)\n\ntotal variance :  7.5\nitem variance :  [0.66666667 0.91666667 0.91666667 1.66666667 0.66666667]\n\n\n\n\n8.5.4.5 Hitung Rata-Rata dari Setiap Item/Fitur\n\nnum_items = len(item_means)\ncronbach_alpha = (num_items / (num_items - 1)) * (1-(np.sum(item_var) / total_var))\nprint(f'Nilai Cronbach Alpha adalah : ', cronbach_alpha)  \n\nNilai Cronbach Alpha adalah :  0.4444444444444444\n\n\nHasil nilai Cronbach Alpha adalah 0.4. Nilai ini menunjukkan bahwa konsistensi dari kuesioner yang terdiri dari 5 fitur (item 1 - 5) tergolong rendah dan tidak mencerminkan kepuasan pelanggan secara akurat.\nPada umumnya, nilai Cronbach Alpha yang tergolong bagus adalah diatas 0.7"
  },
  {
    "objectID": "modul_7.html#keamanan-pelabelan-data",
    "href": "modul_7.html#keamanan-pelabelan-data",
    "title": "7  Transformasi Data",
    "section": "8.6 Keamanan Pelabelan Data",
    "text": "8.6 Keamanan Pelabelan Data\nPelabelan data adalah sebuah pekerjaan yang memakan banyak waktu dan tenaga. Proses pelabelan data mempunyai banyak cabang dan workflow, dan di setiap langkah workflow selalu ada resiko kebocoran data.\nTerlebih jika pekerjaan pelabelan data dikerjakan oleh organisasi lain yang berada di luar kendali kita. Oleh karena itu, kita harus membuat sebuah workflow yang dapat mencegah kebocoran data, serta paham tentang prinsip-prinsip dasar tentang keamanan data.\n\n8.6.1 Resiko Keamanan Outsourcing Data Labeling\n\nMengakses data dari jaringan yang tidak aman atau menggunakan perangkat tanpa perlindungan malware\n\nMengunduh atau simpan sebagian data (mis., screen capture, flash drive)\n\nMemberi label data saat berada di tempat umum\n\nTidak memiliki pelatihan, konteks, atau akuntabilitas terkait dengan aturan keamanan untuk pekerjaan labeling\n\nBekerja di lingkungan fisik atau digital yang tidak disertifikasi untuk mematuhi peraturan data (mis., HIPAA, SOC 2).\n\n\n\n8.6.2 Tiga area yang perlu menjadi perhatian untuk menjaga keamanan dokumen\n\nOrang dan Tenaga Kerja: Ini dapat mencakup pemeriksaan latar belakang untuk pekerja dan mungkin mengharuskan pemberi label untuk menandatangani perjanjian kerahasiaan (NDA) atau dokumen serupa yang menguraikan persyaratan keamanan data.\n\nTeknologi dan Jaringan: Pekerja mungkin diminta untuk menyerahkan perangkat yang mereka bawa ke tempat kerja, seperti ponsel atau tablet.\n\nFasilitas dan Ruang Kerja: Pekerja dapat duduk di tempat yang menghalangi orang lain untuk melihat pekerjaan mereka."
  },
  {
    "objectID": "modul_8.html#pendahuluan",
    "href": "modul_8.html#pendahuluan",
    "title": "8  Pemodelan Data Science",
    "section": "8.1 Pendahuluan",
    "text": "8.1 Pendahuluan\nPemodelan data science adalah proses membangun model yang dapat digunakan untuk memprediksi nilai dari suatu variabel berdasarkan nilai variabel lainnya. Model yang dibangun dapat berupa model statistika, model machine learning, atau model deep learning. Pemodelan data science merupakan salah satu tahapan penting dalam proses data science.\nPada modul ini, data yang akan dijadikan contoh adalah data kamar hotel di Yogyakarta. Data ini berisi informasi mengenai kamar hotel di Yogyakarta yang terdapat pada website Traveloka."
  },
  {
    "objectID": "modul_8.html#tahap-pemodelan-machine-learning",
    "href": "modul_8.html#tahap-pemodelan-machine-learning",
    "title": "8  Pemodelan Data Science",
    "section": "8.2 Tahap Pemodelan Machine Learning",
    "text": "8.2 Tahap Pemodelan Machine Learning\n\n8.2.1 Data Preparation\nUmumnya data terlebih dahulu dibagi menjadi dua bagian, yaitu data training dan data testing. Data training digunakan untuk membangun model, sedangkan data testing digunakan untuk menguji performa model. Data training dan data testing harus memiliki karakteristik yang sama. Data training dan data testing dapat dibagi secara acak. Data training dan data testing dapat dibagi dengan perbandingan 80:20 atau 70:30 dimana data testing seharusnya mendapatkan data yang lebih banyak dibandingkan testing agar model dapat dilatih dengan lebih baik.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# membaca dataset\ndf = pd.read_csv('./dataset/kamar-hotel-yogyakarta.csv')\n\n# Memisahkan fitur dan label\nx = df.iloc[:, 1:]\ny = df.iloc[:, 0]\n# Alternatif\n# x = df.drop('harga', axis=1)\n# y = df['harga']\n\n# membagi data menjadi data training dan data testing dengan perbandingan 80:20\n# Random state digunakan untuk mengatur agar pembagian data menjadi sama setiap kali dijalankan (dapat diisi dengan angka berapapun)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n\n# menampilkan ukuran data training dan data testing\ntrainRatio = round(x_train.shape[0]/len(df), 2)*100\ntestRatio = round(x_test.shape[0]/len(df), 2)*100\n\nprint(f'Train set: {x_train.shape[0]} ({trainRatio}%)')\nprint(f'Test set: {x_test.shape[0]} ({testRatio}%)')\n\n\n8.2.2 Model Training\nAlgoritma yang digunakan dalam modul ini adalah Algoritma Random Forest. Algoritma Random Forest merupakan algoritma yang digunakan untuk melakukan klasifikasi dan regresi. Algoritma Random Forest merupakan pengembangan dari algoritma Decision Tree. Algoritma Random Forest mengambil keputusan berdasarkan hasil voting dari beberapa pohon keputusan.\nLibrary yang digunakan untuk membangun model Random Forest adalah sklearn. Library ini berisi algoritma untuk membangun model machine learning seperti Random Forest, Gradient Boosting, dan lainnya.\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\n\nrf.fit(x_train, y_train)\n\n# menampilkan skor akurasi dari model\nprint(f'Skor R2: {rf.score(x_test, y_test)}')\n\n\n8.2.3 Parameter Tuning\nParameter tuning dilakukan untuk menemukan parameter terbaik yang dapat digunakan untuk membangun model. Parameter tuning dapat dilakukan dengan menggunakan GridSearchCV atau RandomizedSearchCV yang terdapat pada library sklearn. Grid Search merupakan teknik untuk mencari parameter terbaik dengan cara mencoba semua kombinasi dari parameter yang diberikan, maka dari itu waktu komputasi akan lebih panjang. Untuk mempercepat waktu komputasi, dapat digunakan Random Search yang akan mencari parameter terbaik secara acak. Untuk detail mengenai parameter yang digunakan pada algoritma Random Forest dapat dilihat pada dokumentasi sklearn.\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# menentukan parameter yang akan dicoba\nrfParams = {\n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_depth': [None, 5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2', None],\n}\n\n# mencari parameter terbaik\nrfRandom = RandomizedSearchCV(rf, rfParams, random_state=123, n_jobs=-1)\n\nrfRandom.fit(x_train, y_train)\n\n# menampilkan parameter terbaik\nprint(f'Parameter terbaik: {rfRandom.best_params_}')\n\n# menampilkan skor akurasi dari model\nprint(f'Skor R2: {rfRandom.score(x_test, y_test)}')\n\n\n8.2.4 Saving Model\nModel yang telah dibangun dapat disimpan dengan menggunakan library pickle. Library ini digunakan untuk menyimpan objek Python ke dalam file. Model yang telah disimpan dapat digunakan kembali tanpa harus membangun model dari awal.\nimport pickle\n\n# refit model dengan parameter terbaik\nrfModel = rfRandom.best_estimator_.fit(x_train, y_train) \n\n# menyimpan model\npickle.dump(rfModel, open('rfModel.pkl', 'wb'))"
  },
  {
    "objectID": "modul_9.html#apa-itu-evaluasi-model",
    "href": "modul_9.html#apa-itu-evaluasi-model",
    "title": "9  Evaluasi Data",
    "section": "9.1 Apa itu Evaluasi Model?",
    "text": "9.1 Apa itu Evaluasi Model?\nMerupakan salah satu tahap penting dalam proses machine learning yang memiliki tujuan untuk memastikan model dapat menghasilkan prediksi yang akurat."
  },
  {
    "objectID": "modul_9.html#metrik-evaluasi",
    "href": "modul_9.html#metrik-evaluasi",
    "title": "9  Evaluasi Data",
    "section": "9.2 Metrik Evaluasi",
    "text": "9.2 Metrik Evaluasi\n\nAkurasi\nPresisi\nRecall\nF1-Score\nConfusion Matrix\n\n\n9.2.1 Akurasi\n\nMerupakan metrik yang mengukur sejauh mana model dapat melakukan prediksi dengan benar.\nFormula atau rumus akurasi yaitu\nSemakin tinggi nilai akurasi, maka semakin baik model tersebut.\n\n\nSeberapa Penting Nilai Akurasi\n\nAkurasi dapat digunakan untuk mengevaluasi perfoma sebuah model\nAkurasi memberikan gambaran seberapa baik model yang digunakan secara keseluruhan\nDengan nilai akurasi, dapat mengetahui seberapa akurat dalam memprediksi kelas data.\n\n\n\nKelebihan dan Keterbatasan Akurasi\n\nAkurasi memiliki kelebihan sebagai matrik evaluasi yang sederhana dan sangat mudah dimengerti.\nKeterbatasan akurasi dalam mengatasi ketidakseimbangan kelas pada sebuah dataset.\n\n\n\nPahami Code berikut\n\nPerhatikan contoh code di bawah, kira-kira menghasilkan nilai Akurasi berapa persen?\nAnda dapat mencoba code di samping menggunakan dataset yang berbeda, lalu fahami dan lihat nilai Akurasinya.\nPada dataset yang digunakan, dibagi menjadi 80:20 pada tahap split data. Selanjutnya menggunakan algoritma KNN dengan nilai K=3. selanjutnya akan dilakukan prediksi pada data uji dan akurasi model yang dihitung dengan membandingkan hasil prediksi dengan label sebenarnya pada data uji menggunakan fungsi “accuracy_score”\n\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris #dataset dari sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Muat dataset Iris\n#dapat dirubah menggunakan dataset yang lain\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung akurasi model dengan membandingkan prediksi dengan label sebenarnya pada data uji\naccuracy = accuracy_score(y_test, y_pred)\n\n# Tampilkan hasil akurasi\nprint(\"Akurasi model: {:.2f}%\".format(accuracy * 100))\n\n\n\n\n9.2.2 Presisi dan Recall\nPresisi dan Recall merupakan metrik untuk mengukur performa pada model tertentu. Semakin tinggi nilai presisi dan recall, maka semakin baik model pada kelas tertentu.\n\nPresisi\n\nPresisi mengukur sejauh mana, model dapat melakukan identifikasi dengan benar pada kelas tertentu.\nFormula atau rumus presisi sebagai berikut\n\n\n\nRecall\n\nRecall mengukur sejauh mana model dapat menemukan kembali kelas tertentu.\nFormula atau rumus dari recall\n\n\n\nPahami Code berikut\nPada dataset yang digunakan, dibagi menjadi 80:20 pada tahap split data. Selanjutnya menggunakan algoritma KNN dengan nilai K=3. selanjutnya model akan melakukan prediksi pada data uji dan presisi serta recall dari model menggunakan fungsi precision_score dan recall_score dengan parameter average=‘macro’\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung presisi model\nprecision = precision_score(y_test, y_pred, average='macro')\n\n# Hitung recall model\nrecall = recall_score(y_test, y_pred, average='macro')\n\n# Tampilkan hasil presisi dan recall\nprint(\"Presisi model: {:.2f}\".format(precision))\nprint(\"Recall model: {:.2f}\".format(recall))\n\n\n\n9.2.3 F1-Score\n\nMerupakan metrik untuk menggabungkan presisi dan recall\nFormula F1-Score\nSemakin tinggi dari nilai F1-Score, maka semakin baik model pada kelas tersebut. #### Mengapa F1-Score penting? {.unnumbered}\nF1-Score cocok digunakan saat terdapat ketidakseimbangan kelas pada sebuah dataset.\nF1-Score memberikan bobot yang seimbang antara presisi dan recall.\n\n\nPahami Code berikut\nPada dataset yang digunakan, dibagi menjadi 80:20 pada tahap split data. Selanjutnya menggunakan algoritma KNN dengan nilai K=3. selanjutnya model tersebut melakukan prediksi pada data uji dan F1-score dari model dihitung menggunakan fungsi f1_score dengan parameter average=‘macro’\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung F1-score model\nf1score = f1_score(y_test, y_pred, average='macro')\n\n# Tampilkan hasil F1-score\nprint(\"F1-score model: {:.2f}\".format(f1score))\n\n\n\n9.2.4 Pahami Code berikut\n\nApa yang berbeda dari ketiga source code sebelumnya?\nBagaimana hasil nilai akurasi, presisi, recall, dan F1-Score?\n\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung akurasi model\naccuracy = accuracy_score(y_test, y_pred)\n\n# Hitung presisi model\nprecision = precision_score(y_test, y_pred, average='macro')\n\n# Hitung recall model\nrecall = recall_score(y_test, y_pred, average='macro')\n\n# Hitung F1-score model\nf1score = f1_score(y_test, y_pred, average='macro')\n\n# Tampilkan hasil akurasi, presisi, recall, dan F1-score\nprint(\"Akurasi model: {:.2f}%\".format(accuracy * 100))\nprint(\"Presisi model: {:.2f}\".format(precision))\nprint(\"Recall model: {:.2f}\".format(recall))\nprint(\"F1-score model: {:.2f}\".format(f1score))\n\n\n9.2.5 Kurva ROC dan AUC\n\nROC dan AUC merupakan metrik evaluasi model pada sebuah machine learning.\nROC atau Receiver Operating Characteristic merupakan grafik yang menggambarkan sebuah performa model pada berbagai threshold.\nAUC atau Area Under the Curve merupakan luas daerah di bawah kurva ROC yang menggambarkan performa dari keseluruhan model.\n\n\nKurva ROC\n\nROC Curve adalah grafik yang menggambarkan trade-off antara True Positive Rate (TPR) dan False Positive Rate (FPR)\nTPR adalah rasio data positif yang benar diprediksi oleh model, dibandingkan dengan total data positif\nFPR adalah rasio data negatif yang salah diprediksi sebagai positif oleh model, dibandingkan dengan total data negatif\nSemakin dekat kurva ROC ke sudut kiri atas, semakin baik performa model\n\n\n\nKurva AUC\n\nAUC adalah metrik evaluasi yang mengukur performa keseluruhan model\nAUC menghitung luas daerah di bawah kurva ROC\nNilai AUC berada dalam rentang 0 hingga 1, dengan nilai terbaik adalah\n\n\n\nPahami Code berikut\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\n# from sklearn.metrics import roc_auc_ovr, roc_auc_ovo\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model Logistic Regression\nmodel = LogisticRegression(max_iter=1000)\n\n# Peringatan tentang konvergensi mungkin juga menunjukkan bahwa model Logistic Regression yang dilatih tidak sepenuhnya konvergen.\n# Ini dapat diatasi dengan menyesuaikan parameter max_iter pada inisialisasi model:\n# Sesuaikan max_iter sesuai kebutuhan\n# max_iter adalah parameter yang mengontrol jumlah iterasi maksimum yang akan dilakukan oleh algoritma optimasi ketika melatih model.\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Dapatkan probabilitas prediksi untuk kelas positif\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Hitung nilai AUC (Area Under the Curve)\nauc_scores = []\nfor i in range(len(iris.target_names)):\n    auc_score = roc_auc_score(y_test == i, y_prob)\n    auc_scores.append(auc_score)\n    print(\"AUC Class {}: {:.2f}\".format(iris.target_names[i], auc_score))\n\n\n\n9.2.6 Amati dan pahami code berikut\n\nApa yang berbeda dari ketiga source code sebelumnya?\nBagaimana hasil dari code tersebut?\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\n\n# Hitung nilai FPR (False Positive Rate) dan TPR (True Positive Rate) untuk kurva ROC\n# fpr, tpr, _ = roc_curve(y_test, y_prob)\n\n# Load dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model Logistic Regression\nmodel = LogisticRegression(max_iter=1000)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Dapatkan probabilitas prediksi untuk kelas positif\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Inisialisasi variabel untuk menyimpan FPR dan TPR untuk setiap kelas\nfpr = {}\ntpr = {}\nroc_auc = {}\n\n# Perulangan melalui setiap kelas\nfor i in range(len(iris.target_names)):\n    # Membuat label klasifikasi biner untuk kelas ke-i\n    y_true_class = (y_test == i).astype(int)\n\n    # Hitung kurva ROC untuk kelas ke-i\n    fpr[i], tpr[i], _ = roc_curve(y_true_class, y_prob)\n\n    # Hitung AUC untuk kelas ke-i\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot kurva ROC\nplt.figure()\ncolors = ['darkorange', 'cornflowerblue', 'green']  # Untuk pewarnaan\nfor i in range(len(iris.target_names)):\n    plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, label='ROC curve for {} (area = {:.2f})'.format(iris.target_names[i], roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n9.2.7 Confusion Matrix\nConfusion matrix digunakan untuk mengevaluasi kinerja model klasifikasi dan merangkum hasil prediksi pada data uji. Untuk klasifikasi biner, matriks tersebut berbentuk tabel 2x2. Sementara untuk klasifikasi multi-kelas, bentuk matriks akan sama dengan jumlah kelas, yaitu nxn, di mana n adalah jumlah kelas. Confusion matrix terdiri dari empat komponen utama:\n\nTrue Positives (TP): Jumlah kasus yang benar-benar diprediksi dengan benar sebagai positif.\nTrue Negatives (TN): Jumlah kasus yang benar-benar diprediksi dengan benar sebagai negatif.\nFalse Positives (FP): Jumlah kasus yang seharusnya negatif tetapi diprediksi sebagai positif.\nFalse Negatives (FN): Jumlah kasus yang seharusnya positif tetapi diprediksi sebagai negatif.\n\n\n\n\n\n\nEvaluasi Sistem Menggunakan Confusion Matriks\nDari tabel confussion matrix dapat dilakukan perhitungan nilai akurasi,precission,recall dan F1-Score untuk mengukur performa dari algoritma yang digunakan untuk melakukan prediksi  Akurasi :  \\({Accuracy=(TP+TN)/(TP+FN+FP+TN) * 100}\\)  Precision :  \\({Precision= TP/(TP+FP)}\\)  Recall :  \\({Recall= TP/(TP+FN)}\\) F1-Score :  \\({F1Score=(2 * precision * recall)/(precision+ recall)}\\)\nPahami Code Berikut  Binary Class Classification Pada dataset yang digunakan dibagi menjadi data uji dan data latih. lalu latih dan prediksi data menggunakan model decision tree, sehingga selanjutnya dapat menghitung dan memvisualisasikan confusion matriks menggunakan heatmap.\n\n#Import the necessary libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Load the breast cancer dataset\nX, y= load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25)\n\n# Train the model\ntree = DecisionTreeClassifier(random_state=23)\ntree.fit(X_train, y_train)\n\n# preduction\ny_pred = tree.predict(X_test)\n\n# compute the confusion matrix\ncm = confusion_matrix(y_test,y_pred)\n\n#Plot the confusion matrix.\nsns.heatmap(cm, \n            annot=True,\n            fmt='g', \n            xticklabels=['malignant', 'benign'],\n            yticklabels=['malignant', 'benign'])\nplt.ylabel('Prediction',fontsize=13)\nplt.xlabel('Actual',fontsize=13)\nplt.title('Confusion Matrix',fontsize=17)\nplt.show()\n\n\n# Finding precision and recall\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy :\", accuracy)\nprecision = precision_score(y_test, y_pred)\nprint(\"Precision :\", precision)\nrecall = recall_score(y_test, y_pred)\nprint(\"Recall :\", recall)\nF1_score = f1_score(y_test, y_pred)\nprint(\"F1-score :\", F1_score)\n\nMulti-Class Classification\n\n#Import the necessary libraries\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Load the breast cancer dataset\nX, y= load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25)\n\n# Train the model\nclf = RandomForestClassifier(random_state=23)\nclf.fit(X_train, y_train)\n\n# preduction\ny_pred = clf.predict(X_test)\n\n# compute the confusion matrix\ncm = confusion_matrix(y_test,y_pred)\n\n#Plot the confusion matrix.\nsns.heatmap(cm, \n            annot=True,\n            fmt='g')\nplt.ylabel('Prediction',fontsize=13)\nplt.xlabel('Actual',fontsize=13)\nplt.title('Confusion Matrix',fontsize=17)\nplt.show()\n\n\n# Finding precision and recall\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy :\", accuracy)"
  },
  {
    "objectID": "modul_10.html#pendahuluan",
    "href": "modul_10.html#pendahuluan",
    "title": "10  Deployment",
    "section": "10.1 Pendahuluan",
    "text": "10.1 Pendahuluan\nDeployment model data science dilakukan untuk memastikan model yang dibuat dapat digunakan oleh orang lain. Deployment model data science dapat dilakukan dengan berbagai cara, salah satunya adalah dengan menggunakan Streamlit.  Streamlit adalah sebuah framework yang dapat digunakan untuk membuat aplikasi web dengan menggunakan bahasa pemrograman Python. Dengan menggunakan Streamlit, deployment model data science dapat dilakukan dengan mudah dan cepat."
  },
  {
    "objectID": "modul_10.html#instalasi-streamlit",
    "href": "modul_10.html#instalasi-streamlit",
    "title": "10  Deployment",
    "section": "10.2 Instalasi Streamlit",
    "text": "10.2 Instalasi Streamlit\n  Untuk menginstall Streamlit, dapat dilakukan dengan menggunakan perintah berikut pada command prompt atau terminal:\npip install streamlit"
  },
  {
    "objectID": "modul_10.html#membuat-aplikasi-web-dengan-streamlit",
    "href": "modul_10.html#membuat-aplikasi-web-dengan-streamlit",
    "title": "10  Deployment",
    "section": "10.3 Membuat Aplikasi Web dengan Streamlit",
    "text": "10.3 Membuat Aplikasi Web dengan Streamlit\nSetelah Streamlit terinstall, langkah selanjutnya adalah membuat aplikasi web dengan menggunakan Streamlit. Untuk membuat aplikasi web dengan Streamlit, dapat dilakukan dengan cara membuat file python baru dengan nama app.py. Kemudian, pada file tersebut, tuliskan kode berikut:\n# streamlit umumnya diinisialisasi dengan 'st'\nimport streamlit as st\nLink Dataset : Here"
  },
  {
    "objectID": "modul_10.html#api-streamlit",
    "href": "modul_10.html#api-streamlit",
    "title": "10  Deployment",
    "section": "10.4 API Streamlit",
    "text": "10.4 API Streamlit\nAPI Streamlit dapat digunakan untuk membuat aplikasi web dengan Streamlit. API Streamlit dapat dilihat pada dokumentasi Streamlit. Berikut adalah beberapa API Streamlit yang dapat digunakan untuk membuat aplikasi web dengan Streamlit:\n\n10.4.1 Page Config\nst.set_page_config dapat digunakan untuk mengatur konfigurasi halaman. Beberapa konfigurasi yang dapat diatur adalah judul halaman, layout halaman, dan lain-lain. Berikut adalah contoh penggunaan st.set_page_config untuk mengatur judul halaman:\nst.set_page_config(\n    page_title = \"Estimasi CO2 Emissions\",\n    page_icon = ':car: | :red_car:' #nama emoji \n)\n\n\n\n\n\n\n\n10.4.2 Write\nst.write dapat digunakan untuk menampilkan teks, dataframe, dan visualisasi. Format penulisan dalam method ini adalah format markdown. Berikut adalah contoh penggunaan st.write untuk menampilkan teks:\n# st.write dapat digunakan menampilkan test,dataframe,visualisasi\nst.title('Estimasi CO2 Emissions')\nst.write('information about Co2 Emission')\n\n\n\n\n\n\n\n10.4.3 Sidebar\nst.sidebar dapat digunakan untuk membuat sidebar. Sidebar bisa digunakan sekedar untuk informasi tambahan atau bahka bisa sebagai input user. Berikut adalah contoh penggunaan st.sidebar untuk membuat sidebar:\n\n# st.sidebar dapat digunakan untuk membuat sidebar\nst.sidebar.header(\"User Input Features\")\n\n\n10.4.4 Input User\nUntuk memasukkan elemen input user dalam sidebar dapat dilakukan dengan menggunakan st.sidebar.slider, st.sidebar.selectbox ,sidebar.number_input dan lain sebagainya . Berikut adalah contoh input data pada sidebar:\ndef user_input_features():\n    EngineSize = st.sidebar.number_input('Insert a Engine Size', 1.0 , 8.4) #(label,minvalues,maxvalues,initial values)\n    Cylinders = st.sidebar.number_input('Insert a Cylinders', 3 , 16)\n    FuelConsumptionTotal = st.sidebar.number_input('Insert a Total Fuel Comsumption (L/100 km)', 1.0 , 25.8)\n    \n    data = {'ENGINESIZE': EngineSize,\n            'CYLINDERS': Cylinders,\n            'FUELCONSUMPTION_COMB': FuelConsumptionTotal\n            }\n    \n    features = pd.DataFrame(data,index=[0])\n    return features\n\ndf = user_input_features()\nst.header(\"User Input Features\")\nst.write(df)\n\nslider digunakan untuk memasukkan elemen input user berupa angka\ncheckbox digunakan untuk memasukkan elemen input user berupa boolean\nmultiselect digunakan untuk memasukkan elemen input user berupa list\nnumber_input digunakan untuk memasukkan elemen input user berupa numeric\n\n\n\n10.4.5 Pengecekan Dataframe\nSetelah data input user diubah menjadi dataframe, perlu dilakukan pengecekan apakah dataframe tersebut sudah sesuai dengan dataframe yang digunakan untuk training. Berikut adalah code untuk mengecek dataframe:\ndfModel = pd.read_csv('data/FuelConsumption.csv')\ndfModel = dfModel[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]\n\ndf2 = dfModel.iloc[:,:-1]\nif df.columns.equals(df2.columns):\n    st.write(\"Column names are the same.\")\nelse:\n    st.write(\"Column names are different.\")\n\n\n\n\n\n\nWarning\n\n\n\nUrutan kolom hasil input user harus sama dengan urutan kolom pada data training. Jika tidak, maka akan terjadi error atau membuat hasil prediksi menjadi tidak valid.\n\n\n\n\n10.4.6 Prediksi\nUntuk melakukan prediksi, model perlu di-load terlebih dahulu. Berikut adalah code untuk melakukan prediksi:\n# separate the feature and target\nX = dfModel.iloc[:,:-1]\nY = dfModel.iloc[:,-1]\n\n# splitting data into data train and test\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3 ,random_state = 42)\n\n# creating model\nLinearReg = LinearRegression()\nLinearReg.fit(x_train,y_train)\nDalam contoh ini menggunkan model Linear Regression . Untuk melakukan prediksi, dapat dilakukan dengan menggunakan code berikut:\nst.write('press button below to predict : ')\nif st.button('Predict'):\n    # create progres bar widget with initial progress is 0%\n    bar = st.progress(0)\n        # create an empty container or space\n    status_text = st.empty()\n    for i in range(1,101):\n        # create a text to showing a percentage process\n        status_text.text(\"%i%% complete\" %i)\n        # give bar progress values\n        bar.progress(i)\n        # give bar progress time to execute the values\n        time.sleep(0.01)\n\n    ypred = LinearReg.predict(df)\n    ypred = float(ypred[0])\n    ypred = \"{:.2f}\".format(ypred)\n\n    st.subheader('Prediction')\n    st.metric('CO2 Emission',ypred,' g/km')\n\n\n10.4.7 Hasil akhir"
  },
  {
    "objectID": "studi_kasus.html",
    "href": "studi_kasus.html",
    "title": "Studi Kasus",
    "section": "",
    "text": "Berikut daftar studi kasus yang dapat digunakan untuk mempelajari materi pada modul ini:\n\nProject Klasifikasi Sentimen Analisis\nProject Regresi Prediksi Harga Kamar Hotel\nProject Clustering - Spotify Playlist Clustering"
  },
  {
    "objectID": "project_klasifikasi.html#project-klasifikasi-sentimen-analisis",
    "href": "project_klasifikasi.html#project-klasifikasi-sentimen-analisis",
    "title": "Studi Kasus 1",
    "section": "Project Klasifikasi Sentimen Analisis",
    "text": "Project Klasifikasi Sentimen Analisis"
  },
  {
    "objectID": "project_klasifikasi.html#klasifikasi-sentimen-analisis",
    "href": "project_klasifikasi.html#klasifikasi-sentimen-analisis",
    "title": "Studi Kasus 1",
    "section": "Klasifikasi Sentimen Analisis",
    "text": "Klasifikasi Sentimen Analisis\nKlasifikasi dapat berbentuk berupa Sentimen Analisis. Sentimen Analisis merupakan gabungan dari data mining dan text mining. Secara sederhananya merupakan proses mengolah berbagai opini dan argumen dari berbagai media sosial dalam berbagai aspek seperti jasa, produk, atau sesuai dengan isi konten pada media sosial tersebut .\nUntuk dapat mengerjakan sebuah sentimen analisis, diperlukan sebuah dataset yang berisi banyak opini positif, negatif, dan atau netral. Maka dari itu pengambilan dataset merupakan proses yang penting dalam tahapan sentimen analisis. Dataset yang baik harus memiliki ukuran yang cukup besar dengan jumlah yang cukup banyak untuk meminimalkan kesalahan secara perhitungan algoritma.\nUntuk mendapatkan sebuah dataset yang berisikan opini di media sosial dapat dilakukan dengan berbagai cara. Salah satunya adalah mencari dataset yang sudah jadi pada situs penyedia dataset seperti kaggle, dataset tersebut dapat dikategorikan sebagai dataset publik. Selain dengan menggunakan dataset publik, data opini dapat dibuat dengan cara scraping atau crawling. Scraping merupakan teknik yang sering digunakan untuk mendapatkan atau mengekstrak sebuah informasi pada sebuah media sosial dan atau website secara otomatis tanpa harus melakukan penyalinan secara manual."
  },
  {
    "objectID": "project_klasifikasi.html#mencari-data-manual-dengan-scraping",
    "href": "project_klasifikasi.html#mencari-data-manual-dengan-scraping",
    "title": "Studi Kasus 1",
    "section": "Mencari Data Manual dengan Scraping",
    "text": "Mencari Data Manual dengan Scraping\nBerikut adalah cara mendapatkan data untuk sentimen analisis menggunakan Komentar Youtube. Dapat menggunakan API Key berikut AIzaSyDkCRF4cmM_TtyBznV9aKptHNZqooyucqU\n# yang harus di instal\n\n!pip install google-api-python-client\n!pip install google-auth google-auth-oauthlib google-auth-httplib2\n!pip install pickle\n!pip install sastrawi\n!pip install textblob\nCode diatas merupakan library yang harus terintal.\n#library yang digunakan, jika dirasa kurang penting dapat dihapus\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport string\nimport re\nfrom nltk import word_tokenize\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom textblob import TextBlob\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nSetalah mengimport semua library, selanjutnya measukan code berikut\nfrom google.colab import drive\ndrive.mount('/content/drive')\nimport pandas as pd\n\n## Call the \"build()\" function from the Python-client\nfrom googleapiclient.discovery import build\n\napi_key = input(\"API KEY: \")\nyoutube = build(\"youtube\",\"v3\", developerKey=api_key)\nurl = input(\"VIDEOURL: \")\n\ndef get_comments(url):\n    # Get the ID of the video by splitting the URL\n    single_video_id = url.split(\"=\")[1].split(\"&\")[0]\n    # Use the list() method to extract a JSON with key information\n    # from the video.\n    video_list=youtube.videos().list(part=\"snippet\",id=single_video_id).execute()\n    channel_id= video_list[\"items\"][0][\"snippet\"][\"channelId\"]\n    title_single_video= video_list[\"items\"][0][\"snippet\"][\"title\"]\n    playlist_id = None\n    forUserName = None\n\n    nextPageToken_comments = None\n    commentsone=[]\n\n    while True:\n        #Request the first 50 videos of a channel. This is the full dictionary. The result is store in a variable called \"pl_response\".\n        #PageToken at this point is \"None\"\n        pl_request_comment= youtube.commentThreads().list(part=[\"snippet\",\"replies\"],\n                                            videoId=single_video_id,\n                                            maxResults=50,\n                                            pageToken= nextPageToken_comments)\n        pl_response_comment = pl_request_comment.execute()\n\n        ## Send the amount of views and the URL of each video to the videos empty list that was declared at the beginning of the code.\n        for i in pl_response_comment[\"items\"]:\n            vid_comments = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"]\n            comm_author = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n            comm_author_id = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorChannelId\"][\"value\"]\n            comm_date = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n            comm_likes = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n            new_var=i.get(\"replies\",\"0\")\n\n            commentsone.append({\n                \"comm_date\":comm_date,\n                \"author\":comm_author,\n                \"author_id\":comm_author_id,\n                \"likes\":comm_likes,\n                \"comment\":vid_comments,\n                \"video_id\":single_video_id\n            })\n\n\n\n        nextPageToken_comments = pl_response_comment.get(\"nextPageToken\")\n\n        if not nextPageToken_comments:\n            break\n\n    for i in commentsone[:10]:\n        print(i[\"comment\"])\n\n\n    pd.DataFrame.from_dict(commentsone).to_csv(f\"/content/drive/MyDrive/comments/dataset.csv\")\n\nget_comments(url)\npada source code scraping diatas, file yang telah di scraping akan dimasukan kedalam Google Drive dengan nama file comments, lalu nama file akan menjadi dataset.csv\nuntuk menampilkan hasil scraping dapat menggunakan perintah seperti berikut.\ndf = pd.read_csv('/content/drive/MyDrive/comments/dataset.csv')\ndf.head(500)\ndf.count()"
  },
  {
    "objectID": "project_klasifikasi.html#mencari-data-dengan-mengunjungi-website",
    "href": "project_klasifikasi.html#mencari-data-dengan-mengunjungi-website",
    "title": "Studi Kasus 1",
    "section": "Mencari Data dengan mengunjungi website",
    "text": "Mencari Data dengan mengunjungi website\nData sentimen analisis bisa didapatkan melalui website penyedia data seperti kaggle dan UC Irvine Machine Learning Repository."
  },
  {
    "objectID": "project_klasifikasi.html#sentimen-analisis-menggunakan-data-scraping-youtube",
    "href": "project_klasifikasi.html#sentimen-analisis-menggunakan-data-scraping-youtube",
    "title": "Studi Kasus 1",
    "section": "Sentimen Analisis menggunakan data scraping youtube",
    "text": "Sentimen Analisis menggunakan data scraping youtube\ngunakan code berikut untuk melakukan sentimen analisis\nimport nltk\nimport pandas as pd\ndata = pd.read_csv(\"/content/drive/MyDrive/comments/dataset.csv\")\ndata = data.dropna()\nprint(data.head())\ndata_nw = data.drop(['comm_date',\"author\", 'author_id',\"likes\",'video_id'], axis=1 )\ndata_nw\nFungsi code diatas adalah untuk menghapus atau melakukan drop pada kolom ‘comm_date’,“author”, ‘author_id’,“likes”,‘video_id’\ndata_nw.to_csv(\"/content/drive/MyDrive/comments/dataset_drop.csv\") #Fungsinya untuk menyimpan hasil drop\nmembuka file hasil dari dataset_drop.csv\ndata_baru = pd.read_csv(\"/content/drive/MyDrive/comments/dataset_drop.csv\")\ndata_baru.head()\ndef caseFolding(comment):\n          comment = comment.lower()\n          comment = comment.strip(\" \")\n          comment = re.sub(r'[?|$|.|!]',r'', comment)\n          comment = re.sub(r'[^a-zA-Z0-9 ]',r'', comment)\n          return comment\n\ndata_baru['comment'] = data_baru['comment'].apply(caseFolding)\nFungsi dari code diatas adalah untuk menghilangkan tanda baca serta angka yang tidak dibutuhkan dan mengubah huruf kapital menjadi huruf kecil. Lalu data disimpan ke dalam kolom comment.\nSelanjutnya simpan menggunakan perintah berikut\ndata_baru.to_csv(\"/content/drive/MyDrive/comments/dataset_bersih.csv\")\nSelanjutnya data yang sudah bersih, dapat diberi label negatif, positif, dan atau netral pada samping kolom comments. Label dapat di beri nama sentiment.\nSelanjutnya dapat dilakukan uji akurasi menggunakan Algoritma atau model yang sesuai dan yang di inginkan.\nSebagai contoh menggunakan Algoritma KNN, maka dapat menggunakan code berikut.\nimport pandas as pd\n\ndata = pd.read_csv('/content/drive/MyDrive/comments/dataset_bersih.csv')\nX = data['comment']\ny = data['sentimen']\n# Lakukan preprocessing pada teks\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nnltk.download('stopwords')\n\ndef preprocessing(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.strip()\n    tokens = nltk.word_tokenize(text)\n    stop_words = set(stopwords.words('indonesian'))\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    stemmer = PorterStemmer()\n    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n    return ' '.join(stemmed_tokens)\n\nX = X.apply(preprocessing)\n# Lakukan vectorization pada teks\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(X)\n# Lakukan pembagian dataset menjadi data training dan data testing\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Lakukan training model KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n# Lakukan prediksi pada data testing\ny_pred = knn.predict(X_test)\n# Lakukan evaluasi model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1 Score:', f1)\nprint('Confusion Matrix:\\n', cm)"
  },
  {
    "objectID": "project_regresi.html#project-regresi-hotel-yogyakarta",
    "href": "project_regresi.html#project-regresi-hotel-yogyakarta",
    "title": "Studi Kasus 2",
    "section": "Project Regresi Hotel Yogyakarta",
    "text": "Project Regresi Hotel Yogyakarta"
  },
  {
    "objectID": "project_regresi.html#deskripsi",
    "href": "project_regresi.html#deskripsi",
    "title": "Studi Kasus 2",
    "section": "Deskripsi",
    "text": "Deskripsi\nProject ini merupakan contoh project Data Science yang menggunakan data hotel di Yogyakarta. Project ini bertujuan untuk memprediksi harga kamar hotel berdasarkan fitur-fitur yang ada. Project ini menggunakan 3 algoritma yaitu, XGBoost, Random Forest, dan SVM. Project ini juga menggunakan teknik hyperparameter tuning untuk meningkatkan performa model. Kemudian model tersebut dilakukan deployment menggunakan streamlit"
  },
  {
    "objectID": "project_regresi.html#tentang-data",
    "href": "project_regresi.html#tentang-data",
    "title": "Studi Kasus 2",
    "section": "Tentang data",
    "text": "Tentang data\nData ini diperoleh dengan teknik scrapping pada website Traveloka. Data ini berbentuk sqlite yang berisikan 2 tabel bernama hotel_yogyakarta dan hotel_room_yogyakarta\n\nhotel_yogyakarta\nBerikut detail column pada tabel hotel_yogyakarta. Dimensi (378, 12)\n\nid: Unique id hotel\ntype: Tipe penginapan\nname: Nama hotel\nstarRating: Rating bintang hotel\nbuiltYear: Tahun dibuatnya hotel\ndescription: Deskripsi tentang hotel\nlink: URL menuju halaman hotel di Traveloka\naddress: Alamat hotel\ncity: Kota hotel\nimage: URL gambar hotel\nfacilities: Daftar fasilitas pada hotel\nnearestPointofInterests: Area populer / fasilitas umum disekitar hotel\n\n\n\nhotel_room_yogyakarta\nBerikut detail column pada tabel hotel_room_yogyakarta. Dimensi (1199, 16)\n\nid: Unique id hotel\nhotelId: Id hotel\nroomType: Tipe kamar hotel\ndescription: deskripsi kamar hotel\nbedDescription: deskripsi kasur kamar\nsize: Ukuran kamar (\\(m^2\\))\noriginalRate: Harga kamar per malam\nbaseOccupancy: Kapasitas kamar\nmaxChildAge: Umur maksimal anak-anak\nmaxChildOccupancy: Kapasitas kamar untuk anak-anak\nnumExtraBeds: Jumlah kasur tambahan\nisBreakfastIncluded: Fasilitas sarapan\nisWifiIncluded: Fasilitas WiFi\nisRefundable: Fasilitas refund\nhasLivingRoom: Fasilitas ruang keluarga\nfacilities: Daftar fasilitas lainnya pada kamar"
  },
  {
    "objectID": "project_regresi.html#data-analysis",
    "href": "project_regresi.html#data-analysis",
    "title": "Studi Kasus 2",
    "section": "Data analysis",
    "text": "Data analysis\n\nImport library\nfrom sqlite3 import connect\nimport pickle\nimport pandas as pd\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\nLoad data\nBuat koneksi ke database sqlite, lalu baca tabel hotel_yogyakarta danhotel_room_yogyakarta menjadi dataframe pandas.\n# SQLite3 connection\ncon = connect('./dataset/hotel-directories-ORI.sqlite3')\ndf_sql_hotel = pd.read_sql_query(\"SELECT * from hotel_yogyakarta\", con=con)\ndf_sql_room = pd.read_sql_query(\"SELECT * from hotel_room_yogyakarta\", con=con)\ncon.close()\n\n# Table columns\nprint('Kolom Tabel Hotel :')\nprint(df_sql_hotel.columns)\nprint(\"Total Baris :\", df_sql_hotel.shape[0])\nprint(\"Total Kolom :\", df_sql_hotel.shape[1])\n\nprint('-' * 50)\n\nprint('Kolom Tabel Kamar:')\nprint(df_sql_room.columns)\nprint(\"Total Baris :\", df_sql_room.shape[0])\nprint(\"Total Kolom :\", df_sql_room.shape[1])\nUntuk melihat hasil dataframe dapat dilakukan menggunakan code berikut.\nprint(f\"Dimensi : {df_sql_hotel.shape}\")\ndf_sql_hotel.sample(2)\nprint(f\"Dimensi : {df_sql_room.shape}\")\ndf_sql_room.sample(2)\n# Fungsi menghitung unique value\ndef check_unique(df):\n    count = 0\n    for i in df.columns:\n        if df[i].nunique() == 1:\n            count += 1\n            print(f'{i}: {df[i].nunique()}')\n        else:\n            print(f'{i}: {df[i].nunique()}')\n    if count == 0:\n        print('No columns with only one unique value')\nGunakan fungsi check_unique() untuk mengecek apakah terdapat data dengan unique value kurang dari 2. Jika ada, maka data tersebut tidak akan digunakan.\ncheck_unique(df_sql_hotel)\ncheck_unique(df_sql_room)\n\nNilai penghubung kedua tabel adalah id pada data hotel dan hotelId pada data kamar.\nTerdapat beberapa beberapa kolom yang tidak digunakan pada analisis ini\n\nHotel: name, description, link, address, dan image.\nKamar: id, roomType, description, dan bedDescription.\n\nTerdapat beberapa kolom dengan total nilai unik kurang dari 2\n\nHotel: type dan city.\nKamar: bedDescription dan numExtraBeds\n\nSetelah dilakukan penghapusan kolom selanjutnya tabel akan di-merge menjadi satu dataframe.\nstarRating memiliki 8 nilai unik, perlu di teliti lebih lanjut untuk detailnya.\n\n\n\nMenghapus Kolom\nhotelDrop = ['name', 'description', 'link', 'address', 'image', 'type', 'city']\nroomDrop = ['id', 'roomType', 'description', 'bedDescription', 'numExtraBeds']\n\ndf_hotel = df_sql_hotel.drop(hotelDrop, axis=1)\ndf_room = df_sql_room.drop(roomDrop, axis=1)\n\nprint('Total Hotel Table Data : ', df_hotel.shape[0])\nprint('Total Hotel Table Column : ', df_hotel.shape[1])\nprint('-' * 30)\nprint('Total Room Table Data : ', df_room.shape[0])\nprint('Total Room Table Column : ', df_room.shape[1])\n\n\nMerge Data\n# Rename column\ndf_hotel.rename(columns={'id': 'hotelId'}, inplace=True)\ndf_hotel.rename(columns={'facilities': 'hotelFacilities'}, inplace=True)\ndf_room.rename(columns={'facilities': 'roomFacilities'}, inplace=True)\n\nMenyamakan nama id pada tabel hotel dan hotelId pada tabel kamar.\nMenambah prefix hotel dan room pada tiap kolom facilities masing-masing tabel.\n\n# merge hotel dan room data\ndf = pd.merge(df_hotel, df_room, on='hotelId', how='inner')\n\n# remove id column\ndf.drop(columns=['hotelId'], inplace=True)\n\n# re arrange column\ndf = df[['originalRate', 'starRating', 'builtYear', 'size', 'baseOccupancy', 'maxChildAge',\n         'maxChildOccupancy', 'isBreakfastIncluded', 'isWifiIncluded', 'isRefundable',\n         'hasLivingRoom', 'hotelFacilities', 'roomFacilities', 'nearestPointOfInterests']]\n\ndf\n\nDimensi pasca penggabungan adalah (1199, 14)\nMenghapus kolom hotelId karena sudah tidak diperlukan lagi.\nMengubah urutan kolom untuk mempermudah analisis, target kolom berada di kiri dan kolom array berada di kanan.\n\n\n\nTarget Processing\nfor i in range(0, 5):\n    print(df.loc[i, 'originalRate'])\n\nData berbentuk JSON atau Dictionary, maka perlu diubah menjadi nilai harga dengan 1 nilai.\nKarena semua data menggunakan matauang IDR, maka currency tidak diperlukan.\n\n# Extract the amount from originalRate using a lambda function\ndf['rate'] = df['originalRate'].apply(lambda x: json.loads(x)['amount'])\ndf['tax'] = df['originalRate'].apply(lambda x: json.loads(x)['tax'])\ndf = df.drop(columns=['originalRate'])\n# check tipe data\nprint('Tipe data harga :', df['rate'].dtype)\nprint('Tipe data pajak :', df['tax'].dtype)\n# ubah tipe data\ndf['rate'] = df['rate'].astype('int')\ndf['tax'] = df['tax'].astype('int')\n\nprint('Tipe data harga :', df['rate'].dtype)\nprint('Tipe data pajak :', df['tax'].dtype)\n\nRasio Pajak\nTahap ini bertujuan untuk mengetahui rasio pajak dari harga kamar hotel. Rasio pajak ini akan digunakan untuk menghitung harga kamar hotel setelah dikenakan pajak.\n\n# create series for original rate\noriginal_rate = df['rate']\n# create series for tax\ntax = df['tax']\n# create dataframe for original rate, tax, and tax rate\ndf_rate = pd.DataFrame({'original_rate': original_rate, 'tax': tax})\ndf_rate['tax_rate'] = df_rate['tax'] / df_rate['original_rate'] * 100\n\ndf_rate\n# count number of data with tax rate 20% and under 21%, also over 21%\ncount0 = 0\ncount_20 = 0\ncountBetween = 0\nfor i in range(len(df_rate['tax_rate'])):\n    if df_rate['tax_rate'][i] &lt; 20 and df_rate['tax_rate'][i] &gt; 0:\n        countBetween += 1\n    elif df_rate['tax_rate'][i] &gt;= 20:\n        count_20 += 1\n    elif df_rate['tax_rate'][i] == 0:\n        count0 += 1\nprint('Median pajak : ', df_rate['tax_rate'].median())\nprint('null / 0% pajak: ', count0)\nprint('Pajak diantara 0 - 20%:', countBetween)\nprint('Rasio pajak diatas 20% :', count_20)\n\n\n# plot for tax rate and give the total value on the top of the bar if the value is 0% it will not show\nsns.set(rc={'figure.figsize': (10, 7)})\nsns.set_style('darkgrid')\nax = sns.histplot(df_rate['tax_rate'], kde=False, color='dodgerblue', bins=9)\nax.set(xlabel='Tax Rate (%)', ylabel='Count')\nax.set_title('Tax Rate Distribution')\ntotal = len(df_rate['tax_rate'])\nfor p in ax.patches:\n    height = p.get_height()\n    if height != 0:\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 15,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\")\nplt.show()\n# Menghapus kolom pajak\ndf = df.drop(columns=['tax'])\n\n# Mengubah target kolom menjadi di awal\n# sekedar untuk merapikan dataframe\ndf = df[['rate'] + [col for col in df.columns if col != 'rate']]\ndf.columns\n\nPajak hotel di Yogyakarta ada di kisaran 20-22% dengan median 21%\nKarena 94% data memiliki pajak dikisaran tersebut maka nilai pajak dianggap 21%(median) secara keseluruhan.\n\n\n\n\nValidaasi Data\n\nPengecekan Rating Bintang\n# starRating Distribution\nvalue = df.starRating.value_counts()\nprint('OriginalRate Distribution by starRating')\nprint(value)\n\n# starRating Distribution by percentage\nvalue_percentage = value / len(df) * 100\n\n# create a list of tuples where each tuple contains the value and index of each element in the value_percentage series\nvalue_percentage_list = [(value_percentage[i], i)\n                         for i in value_percentage.index]\n\n# sort the list by the value in descending order\nvalue_percentage_list_sorted = sorted(value_percentage_list, reverse=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 0.5]})\nsns.histplot(df, x=\"rate\", hue='starRating', palette='bright',\n             ax=ax[0]).set(title='OriginalRate Distribution by starRating')\n\n# starRating percentage plot\nsns.barplot(x=value_percentage.index, y=value_percentage.values,\n            palette='bright', ax=ax[1]).set(title='starRating percentage')\n\n# add the percentage text using the sorted list\nfor container in ax[1].containers:\n    for bar in container.patches:\n        v = bar.get_height()\n        bar_center = bar.get_x() + bar.get_width() / 2\n        ax[1].text(bar_center, v + 0.5,\n                   f'{v:.2f}%', color='black', fontweight='bold', ha='center')\nfig.tight_layout()\n\nRating bintang memiliki beberapa nilai dengan 0.5 (desimal), tetapi nilai tersebut hanya memiliki persentase jumlah data yang sedikit, maka dari itu rating tersebut dihilangkan angka desimalnya dari rating seharusnya.\n\n# ubah starRating dengan angka bulat\ndf['starRating'] = df['starRating'].replace(2.5, 2)\ndf['starRating'] = df['starRating'].replace(3.5, 3)\n# starRating Distribution\nvalue = df.starRating.value_counts()\nprint('OriginalRate Distribution by starRating')\nprint(value)\n\n# starRating Distribution by percentage\nvalue_percentage = value / len(df) * 100\n\n# create a list of tuples where each tuple contains the value and index of each element in the value_percentage series\nvalue_percentage_list = [(value_percentage[i], i)\n                         for i in value_percentage.index]\n\n# sort the list by the value in descending order\nvalue_percentage_list_sorted = sorted(value_percentage_list, reverse=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 0.5]})\nsns.histplot(df, x=\"rate\", hue='starRating', palette='bright',\n             ax=ax[0]).set(title='OriginalRate Distribution by starRating')\n\n# starRating percentage plot\nsns.barplot(x=value_percentage.index, y=value_percentage.values,\n            palette='bright', ax=ax[1]).set(title='starRating percentage')\n\n# add the percentage text using the sorted list\nfor container in ax[1].containers:\n    for bar in container.patches:\n        v = bar.get_height()\n        bar_center = bar.get_x() + bar.get_width() / 2\n        ax[1].text(bar_center, v + 0.5,\n                   f'{v:.2f}%', color='black', fontweight='bold', ha='center')\nfig.tight_layout()\n\nPersentase persebaran data tiap rating bintang sudah lebih baik setelah dilakukan pengubahan nilai rating bintang.\nTerlihat pada plot yang kiri bahwa terdapat ekor yang sangat panjang, ini menunjukkan adanya outlier pada kolom harga(rate)\n\ndf.info()\n\nKolom starRating masih bertipe data float walau sudah tidak memiliki angka desimal, maka perlu akan menjadi int\nKolom builtYear harus diganti ke tipe data int\nKolom size harus diganti ke tipe data float (tipe data dasar dari tabel sqlite adalah float)\nTerdapat nilai null pada kolom builtYear dan size yang harus ditangani\n\ndf['starRating'] = df['starRating'].astype('int')\nprint('Tipe data starRating :', df['starRating'].dtype)\n\n\nData Cleaning\n\nCheck Duplicate Data\n# show index who has duplicate value\nprint('Total duplicated row = ', df.duplicated().sum())\n# print duplicated data list index 1\ndf[df.duplicated(keep=False)]\n# drop duplicate data\ndf = df.drop_duplicates(keep='first')\ndf.shape\n\nDengan menggunakan parameter keep = 'first' maka data yang duplikat akan dihapus kecuali data pertama yang muncul.\n\n\n\nCheck Null\n# Jumlah baris data\njumlah_baris_ori = df.shape[0]\n\n# crate dataframe for null value\ndf_null = pd.DataFrame(df.isnull().sum(), columns=['null_value'])\ndf_null['null_value_percentage'] = df_null['null_value'] / len(df) * 100\ndf_null\n\nTerdapat 2 data yang memiliki nilai null dengan persentase yang cukup tinggi, yaitu kolom builtYear dan size. Oleh karena itu data tersebut akan diubah dengan nilai median per rating hotel.\n\n# create new dataframe for null value rows\ndf_null_rows = df[df.isnull().any(axis=1)]\ndf_null_rows\n# ubah sementara null value menjadi 0\ndf['builtYear'] = df['builtYear'].fillna(0)\ndf['size'] = df['size'].fillna(0)\n\n# ubah tipe data\ndf['builtYear'] = df['builtYear'].astype('int32')\ndf['size'] = df['size'].astype('float')\n\nprint('Tipe data builtYear :', df['builtYear'].dtype)\nprint('Tipe data size :', df['size'].dtype)\n\n\n\n\n\n\nNote\n\n\n\nKolom yang memiliki nilai null akan membuat tipe data menjadi object, maka dari itu pada penelitian ini akan diisi dengan nilai 0 terlebih dahulu, kemudian kolom tersebut diubah tipe datanya\n\n\n# ubah nilai 0 pada kolom builtYear menjadi median tiap starRating\nfor i in df['starRating'].unique():\n    df.loc[(df['starRating'] == i) & (df['builtYear'] == 0),\n           'builtYear'] = df[df['starRating'] == i]['builtYear'].median()\n\n# ubah nilai 0 pada kolom size menjadi median tiap starRating\nfor i in df['starRating'].unique():\n    df.loc[(df['starRating'] == i) & (df['size'] == 0),\n           'size'] = df[df['starRating'] == i]['size'].median()\n    \n# crate dataframe for null value\ndf_null = pd.DataFrame(df.isnull().sum(), columns=['null_value'])\ndf_null['null_value_percentage'] = df_null['null_value'] / len(df) * 100\ndf_null\n\nData sudah tidak memiliki nilai null\n\n\n\n\nStatistik Deskriptif\ndf.describe()\n\nPada kolom builtYear terdapat nilai minimum 1 yang tidak mungkin terjadi, maka data tersebut akan dihapus.\nnilai median pada rate dan size terpaut cukup jauh dengan nilai maximum, ini menunjukkan adanya outlier pada kolom rate.\n\n\nBuilt Year Data Handling\n# Cek nilai unique pada kolom builtYear dibawah 2000\nprint('Nilai unique builtYear dibawah 2000 :')\nprint(df[df['builtYear'] &lt; 2000]['builtYear'].unique())\n\nprint('Nilai unique bulitYear dibawah 1900 :')\nprint(df[df['builtYear'] &lt; 1900]['builtYear'].unique())\n\nTerdapat data yang memiliki nilai builtYear yang tidak mungkin terjadi, maka data tersebut akan dihapus.\nTidak terdapat hotel dibawah tahun 1900, maka dari itu data yang disimpan adalah data diatas tahun 1900.\n\n# menghapus baris yang memiliki nilai dibawah 1900 pada kolom builtYear\ndf = df[df['builtYear'] &gt; 1900]\n\n# Cek nilai unique pada kolom builtYear dibawah 2000\nprint('Nilai unique builtYear dibawah 2000 :')\nprint(df[df['builtYear'] &lt; 2000]['builtYear'].unique())\n\n\n\nOutlier Handling\n\nRate Data\n# Statistik Harga\nprint('Harga')\nprint(f'maximum value : {df.rate.max()}')\nprint(f'minimum value : {df.rate.min()}')\nprint(f'skew value : {round(df.rate.skew(), 2)}')\n\n# Distribusi harga\nsns.set_style('darkgrid')\nplt.figure(figsize=(20, 10), dpi=80)\nsns.displot(df, x=\"rate\", kind=\"kde\", fill=True).set(\n    title='OriginalRate Distribution')\nplt.show()\n\nKolom rate memiliki nilai skew yang cukup tinggi, selain itu dari plot terlihat memiliki ekor yang cukup panjang. Ini menunjukkan adanya outlier pada kolom rate.\nPenghapusan outlier dilakukan dengan menggunakan metode IQR.\n\n\n\n\n\n\n\nNote\n\n\n\nUntuk penjelasan lebih lanjut mengenai skew dapat dilihat disini\n\n\n# Hitung outlier pada kolom rate\nQ1 = df['rate'].quantile(0.25)\nQ3 = df['rate'].quantile(0.75)\nIQR = Q3 - Q1\n\nprint('Batas bawah :', Q1 - (1.5 * IQR))\nprint('Batas atas :', Q3 + (1.5 * IQR))\n\n# Hitung jumlah outlier\ntotal_outlier = len(df[(df['rate'] &lt; (Q1 - (1.5 * IQR))) | (df['rate'] &gt; (Q3 + (1.5 * IQR)))])\nprint('Jumlah outlier :', total_outlier)\n\nTerdapat 91 data outlier pada kolom rate.\n\n# Hapus outlier\ndf = df[(df['rate'] &gt; (Q1 - (1.5 * IQR))) & (df['rate'] &lt; (Q3 + (1.5 * IQR)))]\ndf.describe()\n\nNilai maksimum sudah cukup menurun setelah dilakukan penghapusan outlier.\nNilai maksimum pada kolom size juga ikut menurun.\n\n# Statistik Harga\nprint('Harga')\nprint(f'maximum value : {df.rate.max()}')\nprint(f'minimum value : {df.rate.min()}')\nprint(f'skew value : {round(df.rate.skew(), 2)}')\n\n# Distribusi harga\nsns.set_style('darkgrid')\nplt.figure(figsize=(20, 10), dpi=80)\nsns.displot(df, x=\"rate\", kind=\"kde\", fill=True).set(\n    title='OriginalRate Distribution')\nplt.show()\n\nNilai skew pada kolom rate sudah menjadi lebih baik setelah dilakukan penghapusan outlier.\n\n\n\nSize Data\n# Statistik Size\nprint('Size')\nprint('maximum value : {}'.format(df['size'].max()))\nprint('minimum value : {}'.format(df['size'].min()))\nprint('skew value : {}'.format(df['size'].skew()))\n\n# Distribusi size\nplt.figure(figsize=(20, 10), dpi=80)\nsns.set_style('darkgrid')\nsns.jointplot(data=df, x='size', y='rate')\nplt.show()\n\nNilai skew sudah sangat mendekati angka 1, ini menunjukkan bahwa distribusi data sudah sangat baik.\n\n# Hasil data cleaning\nprint('Total baris data awal :', jumlah_baris_ori)\nprint('Total baris data yang dihapus :', jumlah_baris_ori - df.shape[0])\nprint('Total baris data setelah cleaning :', df.shape[0])\ndf.isnull().sum()\n\n\n\n\nEncoding Data\ndf = df.reset_index(drop=True)\ndf.info()\n\nKolom hotelfacilities, roomfacilities, dan nearestPointofInterests merupakan sebuah fitur dengan multi label. Oleh karena itu data tersebut akan dilakukan multi-hot encoding.\nProses tersebut akan dilakukan dengan library sklearn.preprocessing.MultiLabelBinarizer\n\n\n\n\n\n\n\nNote\n\n\n\nUntuk penjelasan lebih lanjut tentang multi-label dan multi-class dapat dilihat disini\n\n\n\nCheck data format\ndf['hotelFacilities'].head(2)\ndf['roomFacilities'].head(2)\ndf['nearestPointOfInterests'].head(2)\n\nroomfacilities dan hotelFacilities memiliki format yang sama, yaitu list yang berisi string.\nnearestPointofInterests memiliki format yang berbeda, yaitu list yang berisi dictionary/json yang berisi string dan float.\n\n\n\nData Preprocessing\n# create a MultiLabelBinarizer object\nmlb = MultiLabelBinarizer()\n\nDaftar kolom hasil encoding akan diexport menjadi file pkl yang akan digunakan pada aplikasi streamlit.\n\n# reformat kolom hotelFacilities\ndf['hotelFacilities'] = df['hotelFacilities'].apply(eval)\n\n# multi label binarizer untuk kolom hotelFacilities dengan preifx Hotel_\nhotel_facilities = pd.DataFrame(mlb.fit_transform(\n    df['hotelFacilities']), columns=[f'Hotel_{col}' for col in mlb.classes_])\n\nhotelNewCol = hotel_facilities.shape[1]\nprint('Jumlah kolom :', hotel_facilities.shape[1])\n\n# export hotel_facilities with pickle\nhotelFacilities = hotel_facilities.columns.tolist()\npickle.dump(hotelFacilities, open('hotelFacilities.pkl', 'wb'))\n    \nhotel_facilities.head(2)\n# reformat kolom roomFacilities\ndf['roomFacilities'] = df['roomFacilities'].apply(eval)\n\n# multi label binarizer untuk kolom roomFacilities dengan preifx Room_\nroom_facilities = pd.DataFrame(mlb.fit_transform(df['roomFacilities']), columns=[\n                               f'Room_{col}' for col in mlb.classes_])\n\nroomNewCol = room_facilities.shape[1]\nprint('Jumlah kolom :', roomNewCol)\n\n# export room_facilities with pickle\nroomFacilities = room_facilities.columns.tolist()\npickle.dump(roomFacilities, open('roomFacilities.pkl', 'wb'))\n\nroom_facilities.head(2)\n# reformat kolom nearestPointOfInterests\ndf['nearestPointOfInterests'] = df['nearestPointOfInterests'].apply(\n    lambda x: [item['landmarkType'] for item in json.loads(x)])\n\n# multi label binarizer untuk kolom nearestPointOfInterests dengan preifx Point_\npointOfInterests = pd.DataFrame(mlb.fit_transform(\n    df['nearestPointOfInterests']), columns=[f'Point_{col}' for col in mlb.classes_])\n\npointNewCol = pointOfInterests.shape[1]\nprint('Jumlah kolom :', pointNewCol)\n\n# export pointOfInterests with pickle\npointInterests = pointOfInterests.columns.tolist()\npickle.dump(pointInterests, open('pointInterests.pkl', 'wb'))\n\npointOfInterests.head(2)\n\nMenggabungkan hasil encoding\n# Total kolom encoding\ntotalNewCol = hotelNewCol + roomNewCol + pointNewCol\nprint('Total kolom encoding :', totalNewCol)\n# menghapus kolom hotelFacilities, roomFacilities, dan nearestPointOfInterests\ndf = df.drop(columns=['hotelFacilities', 'roomFacilities', 'nearestPointOfInterests'])\n\nprint('df shape :', df.shape)\nprint('hotel_facilities shape :', hotel_facilities.shape)\nprint('room_facilities shape :', room_facilities.shape)\nprint('pointOfInterests shape :', pointOfInterests.shape)\n\ndf = pd.concat([df, hotel_facilities, room_facilities,\n               pointOfInterests], axis=1)\ndf.head()\ndf.info()\n\n\n\n\nExport Data ke CSV\ndf.to_csv('kamar-hotel-yogyakarta.csv', index=False)\ncol = df.columns\n\n# export col with pickle\npickle.dump(col, open('col.pkl', 'wb'))\nprint(col)\n\n\nData Anlisis\nvalue = df.starRating.value_counts()\nprint(value)\n\n# starRating Distribution by percentage\nvalue_percentage = value / len(df) * 100\n\n# create a list of tuples where each tuple contains the value and index of each element in the value_percentage series\nvalue_percentage_list = [(value_percentage[i], i)\n                         for i in range(len(value_percentage))]\n\n# sort the list by the value in descending order\nvalue_percentage_list_sorted = sorted(value_percentage_list, reverse=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 0.5]})\nsns.histplot(df, x=\"rate\", hue='starRating', palette='bright',\n             ax=ax[0]).set(title='Rate Distribution by starRating')\n\nsns.barplot(x=value_percentage.index, y=value_percentage.values,\n            palette='bright', ax=ax[1]).set(title='starRating percentage')\n\n# add the percentage text using the sorted list\nfor i, (v, index) in enumerate(value_percentage_list_sorted):\n    ax[1].text(index, v + 0.5, str(round(v, 2)) + '%',\n               color='black', fontweight='bold', ha='center')\n\nfig.tight_layout()\nfiltered_0 = df[df['starRating'] == 0.0]\nfiltered_1 = df[df['starRating'] == 1.0]\nfiltered_2 = df[df['starRating'] == 2.0]\nfiltered_3 = df[df['starRating'] == 3.0]\nfiltered_4 = df[df['starRating'] == 4.0]\nfiltered_5 = df[df['starRating'] == 5.0]\n\nprint('Skew value for every starRating')\nprint(df.groupby('starRating')['rate'].skew())\n\n# OriginalRate Distribution by starRating using hisplot inside subplot\nfig, ax = plt.subplots(2, 3, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 1, 1]})\nsns.histplot(filtered_0, x=\"rate\", ax=ax[0, 0]).set(\n    title='Rate Distribution by 0 starRating')\nsns.histplot(filtered_1, x=\"rate\", ax=ax[0, 1]).set(\n    title='Rate Distribution by 1 starRating')\nsns.histplot(filtered_2, x=\"rate\", ax=ax[0, 2]).set(\n    title='Rate Distribution by 2 starRating')\nsns.histplot(filtered_3, x=\"rate\", ax=ax[1, 0]).set(\n    title='Rate Distribution by 3 starRating')\nsns.histplot(filtered_4, x=\"rate\", ax=ax[1, 1]).set(\n    title='Rate Distribution by 4 starRating')\nsns.histplot(filtered_5, x=\"rate\", ax=ax[1, 2]).set(\n    title='Rate Distribution by 5 starRating')\nfig.tight_layout()\n# Statistik Harga tiap rating bintang hotel\n\ndfRateStat = df.groupby('starRating').agg(\n    {'rate': ['mean', 'std', 'min', 'max', lambda x: x.quantile(0.25), 'median', lambda x: x.quantile(0.75)]})\n\n# change the column name from index 4 and 6\ndfRateStat = dfRateStat.rename(\n    columns={'&lt;lambda_0&gt;': '25%', '&lt;lambda_1&gt;': '75%'})\ndfRateStat\n\nPada kota Yogyakarta tidak terdapat banyak hotel bintang 5.\nMayoritas hotel di Yogyakarta adalah hotel bintang 0.\nTerdapat hotel bintang 2 yang memiliki harga setara dengan hotel bintang 5.\n\n\n\n\n\n\n\nNote\n\n\n\nDari tabel statistik tersebut mengindikasikan beberapa nilai yang tidak wajar. Untuk penelitian selanjutnya bisa dilakukan pembersihan data lebih mendalam lagi.\n\n\n\n\n\n\n\n\nNote\n\n\n\nHasil dari multi-hot encoding juga belum dilakukan pembersihan data, selain itu dengan banyaknya hasil kolom juga dapat dilakukan reduksi dimensi, contohnya menggunakan PCA. Oleh karena itu untuk penelitian selanjutnya bisa dilakukan pembersihan data lebih mendalam lagi dan dilakukan reduksi dimensi.\n\n\nMasih banyak informasi-informasi yang dapat di ambil dari data ini, seperti:\n\nPerbandingan harga hotel bintang 5 dengan hotel bintang 0.\nLandmark apa yang paling banyak dicari oleh pengunjung hotel?\nFasilitas apa yang sudah manjadi standar pada hotel bintang 3?\nDan masih banyak lagi."
  },
  {
    "objectID": "project_regresi.html#pemodelan-machine-learning",
    "href": "project_regresi.html#pemodelan-machine-learning",
    "title": "Studi Kasus 2",
    "section": "Pemodelan Machine Learning",
    "text": "Pemodelan Machine Learning\nImport Libraries\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import train_test_split, HalvingGridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom IPython import display\nSeparate and Splitting Data\n\n# separate the feature and target\nx = df.iloc[:, 1:]\ny = df.iloc[:, 0]\n\n# splitting data into data train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n\ntrainRatio = round(x_train.shape[0]/len(df), 2)*100\ntestRatio = round(x_test.shape[0]/len(df), 2)*100\n\nprint(f'Train set: {x_train.shape[0]} ({trainRatio}%)')\nprint(f'Test set: {x_test.shape[0]} ({testRatio}%)')\nInisiasi Model\n\nxgReg = xgb.XGBRegressor(           # Berikut adalah parameter default dari XGBRegressor,   jika tidak diberikan parameter maka akan menggunakan parameter tersebut.\n    objective='reg:squarederror',\n    eval_metric='rmse',\n)\n\nsvr = SVR(\n    cache_size=1500,\n)\n\nrf = RandomForestRegressor()\n\nscoring = ['neg_mean_squared_error','r2']\nTrain and Evaluation Data\n\nxgReg.fit(x_train, y_train, verbose=False)\n\nprint('Train evaluation')\npreds = xgReg.predict(x_train)\nmse = mean_squared_error(y_train, preds)\nrmse = np.sqrt(mse)\nprint('RMSE: ', '{0:,.0f}'.format(rmse))\nprint('R2: ', round(r2_score(y_train, preds), 3))\nprint('-'*20)\n\nprint('Test evaluation')\npreds = xgReg.predict(x_test)\nmse = mean_squared_error(y_test, preds)\nrmse = np.sqrt(mse)\nprint('RMSE: ', '{0:,.0f}'.format(rmse))\nprint('R2: ', round(r2_score(y_test, preds), 3))\n\nsvr.fit(x_train, y_train)\n\nprint('Train evaluation')\npreds = svr.predict(x_train)\nmse = mean_squared_error(y_train, preds)\nrmse = np.sqrt(mse)\nprint('RMSE: ', '{0:,.0f}'.format(rmse))\nprint('R2: ', round(r2_score(y_train, preds), 3))\nprint('-'*20)\n\nprint('Test evaluation')\npreds = svr.predict(x_test)\nmse = mean_squared_error(y_test, preds)\nrmse = np.sqrt(mse)\nprint('RMSE: ', '{0:,.0f}'.format(rmse))\nprint('R2: ', round(r2_score(y_test, preds), 3))\n\nrf.fit(x_train, y_train)\n\nprint('Train evaluation')\npreds = rf.predict(x_train)\nmse = mean_squared_error(y_train, preds)\nrmse = np.sqrt(mse)\nprint('RMSE: ', '{0:,.0f}'.format(rmse))\nprint('R2: ', round(r2_score(y_train, preds), 3))\nprint('-'*20)\n\nprint('Test evaluation')\npreds = rf.predict(x_test)\nmse = mean_squared_error(y_test, preds)\nrmse = np.sqrt(mse)\nprint('RMSE: ', '{0:,.0f}'.format(rmse))\nprint('R2: ', round(r2_score(y_test, preds), 3))\nSelect Parameters for Tuning\n\nxgParams = {\n    'colsample_bytree': [0.5, 0.6, 0.7, 0.8],\n    'learning_rate': [0.01, 0.05, 0.1,],\n    'max_depth': np.arange(3,30,10),\n    'reg_alpha': np.arange(0,5,1),\n    'reg_lambda': np.arange(0,5,1),\n    'subsample': [0.5, 0.6, 0.7, 0.8],\n}\n\nsvrParams = {\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n    'C': [ 1000, 10000, 25000, 50000],\n}\n\nrfParams = {\n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_depth': [None, 5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2', None],\n}\nTuning Parameter XGB Model\n\n# Array for evaluation of models\nMODEL = []\nTEST = []\nTRAIN = []\n\ndef eval(model,testR2, testRMSE,trainR2, trainRMSE):\n    MODEL.append(model)\n    TEST.append({'r2': round(testR2, 2), 'rmse': testRMSE})\n    TRAIN.append({'r2': round(trainR2, 2), 'rmse': trainRMSE})\n\ngridXGB = HalvingGridSearchCV(\n    xgReg, \n    param_grid=xgParams,\n    cv=3, \n    n_jobs=-1,\n    verbose=3,\n    scoring='neg_mean_squared_error',\n    refit=True,\n    random_state=123,\n)\ngridXGB.fit(x_train, y_train)\n\nprint('Train evaluation')\npreds = gridXGB.predict(x_train)\nmse = mean_squared_error(y_train, preds)\nrmseTrain = np.sqrt(mse)\nr2Train = round(r2_score(y_train, preds), 2)\nprint('RMSE: ', '{0:,.0f}'.format(rmseTrain))\nprint('R2: ', r2Train)\nprint('-'*20)\n\nprint('Test evaluation')\npreds = gridXGB.predict(x_test)\nmseTest = mean_squared_error(y_test, preds)\nrmseTest = np.sqrt(mseTest)\nr2Test = round(r2_score(y_test, preds), 2)\nprint('RMSE: ', '{0:,.0f}'.format(rmseTest))\nprint('R2: ', r2Test)\n\neval('XGBRegressor', r2Test, rmseTest, r2Train, rmseTrain)\n\nprint(gridXGB.best_params_)\n\nplt.figure(figsize=(5, 5))\nplt.scatter(preds, y_test, color='r', s=5)\nplt.xlabel('Prediction')\nplt.ylabel('Actual')\nplt.plot([0, 15e5], [0, 15e5])\nplt.savefig('../Picture/XGBprediction.png', bbox_inches='tight', dpi=400)\nplt.show()\n\nTuning Parameter Random Forest Model\n\ngridRF = HalvingGridSearchCV(\n    rf,\n    param_grid=rfParams,\n    cv=3, \n    n_jobs=-1,\n    verbose=3,\n    scoring='neg_mean_squared_error',\n    refit=True,\n    random_state=123\n)\ngridRF.fit(x_train, y_train)\n\nprint('Train evaluation')\npreds = gridRF.predict(x_train)\nmse = mean_squared_error(y_train, preds)\nrmseTrain = np.sqrt(mse)\nr2Train = round(r2_score(y_train, preds), 2)\nprint('RMSE: ', '{0:,.0f}'.format(rmseTrain))\nprint('R2: ', r2Train)\nprint('-'*20)\n\nprint('Test evaluation')\npreds = gridRF.predict(x_test)\nmseTest = mean_squared_error(y_test, preds)\nrmseTest = np.sqrt(mseTest)\nr2Test = round(r2_score(y_test, preds), 2)\nprint('RMSE: ', '{0:,.0f}'.format(rmseTest))\nprint('R2: ', r2Test)\n\neval('RandomForest Regressor', r2Test, rmseTest, r2Train, rmseTrain)\n\nplt.figure(figsize=(5, 5))\nplt.scatter(preds, y_test, color='r', s=5)\nplt.xlabel('Prediction')\nplt.ylabel('Actual')\nplt.plot([0, 15e5], [0, 15e5])\nplt.savefig('../Picture/RFprediction.png', bbox_inches='tight', dpi=400)\nplt.show()\n\nTuning Parameter SVR Model\n\ngridSVR = HalvingGridSearchCV(\n    svr,\n    param_grid=svrParams,\n    cv=3,\n    n_jobs=-1,\n    verbose=3,\n    scoring='neg_mean_squared_error',\n    refit=True,\n)\ngridSVR.fit(x_train, y_train)\nprint(gridSVR.best_params_)\n\nprint('Train evaluation')\npreds = gridSVR.predict(x_train)\nmse = mean_squared_error(y_train, preds)\nrmseTrain = np.sqrt(mse)\nr2Train = round(r2_score(y_train, preds), 2)\nprint('RMSE: ', '{0:,.0f}'.format(rmseTrain))\nprint('R2: ', r2Train)\nprint('-'*20)\n\nprint('Test evaluation')\npreds = gridSVR.predict(x_test)\nmseTest = mean_squared_error(y_test, preds)\nrmseTest = np.sqrt(mseTest)\nr2Test = round(r2_score(y_test, preds), 2)\nprint('RMSE: ', '{0:,.0f}'.format(rmseTest))\nprint('R2: ', r2Test)\n\neval('SVR', r2Test, rmseTest, r2Train, rmseTrain)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(preds, y_test, color='r', s=5)\nplt.xlabel('Prediction')\nplt.ylabel('Actual')\nplt.plot([0, 15e5], [0, 15e5])\nplt.savefig('../Picture/SVRprediction.png', bbox_inches='tight', dpi=400)\nplt.show()\nOpen plot from Picture folder\n\nprint('XGBRegressor prediction')\ndisplay.Image('../Picture/XGBprediction.png', width=600, height=600)\n\nprint('RandomForest Regressor prediction')\ndisplay.Image('../Picture/RFprediction.png', width=600, height=600)\n\nprint('SVR prediction')\ndisplay.Image('../Picture/SVRprediction.png', width=600, height=600)\nRefit The Model With The Best Parameters Into Best Model\n\n# refit the model with the best parameters into bestModel\nxgbModel = gridXGB.best_estimator_.fit(x_train, y_train)\nsvrModel = gridSVR.best_estimator_.fit(x_train, y_train)\nrfModel = gridRF.best_estimator_.fit(x_train, y_train)\nexport The Models\nwith open('../Model/xgbModel.pkl', 'wb') as file:\n    pickle.dump(xgbModel, file)\nwith open('../Model/svrModel.pkl', 'wb') as file:\n    pickle.dump(svrModel, file)\nwith open('../Model/rfModel.pkl', 'wb') as file:\n    pickle.dump(rfModel, file)\nStatistik Data\n\nevalData = {\n    'Model': MODEL,\n    'Test R2': [d['r2'] for d in TEST],\n    'Test RMSE': [d['rmse'] for d in TEST],\n    'Train R2': [d['r2'] for d in TRAIN],\n    'Train RMSE': [d['rmse'] for d in TRAIN]\n}\n\ndf_eval = pd.DataFrame(evalData)\ndf_eval.sort_values(by='Test RMSE', ascending=True)"
  },
  {
    "objectID": "project_clustering.html#project-clustering---spotify-playlist-clustering",
    "href": "project_clustering.html#project-clustering---spotify-playlist-clustering",
    "title": "Studi Kasus 3",
    "section": "Project Clustering - Spotify Playlist Clustering",
    "text": "Project Clustering - Spotify Playlist Clustering"
  },
  {
    "objectID": "project_clustering.html#project-overview",
    "href": "project_clustering.html#project-overview",
    "title": "Studi Kasus 3",
    "section": "Project Overview",
    "text": "Project Overview\nProject ini berfungsi untuk membuat sebuah aplikasi untuk melakukan proses clustering terhadap lagu-lagu yang ada di dalam sebuah playlist spotify.\nLagu-lagu tersebut dikelompokan berdasarkan karakteristik lagu (audio feature) yang kita dapatkan dari spotify web API.\nPerlu diingat bahwa untuk project ini, kita akan buat algoritmanya dahulu, mengetesnya di notebook kita, lalu kita akan mengubahnya menjadi file python dan mengupload nya di Streamlit.\n\nSpotify\n\n\n\n \n\n\n\n\n\n\n\nGambar1. Logo Spotify\n\n\n\n\n\nSpotify adalah platform musik digital yang menyediakan layanan streaming musik dan podcast. Musik dan podcast di-stream melalui internet tanpa perlu mengunduhnya secara permanen. Pengguna dapat membuat playlist pribadi dan menyimpan lagu favorit di Library.\n\n\nPython\n\n\n\n \n\n\n\n\n\n\n\nGambar2. Logo Python\n\n\n\n\n\nPython adalah bahasa pemrograman serba guna yang dikembangkan pada awal 1990-an oleh Guido van Rossum. Tidak perlu mendeklarasikan tipe data variabel, dan nilai variabel dapat berubah saat program berjalan. Python mengutamakan kejelasan dan menghindari penggunaan tanda kurung kurawal atau titik koma. Python menyediakan pustaka bawaan (standard library) yang kaya, serta banyak modul dan pustaka dari pihak ketiga yang memperluas fungsionalitasnya.\n\n\nStreamlit\n\n\n\n \n\n\n\n\n\n\n\nGambar3. Logo Streamlit\n\n\n\n\n\nStreamlit adalah framework open-source untuk mengembangkan aplikasi web interaktif dengan menggunakan bahasa pemrograman Python. Tujuannya adalah menyederhanakan proses pembuatan aplikasi web dengan memungkinkan pengembang untuk membuat aplikasi dengan mudah menggunakan kode Python yang sederhana dan familiar.\n\n\nClustering\n\n\n\n \n\n\n\n\n\n\n\nGambar4. Contoh Grafik Clustering\n\n\n\n\n\nClustering adalah proses pengelompokan data atau objek-objek serupa menjadi kelompok-kelompok yang lebih homogen berdasarkan kesamaan fitur atau karakteristik tertentu.\n\n\nK-Means\n\n\n\n \n\n\n\n\n\n\n\nGambar5. Contoh Clustering Menggunakan Scatter Plot\n\n\n\n\n\nAlgoritma k-means adalah metode clustering yang mengelompokkan data menjadi beberapa kelompok berdasarkan jaraknya ke pusat kelompok yang ditentukan secara iteratif.\nUntuk penjelasan Algoritma Clustering K-Means, bisa mengunjungi video ini https://www.youtube.com/watch?v=3XwBZbqo0mQ"
  },
  {
    "objectID": "project_clustering.html#application-flow",
    "href": "project_clustering.html#application-flow",
    "title": "Studi Kasus 3",
    "section": "Application Flow",
    "text": "Application Flow\nUser flow adalah interaksi antara user dan aplikasi ini harus kita pelajari dan petakan secara detail agar aplikasi bisa digunakan dengan user secara nyaman.\nData flow adalah alur dari data yang ada di dalam aplikasi.\nFunction flow adalah struktur dari fungsi yang ada di dalam aplikasi\n\nUser Flow\nUser flow dari aplikasi yang akan kita buat cukup simple :\n\n\n\nData Flow\n\n\n\nGambar7. Data Flow\n\n\n\n\nFunction Flow\n\n\n\nGambar8. Function Flow"
  },
  {
    "objectID": "project_clustering.html#membuat-akun-spotify-developer",
    "href": "project_clustering.html#membuat-akun-spotify-developer",
    "title": "Studi Kasus 3",
    "section": "Membuat Akun Spotify Developer",
    "text": "Membuat Akun Spotify Developer\n\nDaftar Sebagai Spotify Developer\nUntuk mengakses web API spotify, kita harus mendaftar sebagai spotify developer. Berikut langkahnya:\n- Pergi ke https://developer.spotify.com\n- Klik tombol daftar di pojok kanan atas\n- Isi form, klik daftar, verifikasi email\n- Login dengan akun terverifikasi\n- Pergi ke dashboard\nJika anda sudah bisa masuk dashboard, berarti anda sudah terdaftar menjadi developer spotify! Ikuti langkah-langkah selanjutnya.\n\n\nPergi ke dashboard spotify dan tekan create app\n\n\n\nGambar9. Dashboard Spotify Developer\n\n\n\n\nIsi form dengan data\n\n\n\nGambar10. Isi Form di halaman Create App\n\n\nApp name, nama dari aplikasi, misal spotify_playlist_clusterization\nApp description, deskripsi singkat dari aplikasi yang akan dibuat\nWebsite, website personal dari pembuat aplikasi\nRedirect URI, umumnya user akan ditujukan ke URI ini ketika mengalami kegagalan request. namun untuk project ini, kita dapat mengisinya dengan apapun, seperti URL akun github atau linkedin\n\n\nKlik app yang sudah dibuat\n\n\n\nGambar11. Daftar App di Spotify Developer\n\n\n\n\nPergi ke settings, cari ClientID dan ClientSecret\n\n\n\nGambar12. Client ID dan Client Secret dari App"
  },
  {
    "objectID": "project_clustering.html#buat-repositori-di-github",
    "href": "project_clustering.html#buat-repositori-di-github",
    "title": "Studi Kasus 3",
    "section": "Buat Repositori di Github",
    "text": "Buat Repositori di Github\nGithub adalah aplikasi untuk mengontrol versioning dari koding kita. Github akan melacak perubahan yang kita buat di dalam file koding, sehingga kita dapat membandingkan atau kembali ke versi sebelumnya. Dalam project ini, Github juga digunakan untuk proses deployment di Streamlit.\n\nLogin ke Github\n\n\n\nGambar13. Dashboard Github\n\n\nPergi ke Github.com dan login atau daftar akun baru. Jika anda membuat akun baru, jangan lupa verifikasi akun setelah dibuat. Jika anda sudah login, maka dashboard akun anda akan tampil seperti berikut:\n\n\nBuat Repository Baru\n\n\n\nGambar14. Github Repo\n\n\nDi side tab sebelah kiri dashboard anda klik tombol new untuk membuat sebuah repository baru. Repository adalah sebuah tempat untuk menyimpan code-code anda.\nIsi kolom seperti contoh, nama dan deskripsi bebas. Pastikan bahwa repository bersifat public. Lalu klik create repository\n\n\nSimpan Alamat Git (git remote)\n\n\n\nGambar15. Github Remote\n\n\nSetelah membuat repositori, halaman ini akan muncul. Copy alamat git (https://github.com/rif42/githubtest0.git) dan simpan data ini\n\n\nOpen Folder di Code Editor\n\n\n\nGambar16. Open Folder di VSCode\n\n\nBuka code editor anda (contoh VScode atau IntelliJ), buka terminal, lalu open folder dan pilih sebuah folder.\n\n\nBuat File Baru\n\n\n\nGambar17. Buat File Baru\n\n\nBuat file baru, dan namai file nya spotify_clustering.ipynb. File ini adalah sebuah notebook, mirip seperti jupyter notebook. Format notebook sangat bagus untuk eksperimen koding. Setelah file dibuat, tidak perlu diisi apapun.\n\n\nPush File ke Repository\nBuka terminal dengan cara menekan (ctrl + shift + `) atau membuka terminal &gt; new terminal di top bar vscode. Lalu ketik command berikut:\n\ngit add ., command ini berfungsi untuk menyimpan semua perubahan dalam file\n\ngit commit -m \"first\", command ini berfungsi untuk menyimpan commit, sebuah langkah terakhir untuk menyimpan semua perubahan yang ada di dalam repository. Teks yang ada di dalam tanda petik adalah message atau deskripsi dari commit.\n\ngit remote add origin [url], URL disini adalah alamat git yang didapat di langkah 4c (https://github.com/rif42/githubtest0.git)\n\ngit push origin master, mengirim semua commit ke repository secara final"
  },
  {
    "objectID": "project_clustering.html#pengambilan-dataset",
    "href": "project_clustering.html#pengambilan-dataset",
    "title": "Studi Kasus 3",
    "section": "Pengambilan Dataset",
    "text": "Pengambilan Dataset\nDataset dari aplikasi yang kita gunakan adalah lagu-lagu yang ada didalam playlist dari user. Untuk mengambil lagu tersebut, kita perlu membuat sebuah script untuk berkomunikasi dengan spotify web API dan mengambil data yang kita butuhkan.\n\nBuka File Notebook\nBuka code editor anda dan temukan file notebook yang telah dibuat di langkah sebelumnya. Pastikan bahwa git repository telah aktif dengan menulis git status di terminal.\n\n\n\nGambar18. Buka file notebook (.ipynb)\n\n\n\n\nImport Library yang Dibutuhkan\nJika library belum terinstall, maka jalankan command ini di terminal, lalu import\n- pip install streamlit\n- pip install pandas\n- pip install numpy\n- pip install scikit-learn\n- pip install plotly-express\n- pip install nbformat\n- pip install ipykernel\nLibrary yang tidak disebutkan diatas adalah library built in (seperti base64, json, csv), kita tidak perlu menginstall nya, kita hanya perlu mengimpornya saja\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\nimport base64\nfrom requests import post, get\nimport json\nimport csv\nfrom sklearn import preprocessing\n\n\nMasukkan variable client_id, client_secret, dan playlist_url\nclient_id = '97aeaf1e98f943edb1344ab86f71692a' ##ganti variabel dengan client_id milik anda\nclient_secret = '9f35e123caa7490b904ad6bcb98f4ba9' ##ganti variabel dengan client_secret milik anda\nplaylistId = '1dtCMTYzAOzwKXqklxPJNS'\n\n## 37i9dQZF1DXbrUpGvoi3TS - 1(similar sad songs)\n## 1dtCMTYzAOzwKXqklxPJNS - 2(old songs, rock, rap)\n## 0IN7IWKmIfwlEysGyWUuRg - 3(mix of modern electronic, pop, and rock)\n\ndataset = []\ndataset2 = []\ndataset3 = []\nGanti isi variable dengan client id dan client secret yang diperoleh dari akun spotify developer yang kita bahas di slide 14.\nAnda dapat menggunakan contoh URL playlist atau memasukkan URL playlist anda sendiri.\nSetelah itu, buat 3 variable kosong untuk menampung hasil pengolahan data kita di slide selanjutnya.\n\n\nBuat fungsi getToken()\nKita akan menggunakan key Client ID dan Client Secret untuk mendapatkan akses token. Token ini berfungsi untuk memperbolehkan seseorang mengakses dan menggunakan spotify web API Pertama kita gabungkan string Client ID dan Client Secret, lalu kita encode key tersebut menggunakan algoritma enkripsi base64, dan kita kirim key tersebut ke server spotify web API. Panggil fungsi tersebut untuk mengecek apakah token bisa terambil atau tidak.\ndef getToken():\n    # gabungkan client_id dan client_secret\n    auth_string = client_id + ':' + client_secret\n\n    # encode ke base64\n    auth_b64 = base64.b64encode(auth_string.encode('utf-8'))\n\n    # url untuk mengambil token\n    url = 'https://accounts.spotify.com/api/token'\n\n    # header untuk mengambil token - sesuai dengan guide dari spotify\n    headers = {\n        'Authorization': 'Basic ' + auth_b64.decode('utf-8'),\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n\n    # data untuk mengambil token - sesuai dengan guide dari spotify\n    data = {'grant_type': 'client_credentials'}\n\n    # kirim request POST ke spotify\n    result = post(url, headers=headers, data=data)\n\n    # parse response ke json\n    json_result = json.loads(result.content)\n    token = json_result['access_token']\n\n    # ambil token untuk akses API\n    return token\n\n## panggil fungsi getToken() dibawah ini\n\n\nBuat fungsi getAuthHeader()\nFungsi ini berguna untuk mengambil token dan memasukkan token ke sebuah objek header request. Header request adalah sebuah metode identifikasi dan otorisasi di dalam API.\nBearer berarti kita adalah sebuah klien yang meminta data. Di dalam objek header request, kita menyematkan token yang kita dapatkan untuk menandakan bahwa kita sudah mempunyai izin untuk mengakses API.\nFungsi ini akan dipanggil nanti ketika kita akan me-request data dari API\n## pengambilan token untuk otorisasi API\ndef getAuthHeader(token):\n    return {'Authorization': 'Bearer ' + token}\n\n\nBuat fungsi getAudioFeatures()\nFungsi ini berguna untuk mengambil data karakteristik lagu. Fungsi ini mengambil token dan ID track (sebuah lagu).\nToken didapatkan dari pemanggilan fungsi getToken() dan getAuthHeader(). ID track didapatkan dari list lagu yang diambil oleh fungsi getPlaylistItems(). Data karakteristik lagu yang dihasilkan oleh fungsi ini akan disimpan di variabel dataset2.\n## pengambilan audio features dari track (lagu)\ndef getAudioFeatures(token, trackId):\n    # endpoint untuk akses playlist\n    url = f'https://api.spotify.com/v1/audio-features/{trackId}'\n    # ambil token untuk otorisasi, gunakan sebagai header\n    headers = getAuthHeader(token)\n    result = get(url, headers=headers)  # kirim request GET ke spotify\n    json_result = json.loads(result.content)  # parse response ke json\n\n    # ambil data yang diperlukan dari response\n    audio_features_temp = [\n        json_result['danceability'],\n        json_result['energy'],\n        json_result['key'],\n        json_result['loudness'],\n        json_result['mode'],\n        json_result['speechiness'],\n        json_result['acousticness'],\n        json_result['instrumentalness'],\n        json_result['liveness'],\n        json_result['valence'],\n        json_result['tempo'],\n    ]\n    dataset2.append(audio_features_temp)\n\n\nBuat fungsi getPlaylistItems()\nFungsi ini berguna untuk mengambil lagu-lagu yang ada di playlist. Fungsi ini mengambil token dan playlistID sebagai parameter nya. Token didapatkan dari pemanggilan fungsi getToken() dan getAuthHeader(). playlistID adalah variabel yang berisi URL playlist spotify yang nantinya diisi oleh user. Untuk mengambil data dari spotify web API, kita harus menginput URL yang benar, disertai parameter (limit, market, fields) yang dibutuhkan. Semua variabel, ditambah header akan digabungkan dan membuat request ke web API\ndef getPlaylistItems(token, playlistId):\n    # endpoint untuk akses playlist\n    url = f'https://api.spotify.com/v1/playlists/{playlistId}/tracks'\n    limit = '&limit=100'  # batas maksimal track yang diambil\n    market = '?market=ID'  # negara yang tempat aplikasi diakses\n    # format data dari track yang diambil\n    fields = '&fields=items%28track%28id%2Cname%2Cartists%2Cpopularity%2C+duration_ms%2C+album%28release_date%29%29%29'\n    url = url+market+fields+limit  # gabungkan semua parameter\n    # ambil token untuk otorisasi, gunakan sebagai header\n    headers = getAuthHeader(token)\n    result = get(url, headers=headers)  # kirim request GET ke spotify\n    json_result = json.loads(result.content)  # parse response ke json\n    # print(json_result)\nMasih di fungsi yang sama, Hasil request kita yang disebut dengan response, akan ditampung di variabel json_result. Namun kita hanya mengambil beberapa fitur saja. Selanjutnya fitur-fitur tersebut kita masukkan ke variabel dataset.\n    # ambil data yang diperlukan dari response\n    for i in range(len(json_result['items'])):\n        playlist_items_temp = []\n        playlist_items_temp.append(json_result['items'][i]['track']['id'])\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['name'].encode('utf-8'))\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['artists'][0]['name'].encode('utf-8'))\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['popularity'])\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['duration_ms'])\n        playlist_items_temp.append(\n            int(json_result['items'][i]['track']['album']['release_date'][0:4]))\n        dataset.append(playlist_items_temp)\nVariabel dataset tadi berisikan lagu-lagu yang ada di dalam sebuah playlist. Sekarang kita akan ambil karakteristik lagu-lagu tersebut. Untuk mengambil lagu menggunakan fungsi getAudioFeatures() kita membutukan track ID dan token. Jadi, kita akan membuat sebuah for loop di dalam dataset, mengambil trackID nya saja (menggunakan array index 0), lalu kita panggil fungsi getAudioFeatures() dan sematkan trackID dan token sebagai parameternya.\n    for i in range(len(dataset)):\n        getAudioFeatures(token, dataset[i][0])\nHasil dari fungsi getPlaylistItems() disimpan di variabel dataset. Hasil dari fungsi getAudioFeatures() disimpan di variabel dataset2. Selanjutnya, kita akan menggabungkan isi dari kedua variabel kedalam variabel dataset3, lalu meng-export nya menjadi file .csv\n\n    # gabungkan dataset dan dataset2\n    for i in range(len(dataset)):\n        dataset3.append(dataset[i]+dataset2[i])\n\n    print(dataset3)\n    # convert dataset3 into csv\n    with open('dataset.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"id\", \"name\", \"artist\", \"popularity\", \"duration_ms\", \"year\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\",\n                         \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"])\n        writer.writerows(dataset3)\nUntuk langkah terakhit, tulis coding dibawah untuk menjalankan semua fungsi dan menghasilkan dataset.csv\ntoken = getToken() print('access token : '+token) getPlaylistItems(token, playlistId)\nJika kita gabungkan semua coding, maka coding akan menjadi seperti yang dibawah :\n# pengambilan track (lagu) dari playlist\ndef getPlaylistItems(token, playlistId):\n    # endpoint untuk akses playlist\n    url = f'https://api.spotify.com/v1/playlists/{playlistId}/tracks'\n    limit = '&limit=100'  # batas maksimal track yang diambil\n    market = '?market=ID'  # negara yang tempat aplikasi diakses\n    # format data dari track yang diambil\n    fields = '&fields=items%28track%28id%2Cname%2Cartists%2Cpopularity%2C+duration_ms%2C+album%28release_date%29%29%29'\n    url = url+market+fields+limit  # gabungkan semua parameter\n    # ambil token untuk otorisasi, gunakan sebagai header\n    headers = getAuthHeader(token)\n    result = get(url, headers=headers)  # kirim request GET ke spotify\n    json_result = json.loads(result.content)  # parse response ke json\n    # print(json_result)\n\n    # ambil data yang diperlukan dari response\n    for i in range(len(json_result['items'])):\n        playlist_items_temp = []\n        playlist_items_temp.append(json_result['items'][i]['track']['id'])\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['name'].encode('utf-8'))\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['artists'][0]['name'].encode('utf-8'))\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['popularity'])\n        playlist_items_temp.append(\n            json_result['items'][i]['track']['duration_ms'])\n        playlist_items_temp.append(\n            int(json_result['items'][i]['track']['album']['release_date'][0:4]))\n        dataset.append(playlist_items_temp)\n\n    # ambil audio features dari semua track di dalam playlist\n    for i in range(len(dataset)):\n        getAudioFeatures(token, dataset[i][0])\n\n    # gabungkan dataset dan dataset2\n    for i in range(len(dataset)):\n        dataset3.append(dataset[i]+dataset2[i])\n\n    print(dataset3)\n    # convert dataset3 into csv\n    with open('dataset.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"id\", \"name\", \"artist\", \"popularity\", \"duration_ms\", \"year\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\",\n                         \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"])\n        writer.writerows(dataset3)\n\ntoken = getToken()\nprint('access token : '+token)\ngetPlaylistItems(token, playlistId)\nJika anda masih bingung, bisa melihat gambar function flow dibawah ini :\n\n\n\nData Preprocessing\nSpotify web API terkenal dengan format dan prosedur API yang sangat tangguh, sehingga bug atau kesalahan dataset jarang ditemui. Namun kita tetap harus mengecek dataset kita.\n\nData Understanding\nSebagai seorang data scientist, kita perlu memahami dataset kita secara sepenuhnya. Berikut adalah penjelasan singkat dari beberapa fitur di dalam dataset yang akan kita gunakan. Untuk deskripsi sepenuhnya, bisa mengunjungi https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features.\n\nAcousticness, Tingkat intensitas akustik di dalam lagu tersebut. 0.0 = Lagu tersebut tidak akustik, 1.0 = Lagu tersebut sangat akustik.\n\nLoudness, Kekuatan suara keseluruhan dari sebuah lagu dalam desibel (dB). Nilai kekuatan suara dihitung rata-rata sepanjang seluruh lagu dan bermanfaat untuk membandingkan kekuatan suara relatif antara lagu-lagu. Kekuatan suara adalah kualitas dari suara yang merupakan korelasi psikologis utama dari kekuatan fisik (amplitudo). Nilai-nilai biasanya berkisar antara -60 hingga 0 dB.\n\nTempo, Rata rata kecepatan atau tempo dari sebuah lagu, diukur menggunakan beats (ketukan) per minute (BPM).\n\n\n\nMuat Data\nKetika coding pengambilan dataset dijalankan, sebuah file csv bernama dataset.csv akan muncul. File ini berisi dataset yang akan kita olah. Muat dataset menggunakan pandas dataframe dan check apakah dataset sudah benar.\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nimport plotly.express as px \n\n## muat dataset\ndata = pd.read_csv('dataset.csv')\ndata.head()\n\n\nDecode - Encode ke base64\nDataset harus di encode-decode agar semua karakter ter-standarisasi dan file bisa dibaca dan diproses, bebas dari korupsi data. Karena data yang diambil dari spotify web API di encode menggunakan bas64, maka kita akan men-decode data tersebut menggunakan algoritma yang sama.\nNamun ketika di decode, muncul efek samping seperti penambahan (‘b) di depan data dan (’) dibelakang data.\nKita akan menghapus efek samping tersebut menggunakan teknik string slicing.\n## Hapus karakter yang tidak perlu pada kolom artist dan name\ndata['artist'] = data['artist'].map(lambda x: str(x)[2:-1])\ndata['name'] = data['name'].map(lambda x: str(x)[2:-1])\n\ndata.head()\n\n\nCek Nilai Kosong\nSebagai efek samping dari encode-decode, terkadang kolom nama kosong. Maka kita harus menemukan data tersebut dan menghapusnya, agar algoritma dapat memproses data yang benar.\n##delete empty string in name column\ndata = data[data['name'] != '']\n\n##reset index\ndata = data.reset_index(drop=True)\ndata.head()\n\n\nFeature Selection\nTujuan utama kita adalah untuk melakukan clustering terhadap karakteristik lagu. Dalam dataset, ada beberapa fitur yang tidak mencerminkan karakteristik dari sebuah lagu, maka kita harus menghapus fitur-fitur tersebut.\nanda bisa melihat deskripsi dari fitur tersebut di section data understanding di slide sebelumnya.\n## drop name artist and year column\ndata2 = data.copy()\ndata2 = data2.drop(['artist', 'name', 'year', 'popularity', 'key','duration_ms', 'mode', 'id'], axis=1)\n\ndata2.head()\n\n\nNormalisasi MinMax\nbeberapa fitur mempunyai batasan nilai yang berbeda-beda dan jaraknya cukup jauh. sebagai contoh, di fitur loudness, batasan nilai adalah -60 sampai -3, sedangkan di variabel instrumentalness, batasan nilai adalah 0 sampai 3.490000e-01. Untuk data min max selengkapnya bisa dicek di code dibawah.\nnilai spread yang cukup besar ini dapat mempengaruhi hasil akhir clusterisasi, maka kita harus melakukan proses standarisasi agar semua fitur mempunyai batasan yang sama, yaitu 0 sampai 1\nfrom sklearn import preprocessing\n\n## normalize all data to 0 and 1\nx = data2.values ##returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndata2 = pd.DataFrame(x_scaled)\n\n## convert to dataframe\ndata2.columns = ['acousticness','danceability','energy','instrumentalness','loudness', 'liveness', 'speechiness', 'tempo','valence']\n\n## data2.head()\ndata2.describe()\n\n\nDimensionality Reduction using Principal Component Analysis (PCA)\nSetelah kita menghapus fitur yang tidak terpakai, kita masih mempunyai 9 fitur. Kita tidak dapat melakukan proses clustering dengan 9 fitur, karena proses clustering hanya bisa dilakukan dengan maksimal 2 fitur. Oleh karena itu diperlukan metode dimensionality reduction dengan Principal Component Analysis untuk mengkondensasi 9 fitur menjadi 2 fitur. Nilai intrinsik yang ada di 9 fitur tidak akan hilang, nilai tersebut akan direpresentasikan ke bentuk yang lebih kecil, menjadi 2 fitur.\n## Lakukan PCA untuk mengurangi jumlah fitur menjadi 2 fitur saja\npca = PCA(n_components=2)\npca.fit(data2)\npca_data = pca.transform(data2)\n\n## buat dataframe dari hasil pca\npca_df = pd.DataFrame(data=pca_data, columns=['x', 'y'])\npca_df.head()\n\n## plot pca_df using plotly\nfig = px.scatter(pca_df, x='x', y='y', title='PCA')\nfig.show()"
  },
  {
    "objectID": "project_clustering.html#clustering-menggunakan-k-means",
    "href": "project_clustering.html#clustering-menggunakan-k-means",
    "title": "Studi Kasus 3",
    "section": "Clustering Menggunakan K-Means",
    "text": "Clustering Menggunakan K-Means\nClustering adalah proses pengelompokan data atau objek-objek serupa menjadi kelompok-kelompok yang lebih homogen berdasarkan kesamaan fitur atau karakteristik tertentu.\nAlgoritma k-means adalah metode clustering yang mengelompokkan data menjadi beberapa kelompok berdasarkan jaraknya ke pusat kelompok yang ditentukan secara iteratif.\nDataset akan kita ubah bentuknya menjadi list menggunakan fungsi zip.\nLalu kita aplikasikan algoritma K-Means menggunakan parameter n-init=10 dan max_iter=1000. N-init berfungsi untuk menambah konsistensi hasil dan max_iter=1000 untuk menambah akurasi clustering.\nLalu kita buat graf scatter plot menggunakan library plotly-express\n## rubah bentuk data ke list \ndata2 = list(zip(pca_df['x'], pca_df['y']))\n\n## fit kmeans model\nkmeans = KMeans(n_init=10, max_iter=1000).fit(data2)\n\n## make scatter plot using plotly\nfig = px.scatter(pca_df, x='x', y='y', color=kmeans.labels_, color_continuous_scale='rainbow', hover_data=[data.artist, data.name])\nfig.show()"
  },
  {
    "objectID": "project_clustering.html#deploy-ke-streamlit",
    "href": "project_clustering.html#deploy-ke-streamlit",
    "title": "Studi Kasus 3",
    "section": "Deploy ke Streamlit",
    "text": "Deploy ke Streamlit\nStreamlit adalah framework open-source untuk mengembangkan aplikasi web interaktif dengan menggunakan bahasa pemrograman Python.\nTujuannya adalah menyederhanakan proses pembuatan aplikasi web dengan memungkinkan pengembang untuk membuat aplikasi dengan mudah menggunakan kode Python yang sederhana dan familiar.\n\nBuat file bernama streamlit_app.py\nLangkah pertama adalah membuat sebuah file baru bernama streamlit_app.py. Kita akan masukkan semua koding yang telah kita buat sebelumnya ke dalam satu file ini.\nKita telah membahas proses pengambilan dataset, dan telah dijelaskan bahwa kita harus membuat beberapa fungsi seperti getToken(), getPlaylistItems(). Masukkan semua fungsi-fungsi tersebut ke dalam file streamlit_app.py.\nSelain itu, pada bab data preprocessing, terdapat banyak potongan code untuk pemrosesan data. Masukkan semua potongan code tersebut ke dalam sebuah function bernama dataProcessing().\nAkhirnya, berikut fungsi-fungsi yang ada di dalam streamlit_app.py:\n- getToken(), untuk mengambil token\n- getAuthHeader(), untuk mengisi header request dengan token yang didapat\n- getAudioFeatures(), untuk mendapatkan data karakteristik lagu\n- getPlaylistItems(), untuk mengambil lagu-lagu yang ada di dalam playlist\n- dataProcessing(), untuk memproses dataset dan menghasilkan clustering\n\n\nTambahkan fungsi dataProcessing()\nTambahkan dataProcessing() (pemanggilan function dataProcessing()) di bagian paling akhir function getAudioFeatures(). Gunanya untuk melanjutkan pemrosesan data secara otomatis.\nSelain itu, fungsi ini berisi beberapa fungsi-fungsi khusus streamlit yang berguna untuk menampilkan grafik dan hasil dari coding.\nBerikut beberapa contoh fungsi-fungsi streamlit:\n- st.write(), untuk menulis teks atau paragraph. Masukkan teks ke dalam parameter fungsi untuk menampilkannya. Anda dapat menggunakan sintaks markdown (seperti # untuk header, dan ** untuk bold) untuk memanipulasi format teks.\n- st.plotly_chart(), untuk menampilkan grafik menggunakan library plotly. Masukkan variable grafik (fig) ke parameter fungsi untuk meggunakannya.\n- st.text_input(), untuk mendapatkan input yang dimasukkan ke text box oleh user. Masukkan teks ke dalam parameter fungsi untuk memberi label dari text box. Hasil input dari user akan dimasukkan ke sebuah variabel yang dipakai oleh fungsi.\n- st.button(), untuk membuat sebuah tombol. Gunakan fungsi if untuk menjalankan sesuatu jika tombol ditekan. Masukkan teks ke dalam parameter fungsi untuk memberi label dari button.\nUntuk coding dari fungsi dataProcessing() dan fungsi driver streamlit bisa di simak dibawah ini :\ndef dataProcessing():\n    data = pd.read_csv('dataset.csv')\n    data\n    st.write(\"## Preprocessing Result\")  # streamlit widget\n\n    data = data[['artist', 'name', 'year', 'popularity', 'key', 'mode', 'duration_ms', 'acousticness',\n                'danceability', 'energy', 'instrumentalness', 'loudness', 'liveness', 'speechiness', 'tempo', 'valence']]\n    data = data.drop(['mode'], axis=1)\n    data['artist'] = data['artist'].map(lambda x: str(x)[2:-1])\n    data['name'] = data['name'].map(lambda x: str(x)[2:-1])\n    st.write(\"### Data to be deleted:\")\n    data[data['name'] == '']\n    data = data[data['name'] != '']\n\n    st.write(\"## Normalization Result\")  # streamlit widget\n    data2 = data.copy()\n    data2 = data2.drop(\n        ['artist', 'name', 'year', 'popularity', 'key', 'duration_ms'], axis=1)\n    x = data2.values\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    data2 = pd.DataFrame(x_scaled)\n    data2.columns = ['acousticness', 'danceability', 'energy', 'instrumentalness',\n                     'loudness', 'liveness', 'speechiness', 'tempo', 'valence']\n    data2\n\n    st.write(\"## Dimensionality Reduction with PCA\")  # streamlit widget\n    pca = PCA(n_components=2)\n    pca.fit(data2)\n    pca_data = pca.transform(data2)\n    pca_df = pd.DataFrame(data=pca_data, columns=['x', 'y'])\n    fig = px.scatter(pca_df, x='x', y='y', title='PCA')\n    st.plotly_chart(fig)  # output plotly chart using streamlit\n\n    st.write(\"## Clustering with K-Means\")  # streamlit widget\n    data2 = list(zip(pca_df['x'], pca_df['y']))\n    kmeans = KMeans(n_init=10, max_iter=1000).fit(data2)\n    fig = px.scatter(pca_df, x='x', y='y', color=kmeans.labels_,\n                     color_continuous_scale='rainbow', hover_data=[data.artist, data.name])\n    st.plotly_chart(fig)  # output plotly chart using streamlit\n\n    st.write(\"Enjoy!\")\n\n\n# streamlit widgets\nst.write(\"# Spotify Playlist Clustering\")\nclient_id = st.text_input(\"Enter Client ID\")\nclient_secret = st.text_input(\"Enter Client Secret\")\nplaylistId = st.text_input(\"Enter Playlist ID\")\n\n# streamlit widgets\nif st.button('Create Dataset!'):\n    token = getToken()\n    getPlaylistItems(token, playlistId)\n\n\nBuat file requirements.txt\nFungsi dari file ini adalah untuk memberitahu streamlit package apa yang harus di install sebelum di deploy.\nIsi file requirements.txt dengan\nscikit-learn\nplotly-express\nSetelah itu, save file.\n\n\nUpload ke github repository\nJika semua coding sudah selesai, maka kita harus meng-upload semua coding kita ke github.\n\nlogin ke github\n\ndi dashboard github, klik New untuk membuat repository baru\n\nberi nama repositori, deskripsi, dan pastikan bahwa repository bersifat public. Lalu klik create repository\n\nkembali ke code editor, masukkan file streamlit_app.py ke dalam folder\n\nketik git init\n\nketik git add .\n\nketik git remote add origin https://github.com/[username]/[repository_name].git\n\n\n\nBuat Akun Streamlit\n\nPergi ke streamlit.io\n\nBuat akun/masuk menggunakan akun\n\nSangat direkomendasikan masuk menggunakan akun github\n\n\n\nCreate new app, isi form\n\n\n\nGambar19. Form create new app di streamlit\n\n\n\nRepository, Nama repositori dari github yang akan di upload ke streamlit. format [user github]/[nama repository], contoh : rif42/spotifyclustering\n\nBranch, Branch atau cabang dari repositori yang akan digunakan. Secara default akan diisi master\n\nMain File Path, Letak file yang akan dijadikan host dari aplikasi streamlit. Pastikan file sudah masuk didalam directory dan repository. Pastikan juga nama dari file sudah benar, sesuai dengan yang ada di repository.\n\nApp URL, domain atau URL yang akan digunakan untuk mengakses aplikasi yang sudah di deploy.\n\nKlik create app dan tunggu sampai streamlit selesai melakukan proses deploy"
  },
  {
    "objectID": "project_clustering.html#kesimpulan",
    "href": "project_clustering.html#kesimpulan",
    "title": "Studi Kasus 3",
    "section": "Kesimpulan",
    "text": "Kesimpulan\nUntuk project machine learning clustering ini, kita menggunakan dataset lagu yang diambil dari spotify.\nUntuk melakukan proses pengambilan lagu, kita membutuhkan akses API dari spotify. Pertama kita harus mendaftar sebagai spotify developer di web resmi spotify. Lalu kita mengambil sebuah token menggunakan kredensial yang kita dapat dari akun developer kita. Sebuah token memungkinkan kita untuk mengambil data dari spotify web API secara langsung. user menginput url playlist dan aplikasi akan mengambil semua lagu didalam playlist tersebut dan membuat dataset secara otomatis.\nDataset yang dihasilkan sudah cukup solid karena spotify web API memang dikenal mempunyai kualitas tinggi. Namun kita tetap melakukan pengecekan konsistensi data. Ketika data diambil dari web, beberapa lagu/artist mempunyai karakter yang tidak dapat ditampilkan seperti huruf kanji, maka kita harus encode-decode karakter tersebut ke standar utf-8. Namun terkadang proses dari encode-decode menghasilkan sebuah string kosong. Walaupun fitur yang lain tidak kosong, data ini tidak bisa kita identifikasi, jadi kita akan hapus data ini.\nSetelah di preproses, kita akan melakukan proses dimensionality reduction menggunakan principal component analysis (PCA). Proses clustering menggunakan scatter plot mengharuskan data mempunyai 2 fitur. Tujuan dari proses PCA adalah mengurangi jumlah fitur dari 9 menjadi 2 tanpa mengurangi makna yang ada didalam dataset.\nKita melakukan clustering dataset menggunakan algoritma K-Means dengan titik cluster yang diatur secara otomatis. Adapun parameter yang kita masukkan ke algoritma yaitu n_init=10 untuk menambah konsistensi hasil dan max_iter=1000 untuk menambah akurasi clustering.\nHasilnya dataset terbagi menjadi 7 buah cluster (secara otomatis), dan masing masing lagu di dalam cluster mempunyai keunikan tersendiri. Keunikan tersebut dapat direpresentasikan dengan tempo lagu, jenis melodi, jenis instrumen, overall ‘vibe’ dari lagu, dan lain lain. Jumlah cluster dapat diatur menggunakan parameter n_cluster.\nUntuk melakukan proses deploy, pertama kita buat github repository dulu. Isi github repository dengan file python (file py, bukan ipynb) yang akan kita gunakan untuk proses deploy. File python ini berisi fungsi-2fung"
  },
  {
    "objectID": "modul_2.html#daftar-sumber-data-daring",
    "href": "modul_2.html#daftar-sumber-data-daring",
    "title": "2  Telaah Data",
    "section": "4.3 Daftar Sumber Data Daring",
    "text": "4.3 Daftar Sumber Data Daring\n\nPortal Satu Data Indonesia\nPortal Data Jakarta\nPortal Data Bandung\n\nBadan Pusat Statistik\n\nBadan Informasi Geospasial\n\nUCI Machine Learning repository\nKaggle\n\nWorld Bank Open Data"
  },
  {
    "objectID": "modul_2.html#apa-itu-telaah-data",
    "href": "modul_2.html#apa-itu-telaah-data",
    "title": "2  Telaah Data",
    "section": "2.1 Apa itu telaah data?",
    "text": "2.1 Apa itu telaah data?\n\nTelaah data merujuk pada proses pengumpulan, pembersihan, eksplorasi, dan analisis data untuk mendapatkan pemahaman yang lebih yang terkandung dalam data tersebut. Tujuan dari telaah data adalah mendukung pengambilan keputusan berdasarkan bukti yang ditemukan dalam data."
  },
  {
    "objectID": "modul_2.html#mengapa-perlu-telaah-data",
    "href": "modul_2.html#mengapa-perlu-telaah-data",
    "title": "2  Telaah Data",
    "section": "2.2 Mengapa perlu telaah data?",
    "text": "2.2 Mengapa perlu telaah data?\n\n\nData dari masing-masing sumber belum tentu dapat langsung dipakai karena:\n\n\nmaksud dan tujuan data berbeda-beda\nkeadaan asal terpisah-pisah atau justru terintegrasi secara ketat.\ntingkat kekayaan (richness) berbeda-beda\ntingkat keandalan (reliability) berbeda-beda\n\n\nData understanding memberikan gambaran awal tentang:\n\n\nkekuatan data\nkekurangan dan batasan penggunaan data\ntingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan\nketersediaan data (terbuka/tertutup, biaya akses, dsb.)"
  },
  {
    "objectID": "modul_5.html#sampling-data",
    "href": "modul_5.html#sampling-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.6 Sampling Data",
    "text": "5.6 Sampling Data\nSampling adalah teknik utama yang digunakan untuk reduksi data. Sampling data adalah proses pengambilan sebagian kecil dari suatu populasi untuk mewakili seluruh populasi dengan tujuan mengambil kesimpulan atau membuat estimasi. Sampling sering digunakan untuk investigasi awal data dan analisis data akhir.\n\n5.6.1 Tujuan Sampling Data\n\nMendapatkan data yang relevan dengan penelitian.\nDapat memberikan estimasi yang akurat jika dilakukan dengan benar.\nDijadikan acuan untuk mengambil kesimpulan, saran, dan keputusan setelah penelitian dilakukan.\n\n\n\n5.6.2 Keuntungan Sampling Data\n\nDapat menghemat waktu, biaya, dan sumber daya dengan tidak mengumpulkan data dari seluruh populasi.\nHasil pemeriksaan sampel sangat obyektif dan dapat dipertahankan (objective and defensible).\nMemungkinkan untuk memperkirakan besarnya kesalahan sampling (sampling error).\nSesuai digunakan saat mengambil kesimpulan tentang data dalam jumlah yang banyak.\nDapat menentukan banyaknya elemen sampel (sample size) sebelum pemeriksaan dilakukan.\n\n\n\n5.6.3 Metode Sampling\n\nRandom Sampling\n\n\nSetiap anggota populasi memiliki kesempatan yang sama untuk dipilih.\nKeuntungan : menghasilkan sampel yang representatif jika dilakukan dengan benar, meminimalisir bias, dan bisa mengetahui standar error dari penelitian.\nKekurangan : sampel yang diambil kemungkinan tidak mewakili populasi yang ditentukan.\nHands On Coding\n\n    import pandas as pd\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n\n    # Load dataset Iris\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Buat DataFrame\n    df = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n    df['target'] = y\n\n    # Tampilan DataFrame asli\n    print(\"DataFrame Asli:\")\n    print(df.head(10))\n\n    # Random Sampling\n    sampled_df = df.sample(n=50, random_state=1)  # Ambil 50 sampel secara acak\n    print(\"\\nDataFrame Setelah Random Sampling:\")\n    print(sampled_df.head(10))\n\n\nStratified Sampling\n\n\nTeknik pengambilan sampel jenis ini dilakukan dengan membagi populasi ke dalam tingkatan atau strata tertentu yakni tinggi, sedang, dan rendah. Kemudian diambil sampel tiap tingkatan tersebut.\nMemastikan setiap kelompok diwakili dalam sampel.\nHands On Coding\n\n    import pandas as pd\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n\n    # Load dataset Iris\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Buat DataFrame\n    df = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n    df['target'] = y\n\n    # Tampilan DataFrame asli\n    print(\"DataFrame Asli:\")\n    print(df.head(10))\n\n    # Stratified Sampling berdasarkan variabel 'target' \n    stratified_sample = df.groupby('target', group_keys=False).apply(lambda x: x.sample(2, random_state=1))\n    print(\"\\nDataFrame Setelah Stratified Sampling:\")\n    print(stratified_sample)"
  },
  {
    "objectID": "modul_5.html#keuntungan-sampling-data",
    "href": "modul_5.html#keuntungan-sampling-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.8 Keuntungan Sampling Data",
    "text": "5.8 Keuntungan Sampling Data\n\nDapat menghemat waktu, biaya, dan sumber daya dengan tidak mengumpulkan data dari seluruh populasi.\nHasil pemeriksaan sampel sangat obyektif dan dapat dipertahankan (objective and defensible).\nMemungkinkan untuk memperkirakan besarnya kesalahan sampling (sampling error).\nSesuai digunakan saat mengambil kesimpulan tentang data dalam jumlah yang banyak.\nDapat menentukan banyaknya elemen sampel (sample size) sebelum pemeriksaan dilakukan."
  },
  {
    "objectID": "modul_5.html#seleksi-fitur",
    "href": "modul_5.html#seleksi-fitur",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.7 Seleksi Fitur",
    "text": "5.7 Seleksi Fitur\nSeleksi fitur adalah proses memilih subset fitur dari sekumpulan fitur yang ada dalam data, dengan tujuan meningkatkan kinerja model dan mengurangi kompleksitas.\n\n5.7.1 Tujuan Seleksi Fitur\n\nMeningkatkan Kinerja Model :\n\nMemilih fitur yang paling informatif untuk meningkatkan akurasi dan generalisasi model.\n\nMengurangi Overfitting :\n\nMenghindari penggunaan fitur yang redundant atau tidak relevan untuk mengurangi risiko overfitting.\n\nEfisiensi Komputasi :\n\nMengurangi dimensi data untuk meningkatkan efisiensi komputasi, terutama pada dataset yang besar.\n\n\n\n\n5.7.2 Metode Seleksi Fitur\n\nFilter Methods Metode ini menggunakan metrik statistik untuk menilai korelasi antara fitur dan target. Contoh: Pengujian ANOVA, Korelasi Pearson.\nWrapper Methods Dengan mengevaluasi kinerja model dengan subset fitur tertentu. Contoh: Recursive Feature Elimination (RFE), Forward Selection.\nEmbedded Methods Menyeleksi fitur terintegrasi dengan proses pembuatan model. Contoh: Regresi Lasso, Pemilihan Fitur berbasis Pohon (Tree-based Feature Selection).\n\n\n\n5.7.3 Teknik Seleksi Fitur\n\nAnalisis Korelasi, memahami hubungan antara fitur dan target untuk mengidentifikasi fitur yang paling relevan.\nKeterkaitan dengan Bisnis/Tujuan, menyesuaikan seleksi fitur dengan tujuan bisnis atau analisis spesifik.\nValidasi Silang (Cross-Validation), mengevaluasi kinerja seleksi fitur melalui teknik validasi silang untuk memastikan generalisasi yang baik.\n\n\nHands On Coding\n\n    import pandas as pd\n    from sklearn.datasets import load_iris\n    from sklearn.feature_selection import SelectKBest\n    from sklearn.feature_selection import f_classif\n\n    # Load dataset contoh\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Buat DataFrame\n    df = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n    df['target'] = y\n\n    # Tampilan DataFrame asli\n    print(\"DataFrame Asli:\")\n    print(df.head(10))\n\n    # Seleksi Fitur dengan ANOVA (f_classif)\n    k_best = SelectKBest(score_func=f_classif, k=2)  # Pilih 2 fitur terbaik\n    X_selected = k_best.fit_transform(X, y)\n\n    # Tampilan DataFrame setelah seleksi fitur\n    selected_df = pd.DataFrame(data=X_selected, columns=['selected_feature_1', 'selected_feature_2'])\n    selected_df['target'] = y\n    print(\"\\nDataFrame Setelah Seleksi Fitur:\")\n    print(selected_df.head(10))"
  },
  {
    "objectID": "modul_5.html#train-test-split",
    "href": "modul_5.html#train-test-split",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.8 Train Test Split",
    "text": "5.8 Train Test Split\n\nMembagi data menjadi subset pelatihan dan pengujian.\nUmumnya digunakan untuk mengevaluasi kinerja model.\nHands On Coding\n\n        # Train-Test Split (80% train, 20% test)\n        train_df, test_df = train_test_split(df, test_size=0.2, random_state=1)\n        print(\"\\nDataFrame Train Setelah Train-Test Split:\")\n        print(train_df.head(10))\n        print(\"\\nDataFrame Test Setelah Train-Test Split:\")\n        print(test_df.head(10))"
  },
  {
    "objectID": "modul_6.html#outlier",
    "href": "modul_6.html#outlier",
    "title": "6  Cleaning Data",
    "section": "6.3 Outlier",
    "text": "6.3 Outlier\nOutlier merupakan data yang jauh dari nilai rata-rata atau nilai normal. Outlier dapat mempengaruhi hasil analisis dan machine learning.\nOutlier dapat terjadi karena berbagai alasan, seperti kesalahan pengukuran, kesalahan entri data, variasi alami, atau peristiwa langka. Outlier dapat memiliki dampak signifikan pada analisis statistik, model pembelajaran mesin, dan interpretasi data, yang dapat mengarah pada hasil yang bias atau kesimpulan yang tidak akurat.\n\nStrategi untuk mengatasi Outlier :\n\nMenghapus outlier.\nMenggantikan nilai outlier dengan nilai lain seperti nilai rata-rata atau median.\nMenggunakan teknik scaling atau normalisasi.\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis.\n\n6.3.1 Deteksi Outlier\nOutlier dapat dideteksi menggunakan beberapa metode, antara lain\n\n6.3.1.1 Visualisasi\nAdalah sebuah teknik untuk memvisualisasikan sebuah data menjadi suatu bentuk yang dapat dilihat secara menyeluruh sehingga kita dapat menganalisa bentuk data, ukuran data, data point dan mendeteksi outlier.\nBeberapa bentuk visualisasi yang sering digunakan untuk mendeteksi outlier yaitu\n\nHistogram\n\nScatter plot\n\nBox plot\n\n\n\n\n6.3.2 Kategori Outlier\nVariate dan univariate adalah dua jenis data dalam statistik. Outlier adalah observasi atau nilai yang secara signifikan berbeda dari pola atau pola umum data yang lain.\n\n6.3.2.1 Variate Data\nVariate data merujuk pada set data yang terdiri dari beberapa variabel atau fitur. Contoh umum variate data adalah dataset yang terdiri dari beberapa kolom, di mana setiap kolom mewakili variabel yang berbeda. Misalnya, jika kita memiliki dataset tentang mahasiswa yang mencakup variabel seperti tinggi, berat badan, dan usia, maka kita memiliki variate data. Outlier dalam variate data merujuk pada observasi atau nilai yang di luar kisaran yang diharapkan dalam setidaknya satu variabel.\n\n\n6.3.2.2 Univariate Data\nUnivariate data merujuk pada set data yang hanya memiliki satu variabel atau fitur. Contoh umum univariate data adalah dataset yang hanya terdiri dari satu kolom, seperti data tinggi badan seseorang. Outlier dalam univariate data merujuk pada observasi atau nilai yang sangat ekstrem atau jauh dari rentang nilai yang diharapkan.\n\n\n\n6.3.3 Mengatasi Outlier\n\n6.3.3.1 Menghapus Outlier\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom scipy import stats\n\n# Load dataset Iris dari scikit-learn\ndata = load_iris()\niris_df = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Tampilkan informasi tentang dataset Iris sebelum penghapusan outlier\nprint(\"Informasi tentang dataset Iris sebelum penghapusan outlier:\")\nprint(iris_df.describe())\n\n# Definisikan fungsi untuk menghapus outlier berdasarkan z-score\ndef remove_outliers_zscore(df, z_threshold=3):\n    z_scores = stats.zscore(df)\n    return df[(z_scores &lt; z_threshold).all(axis=1)]\n\n# Hapus outlier dari dataset Iris berdasarkan z-score\niris_cleaned = remove_outliers_zscore(iris_df)\n\n# Tampilkan informasi tentang dataset Iris setelah penghapusan outlier\nprint(\"\\nInformasi tentang dataset Iris setelah penghapusan outlier:\")\nprint(iris_cleaned.describe())\nPada contoh di atas, digunakan metode z-score untuk mendeteksi outlier dalam dataset Iris. Outlier adalah data yang memiliki z-score lebih besar dari z_threshold yang telah ditentukan (z_threshold=3). Fungsi remove_outliers_zscore digunakan untuk menghapus baris yang mengandung outlier berdasarkan z-score, yaitu baris yang memiliki setidaknya satu fitur (kolom) dengan z-score melebihi z_threshold. Penting untuk berhati-hati karena penghapusan outlier dapat mempengaruhi hasil analisis atau model machine learning. Selain z-score, ada banyak teknik deteksi outlier lainnya seperti IQR dan pendekatan berbasis model machine learning atau domain knowledge.\n\n\n6.3.3.2 Menggantikan nilai outlier dengan nilai lain seperti nilai rata-rata atau median.\nimport pandas as pd\n\n# membaca dataset publik\ndata = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n\n# menentukan batas outlier\nq1 = data[0].quantile(0.25)\nq3 = data[0].quantile(0.75)\niqr = q3 - q1\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n\n# menggantikan outlier dengan nilai rata-rata\ndata[0] = data[0].apply(lambda x: data[0].mean() if x &lt; lower_bound or x &gt; upper_bound else x)\n\n# menggantikan outlier dengan nilai median\ndata[0] = data[0].apply(lambda x: data[0].median() if x &lt; lower_bound or x &gt; upper_bound else x)\n\n# menampilkan dataset yang telah diubah\nprint(data)\nPada contoh kode di atas, dataset publik yang digunakan adalah Iris Dataset yang tersedia di UCI Machine Learning Repository. Pertama-tama, kita menentukan batas outlier dengan menghitung kuartil pertama (Q1), kuartil ketiga (Q3), dan rentang antar kuartil (IQR). Kemudian, kita menentukan batas bawah dan batas atas untuk menentukan nilai outlier.\nKemudian, kita menggantikan nilai outlier dengan nilai rata-rata atau median. Untuk menggantikan dengan nilai rata-rata, kita menggunakan fungsi apply() pada kolom yang menghasilkan nilai rata-rata jika nilai kurang dari batas bawah atau lebih dari batas atas, sedangkan untuk menggantikan dengan nilai median, kita juga menggunakan fungsi apply() pada kolom yang menghasilkan nilai median jika nilai kurang dari batas bawah atau lebih dari batas atas.\nTerakhir, kita menampilkan dataset yang telah diubah dengan fungsi print().\n\n\n6.3.3.3 Mengisi nilai kosong dengan nilai yang sering muncul\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom sebelum pengisian\nprint(\"Informasi tentang dataset Titanic sebelum pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n# Mengisi nilai kosong pada kolom 'Embarked' dengan nilai yang sering muncul (mode) dari kolom tersebut\nembarked_mode = titanic_df['Embarked'].mode()[0]\ntitanic_df['Embarked'].fillna(embarked_mode, inplace=True)\n\n# Tampilkan informasi tentang dataset setelah pengisian nilai kosong\nprint(\"\\nInformasi tentang dataset Titanic setelah pengisian nilai kosong:\")\nprint(titanic_df.info())\nPada contoh di atas, menggunakan metode mode() dari pandas untuk menghitung nilai yang sering muncul (mode) pada kolom ‘Embarked’, lalu mengisi nilai kosong pada kolom tersebut dengan nilai mode yang dihitung. Penggunaan parameter inplace=True memastikan perubahan dilakukan langsung pada DataFrame asli. Setelah mengisi nilai kosong, hasilnya dapat diperiksa untuk memastikan tidak ada lagi nilai yang hilang pada kolom ‘Embarked’.\nPerlu diingat, pengisian nilai kosong dengan nilai yang sering muncul merupakan salah satu teknik imputasi yang umum. Teknik lainnya termasuk pengisian dengan nilai rata-rata, nilai median, atau menggunakan model machine learning untuk memprediksi nilai yang hilang berdasarkan data lainnya. Pilihan teknik imputasi tergantung pada karakteristik dataset dan tujuan analisis atau model machine learning yang ingin diimplementasikan.\n\n\n6.3.3.4 Discretization/Binning\nDiscretization atau binning adalah proses mengubah data kontinu menjadi data diskrit dengan cara membagi rentang nilai kontinu menjadi beberapa interval atau kelompok yang disebut “bin” atau “bucket”. Tujuan utama dari discretization adalah mengurangi kompleksitas data kontinu dengan mengelompokkan nilainya ke dalam kategori atau range tertentu.\n\n\n\nGambar10. Grafik Proses Binning Dataset\n\n\n\nKelebihan\n\nDapat diterapkan pada data kategorik dan numerik.\n\nModel lebih robust dan mencegah overfitting.\n\n\n\nKekurangan\n\nMeningkatnya biaya kinerja perhitungan.\n\nMengorbankan informasi.\n\nUntuk kolom data numerik, dapat menyebabkan redudansi untuk beberapa algoritma.\n\nUntuk kolom data kategorik, label dengan frekuensi rendah berdampak negatif pada robustness model statistik.\n\n\n\n\n6.3.3.5 Trimming\nTrimming adalah proses penghapusan data yang dianggap sebagai outlier. Trimming biasanya dilakukan berdasarkan presentase data yang akan di trim, contohnya 5%.\n\n\nKelebihan\n\nCepat dan mudah\n\nDapat memperbaiki rata-rata data\n\n\n\nKekurangan\n\nHilangnya data yang dapat mengandung informasi\n\nDapat menyebabkan bias terhadap variansi data\n\n\n\n\n6.3.3.6 Winsorizing\nWinsorizing adalah proses penggantian data outlier dengan nilai-nilai yang berada dalam distribusi yang ditentukan.\n\nKelebihan\n\nMempertahankan informasi\n\nMengurangi efek dari outlier\n\nMudah diimplementasikan\n\n\n\nKekurangan\n\nDapat menghasilkan bias\n\nPemilihan presentil dapat mempengaruhi hasil analisis data\n\n\n\n\n6.3.3.7 Imputing\nImputing adalah proses penggantian data outlier dengan nilai-nilai yang diprediksi atau diestimasi berdasarkan karakteristik data. Teknik imputasi sudah di bahas secara detail di bab sebelumnya\n\nKelebihan\n\nMempertahankan informasi dan ukuran sampel\n\nMeningkatkan akurasi analisis\n\n\n\nKekurangan\n\nPemilihan metode dan implementasi bisa cukup sulit\n\nBerpotensi merusak distribusi data\n\n\n\n\n6.3.3.8 Normalization\nAdalah metode untuk mengubah skala nilai dalam dataset sehingga nilainya berkisar antara 0 dan 1. Untuk melakukan normalisasi data, kita membagi data berdasarkan nilai minimum dan maksimum dari data. Proses normalisasi baik digunakan untuk dataset yang mempunyai distribusi data non-normal atau tidak beraturan.\nRumus dari normalisasi adalah:\n\\[\nnormalized_x = \\dfrac{x - min(x)}{max(x)-min(x)}\n\\]\n\nKelebihan\n\nMempertahankan informasi dan ukuran sampel\n\nMeningkatkan akurasi analisis\n\nMudah diimplementasikan\n\n\n\nKekurangan\n\nMetode harus sesuai dengan karakteristik data\n\nTidak menghilangkan outlier secara langsung, hanya mengurangi efek dari outlier\n\n\n\n\n6.3.3.9 Standarization / Z-Score\nStandarisasi data adalah suatu proses dalam analisis data yang mengubah variabel-variabel menjadi memiliki rata-rata nol dan standar deviasi satu. Dalam standarisasi data, setiap nilai data dikurangi dengan rata-rata dari seluruh data, kemudian hasilnya dibagi dengan standar deviasi data. Dengan melakukan hal ini, nilai-nilai data akan berada pada skala yang relatif terhadap variabilitas data. Proses standarisasi menggunakan rumus sebagai berikut:\n\\[\nstandardized_x = \\dfrac{x - mean(x)}{standard deviation(x)}\n\\]\n\nKelebihan\n\nMean dan standar deviasi tidak berubah\n\nTidak sensitif terhadap outlier\n\n\n\nKekurangan\n\nTidak dapat menentukan batasan data\n\nHasil dapat berupa angka negatif\n\n\n\n\n6.3.3.10 Hands On Coding\n\n6.3.3.10.1 Binning\nLink Dataset : Here\n\n    # Bining\n    import pandas as pd\n    import plotly.express as px\n    import numpy as np\n    # Load Data\n    df = pd.read_csv('data_sampled_100.csv')\n\n    # binning data menjadi 5 kategori\n    df['loudness'] = pd.cut(df['loudness'], bins=3, labels=[\n                            'sunyi', 'standar', 'bising'])\n\n    # urutkan data berdasarkan fitur loudness\n    df_sorted = df.sort_values(by='loudness')\n\n    # visualisasikan data menggunakan histogram\n    fig = px.histogram(df_sorted, x='loudness')\n    fig.show()\n\n\n                                                \n\n\n\n\n6.3.3.10.2 Trimming\n\n    # Trimming\n\n    # Load Data\n    df = pd.read_csv('./data_sampled_100.csv')\n    df_sorted = df.sort_values(by='loudness')\n    df.head()\n\n    # ambil 1% dari data tersebut, hanya dari tail atau ujung belakang dari dataset. Nilai ini nantinya akan menjadi batas dari trimming.\n    q = df['loudness'].sort_values().quantile(0.01)\n    print(q)\n\n    # batas dari trimming kita adalah -27.3332. Selanjutnya kita akan hilangkan data yang melebihi nilai ini.\n    df_clean = df[df['loudness'] &gt; q]\n    dx = df[df['loudness'] &lt; q]\n    dx.head()\n\n    # tampilkan dalam scatter plot\n    fig = px.scatter(df_clean, x='loudness', y='popularity', color='loudness',\n                    hover_data=['artists', 'name', 'loudness', 'popularity'])\n    fig.show()\n\n-27.3332\n\n\n\n                                                \n\n\nLagu Thursday Afternoon - 2005 digital remaster, oleh Brian Eno yang mempunyai loudness kurang dari batas trimming sudah hilang.\n\n\n6.3.3.10.3 Winsorizing\nMenggunakan nilai yang sama dengan metode sebelumnya(trimming) yaitu 1% dan batas trimming -27.3332.\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf_sorted = df.sort_values(by='loudness')\ndf.head()\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n0\n0.940\n2006\n0.330\n['Los Horóscopos De Durango']\n0.687\n204587\n0.724\n0\n2KIUMqD9ZkcxhgSVyIDP4t\n0.000273\n4\n0.248\n-2.478\n0\nMi Amor Por Ti\n48\n2006-01-01\n0.0301\n141.507\n\n\n1\n0.165\n2002\n0.109\n['Good Charlotte']\n0.515\n242933\n0.566\n0\n0eYcZLnlLKaItC1WC4B1pc\n0.000000\n6\n0.160\n-6.176\n1\nEmotionless\n42\n2002-10-04\n0.0303\n154.320\n\n\n2\n0.551\n1987\n0.795\n['Raphael']\n0.402\n159560\n0.427\n0\n6b3ub116kE1T15h1yzaiTy\n0.000000\n2\n0.222\n-11.328\n0\nYo soy aquél\n38\n1987-08-25\n0.0359\n111.065\n\n\n3\n0.557\n1998\n0.790\n['Joan Sebastian', 'Pepe Aguilar']\n0.392\n180976\n0.426\n0\n2bWImXJQNVfgrROo4Xj630\n0.000004\n9\n0.208\n-7.088\n1\nEstás Fallando\n48\n1998-09-10\n0.0404\n183.039\n\n\n4\n0.480\n1965\n0.988\n['Vince Guaraldi Trio']\n0.339\n149227\n0.243\n0\n5dwY5r2PfjMteikJViyNIT\n0.893000\n0\n0.113\n-16.851\n0\nGreat Pumpkin Waltz\n55\n1965\n0.0382\n151.481\n\n\n\n\n\n\n\nPerbedaan utama antara trimming dan winsorizing adalah, di proses trimming, nilai dibawah batas dihilangkan, namun di proses winsorizing, nilai dibawah batas digantikan dengan nilai diantara quartil 1 dan quartil 3.\nUntuk itu, kita akan menggunakan box plot karena box plot sudah memberikan nilai high fence dan low fence secara langsung.\n\nfig = px.box(df, x='loudness')\nfig.show()\n\n\n                                                \n\n\nSelanjutnya, kita akan menghitung berapa banyak data yang ada dibawah batas trimming\n\nq = df['loudness'].sort_values().quantile(0.01)\noutlier = df[df['loudness'] &lt; q]\noutlier\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n69\n0.0528\n1985\n0.976\n['Brian Eno']\n0.0918\n3650800\n0.0569\n0\n4t3Yh6tKkxXrc458pNI7zZ\n0.884\n0\n0.0842\n-31.808\n1\nThursday Afternoon - 2005 Digital Remaster\n39\n1985-10-01\n0.0358\n81.944\n\n\n\n\n\n\n\nDiketahui ada 1 buah outlier. Selanjutnya kita akan menentukan nilai q1 dan q3 sebagai batasan nilai winsorizing kita.\n\nq1 = np.percentile(df['loudness'], 25)\nq3 = np.percentile(df['loudness'], 75)\nprint(f'q1 = ', q1)\nprint(f'q3 = ', q3)\n\nq1 =  -15.43075\nq3 =  -7.177250000000001\n\n\nNilai q1 dan q3 ini akan kita gunakan untuk memberi batasan terhadap angka random yang akan kita generate sebagai nilai winsorizing kita.\n\nimport random\nimport time\n\n# nilai n menyesuaikan jumlah data outlier\noutliercount = len(outlier)\n\n# generate random number\nrandom.seed(time.time_ns())\nwinsorized_outlier = [random.randint(int(q1), int(q3))\n                      for i in range(outliercount)]\nprint(winsorized_outlier)\n\n[-14]\n\n\nIngat, nilai ini di generate secara random, jadi nilai akan berubah setiap code di run.\nSelanjutnya, kita akan memasukkan nilai winsorizing ke outlier, lalu masukkan outlier ke dataset kita.\n\nfor i in range(len(winsorized_outlier)):\n    outlier.iloc[i, 12] = winsorized_outlier[i]\n\n# masukkan outlier ke dataset induk\nfor i in range(len(outlier)):\n    id = outlier.iloc[i, 8]  # ambil ID dari data outlier\n    # cari index dari data outlier di dataset\n    index = df[df['id'] == id].index[0]\n    df.iloc[index, 12] = outlier.iloc[i, 12]  # gantikan dataset dengan outlier\n\ndf = df.sort_values(by='loudness')\ndisplay(df)\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n22\n0.0447\n1945\n0.90100\n['Anca Elena']\n0.573\n214571\n0.00574\n0\n4SriIcIMNhfwyA8oMH79KP\n0.317000\n9\n0.1660\n-27.288\n1\nRegard du temps\n0\n1945\n0.0678\n131.582\n\n\n19\n0.0454\n2004\n0.96100\n['Ludovico Einaudi']\n0.191\n357707\n0.05820\n0\n3weNRklVDqb4Rr5MhKBR3D\n0.890000\n8\n0.0941\n-25.398\n1\nNuvole Bianche\n72\n2004-01-01\n0.0578\n132.614\n\n\n76\n0.0590\n1965\n0.99200\n['Frédéric Chopin', 'Arthur Rubinstein']\n0.260\n325893\n0.01910\n0\n6mry9fDj4oTFudQAMRo1lV\n0.892000\n3\n0.0864\n-23.594\n0\nNocturnes, Op. 9: No. 1 in B-Flat Minor\n31\n1965\n0.0410\n62.106\n\n\n97\n0.0562\n1952\n0.22700\n['Johannes Brahms', 'Pierre Monteux']\n0.183\n122600\n0.12800\n0\n1kzteGupSBIJVs0Aff51SC\n0.785000\n0\n0.1570\n-22.986\n1\nSong of Destiny, Op. 54: III. Adagio\n0\n1952\n0.0368\n111.440\n\n\n92\n0.2360\n1926\n0.47500\n['Georgette Heyer', 'Irina Salkow']\n0.679\n134571\n0.12100\n0\n5bsAPgurneSpyfqfNCSElq\n0.000000\n4\n0.1160\n-21.337\n1\nKapitel 272 - Der Page und die Herzogin\n1\n1926\n0.9440\n145.910\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n89\n0.5670\n2016\n0.19700\n['DJ Drama', 'Chris Brown', 'Jhené Aiko', 'Tor...\n0.563\n330387\n0.69100\n1\n6Op6z8dK5XC9lGRZe4XRF2\n0.000000\n3\n0.0886\n-4.464\n0\nWishing Remix (feat. Chris Brown, Fabolous, Tr...\n57\n2016-09-29\n0.3820\n108.406\n\n\n33\n0.9380\n2007\n0.00117\n['Linkin Park']\n0.655\n189293\n0.88500\n1\n1fLlRApgzxWweF1JTf8yM5\n0.000473\n7\n0.0448\n-4.116\n1\nGiven Up\n68\n2007-05-14\n0.0438\n100.088\n\n\n60\n0.6930\n1964\n0.47200\n['Little Richard']\n0.337\n159200\n0.79700\n0\n6oSoqM0Otpmuowlzx0jYvK\n0.000015\n8\n0.1370\n-3.838\n1\nGoodnight Irene\n26\n1964-08\n0.0756\n173.437\n\n\n20\n0.0839\n2001\n0.00179\n['Slayer']\n0.238\n215693\n0.99400\n1\n0esBc6VgM4lFk3SOlL3Ys4\n0.009440\n8\n0.2020\n-3.578\n1\nDisciple\n49\n2001-01-01\n0.2500\n95.569\n\n\n0\n0.9400\n2006\n0.33000\n['Los Horóscopos De Durango']\n0.687\n204587\n0.72400\n0\n2KIUMqD9ZkcxhgSVyIDP4t\n0.000273\n4\n0.2480\n-2.478\n0\nMi Amor Por Ti\n48\n2006-01-01\n0.0301\n141.507\n\n\n\n\n100 rows × 19 columns\n\n\n\n\n\n6.3.3.10.4 Normalisasi\n\nfrom sklearn.preprocessing import MinMaxScaler\n# Normalisasi\n\n# Load Data\ndf = pd.read_csv('./data_sampled_100.csv')\ndf['loudness'].head()\n\n# batas minimum data adalah 31.808000 dan dan batas maksimum data adalah -2.478000\n# Batas data ini akan diubah menjadi 0 - 1 menggunakan metode normalization. Cek menggunakan histogram\nfig = px.histogram(df, x='loudness')\nfig.show()\n\n# buat objek scaler\nscaler = MinMaxScaler()\n\n# transformasi data tempo menggunakan objek scaler\ndf['loudness'] = scaler.fit_transform(df[['loudness']])\n\n# grafik histogram untuk fitur tempo\nfig = px.histogram(df, x='loudness')\nfig.show()\n\n# cek batas-batas data dari data yang sudah di normalisasi.\ndf['loudness'].describe()\n\n\n                                                \n\n\n\n                                                \n\n\ncount    100.000000\nmean       0.683209\nstd        0.193857\nmin        0.000000\n25%        0.558379\n50%        0.699898\n75%        0.839780\nmax        1.000000\nName: loudness, dtype: float64\n\n\n\n\n6.3.3.10.5 Standarization\n\n# standarization \n\n# Load data\ndf = pd.read_csv('./data_sampled_100.csv')\ndf['loudness'].describe()\n\n# visualisasi data dengan bar chart\nfig = px.histogram(df, x='loudness')\nfig.show()\n\n# gunakan function z-score scaler untuk melakukan proses normalisasi terhadap data loudness.\nmean = round(np.mean(df['loudness']), 2)\nstd = round(np.std(df['loudness']), 2)\nnormalized = round((df['loudness']-mean)/std, 2)\n\nprint(f'mean = ', mean)\nprint(f'std deviation = ', std)\nnormalized.describe()\n\n# buat diagram histogramnya untuk mengecek perubahan distribusi data.\nfig = px.histogram(normalized, x=\"loudness\")\nfig.show()\n\n\n                                                \n\n\nmean =  -11.77\nstd deviation =  5.66\n\n\n\n                                                \n\n\nJika kita bandingkan histogram data asli dengan data yang di standarisasi, distribusi data tidak berbeda jauh. Selain itu, batas data minimal mengecil dari -31 menjadi -3, dan batas data maksimal membesar dari -2 menjadi 1."
  },
  {
    "objectID": "modul_8.html#machine-learning-model",
    "href": "modul_8.html#machine-learning-model",
    "title": "8  Pemodelan Data Science",
    "section": "8.3 Machine Learning Model",
    "text": "8.3 Machine Learning Model\n\n8.3.1 Decision Tree\nHands On Coding\nimport numpy as np \nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nimport sklearn.tree as tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Buat DataFrame\ndf = pd.DataFrame(data=X, columns=[f'feature_{i}' for i in range(X.shape[1])])\ndf['target'] = y\n\n# Rename column dataframe\ndf = df.rename(columns={'feature_0':'Sepal Length','feature_1':'Sepal Width','feature_2':'Petal Length','feature_3':'Petal Width','target':'Species'})\n\n# gantikan nilai kolom Spesies dengan nama spesies\nreplace_value = {0:'Iris Setosa', 1:'Iris Versicolour',2:'Iris Virginica'}\n\n# replace the values of Species columns\ndf['Species'] = df['Species'].replace(replace_value)\n\n# Checking Data\nprint(df.info())\nprint(\"\")\nprint(\"NUll Value : \\n\",df.isnull().sum())\n\n# separate the feature and target\nX = df.iloc[:,:-1]\nY = df.iloc[:,-1]\n\n# spliting data into Data train and test\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3 , random_state = 42)\n\n# making model Decision Tree\nTree = DecisionTreeClassifier() \n# Train data using model\nTree.fit(x_train,y_train)\n\n# predict the data using model\nypred = Tree.predict(x_test)\n# calculate the accuracy of model\naccuracy = accuracy_score(y_test,ypred)\nprint(\"\")\nprint(\"Akurasi Model : \" ,accuracy)\n\n# display tree plot\ntree.plot_tree(Tree)\nplt.show()\n\n\n\n8.3.2 ANN\n\n8.3.2.1 Definisi\nArtificial Neural Network (ANN) adalah model matematika yang terinspirasi oleh struktur dan fungsi jaringan saraf biologis. Ini digunakan untuk memodelkan dan menangani hubungan kompleks antara input dan output.\nProses pembelajaran ANN umumnya melibatkan dua tahap utama: feedforward, di mana input diteruskan melalui lapisan-lapisan untuk menghasilkan output, dan backpropagation, di mana kesalahan antara output yang dihasilkan dan output yang diharapkan digunakan untuk menyesuaikan bobot.\nKomponen Utama :\n\nNeuron: Unit pemrosesan informasi.\nLayer: Kumpulan neuron yang berada pada tingkat yang sama.\nBobot (Weights): Parameter yang disesuaikan selama pelatihan untuk mengoptimalkan model.\nFungsi Aktivasi: Fungsi matematis yang memutuskan apakah suatu neuron harus aktif atau tidak.\n\n\n\n8.3.2.2 Dasar-Dasar Neural Network\n\n8.3.2.2.1 Struktur Dasar ANN\n\nInput Layer: Neuron di lapisan ini mewakili fitur dari data input.\nHidden Layer(s): Neuron di lapisan ini memproses input dan menghasilkan representasi yang semakin terabstraksi.\nOutput Layer: Neuron di lapisan ini menghasilkan output yang sesuai dengan tugas yang diinginkan (misalnya, klasifikasi).\n\n\n\n8.3.2.2.2 Fungsi Aktivasi\nFungsi aktivasi memperkenalkan non-linearitas ke dalam model, memungkinkan ANN untuk memodelkan hubungan yang kompleks. Fungsi aktivasi digunakan untuk menambahkan unsur non-linearitas ke dalam model.\nContoh fungsi aktivasi termasuk Sigmoid, Hyperbolic Tangent (tanh), dan Rectified Linear Unit (ReLU).\n\n\n8.3.2.2.3 Feedforward dan Backpropagation\nFeedforward adalah proses di mana input disampaikan melalui layer-layer untuk menghasilkan output. Sedangkan Backpropagation adalah proses pelatihan di mana model “belajar” dengan menyesuaikan bobot berdasarkan perbedaan antara output yang dihasilkan dan nilai yang diharapkan. Optimizer seperti Stochastic Gradient Descent (SGD) atau Adam digunakan untuk meminimalkan fungsi loss.\n\n\n\n8.3.2.3 Hands On Coding\nDalam kode program ini menggunakan dataset Iris dengan algoritma ANN. Model ANN dibangun dengan menggunakan TensorFlow Sequential API. Dalam kode program ini:\n\nInput layer memiliki 4 neuron (sesuai dengan jumlah fitur pada dataset Iris).\nHidden layer memiliki 8 neuron dengan fungsi aktivasi ReLU.\nOutput layer memiliki 3 neuron dengan fungsi aktivasi softmax, sesuai dengan jumlah kelas iris.\n\n\n# Import library yang dibutuhkan\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import load_iris\n\n# Memuat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target.reshape(-1, 1)\n\n# Menggunakan OneHotEncoder untuk mengonversi label menjadi one-hot encoding\nencoder = OneHotEncoder(sparse_output=False)\ny_onehot = encoder.fit_transform(y)\n\n# Membagi dataset menjadi data pelatihan dan data pengujian\nX_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n\n# Normalisasi data\nmean = X_train.mean(axis=0)\nstd = X_train.std(axis=0)\nX_train = (X_train - mean) / std\nX_test = (X_test - mean) / std\n\n# Membangun model ANN menggunakan TensorFlow\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(8, input_shape=(4,), activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\n\n# Menentukan fungsi loss, optimizer, dan metrik\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Melatih model\nmodel.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))\n\n# Evaluasi model pada data pengujian\naccuracy = model.evaluate(X_test, y_test)[1]\nprint(f'Akurasi pada data pengujian: {accuracy * 100:.2f}%')\n\n\n\n8.3.3 K-Means\nLink Dataset : Here\nHands On Coding\n\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import KMeans \nfrom sklearn.preprocessing import StandardScaler\n\n\ndf = pd.read_csv(\"data/Cust_Segmentation.csv\")\ndf.head()\n\n# drop address column\ndf = df.drop('Address', axis=1)\ndf.head()\n\n# check null values\nprint(df.isnull().sum())\n\n# input the features except cust id to X\nX = df.values[:,1:]\n# repacle nan value to 0\nX = np.nan_to_num(X)\n# use StandardScaler\nClus_dataSet = StandardScaler().fit_transform(X)\nprint(Clus_dataSet)\n\n# making Kmeans Model\nclusterNum = 3\nk_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\n# train the dataset with kmeans model\nk_means.fit(Clus_dataSet)\n# make kmeans labels\nlabels = k_means.labels_\nprint(labels)\n\n# insert labels into dataset columns\ndf[\"Clus_km\"] = labels\ndf.head(5)\n# gruping by clus_km column \nprint(df.groupby('Clus_km').mean())\n\n# make scatter plot\nplt.scatter(df.iloc[:, 1], df.iloc[:, 4], c=labels.astype(float), alpha=0.5)\nplt.xlabel('Age', fontsize=18)\nplt.ylabel('Income', fontsize=16)\n\nplt.show()\n\n\n8.3.4 Regression\nModel regresi adalah salah satu metode statistik yang digunakan untuk menganalisis hubungan antara satu atau lebih variabel independen (juga disebut variabel prediktor) dengan variabel dependen (juga disebut variabel respons) dengan tujuan untuk memprediksi atau menjelaskan nilai variabel dependen berdasarkan variabel-variabel independen yang ada. Model regresi dapat digunakan untuk memahami bagaimana perubahan dalam satu atau lebih variabel independen dapat mempengaruhi perubahan dalam variabel dependen.\nTerdapat berbagai jenis model regresi, namun dua jenis yang paling umum adalah:\n\nRegresi Linear: Model regresi linear adalah salah satu yang paling sederhana. Dalam regresi linear, asumsi dasarnya adalah bahwa hubungan antara variabel independen dan dependen adalah linier. Dengan kata lain, kita mencoba untuk menemukan garis lurus yang terbaik (garis regresi) yang dapat menjelaskan hubungan antara variabel independen dan dependen.\nRegresi Logistik: Regresi logistik digunakan ketika variabel dependen adalah variabel biner (memiliki dua nilai, seperti ya/tidak, sukses/gagal, dll.). Regresi logistik membantu memahami probabilitas kejadian atau tidaknya suatu peristiwa berdasarkan variabel-variabel independen.\n\nLink Dataset Linear Regression: Here\nLink Dataset Logistic Regression: Here\nHands On Coding Linear Regression\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import accuracy_score as asc\nfrom sklearn.metrics import jaccard_score as js\nfrom sklearn.metrics import f1_score as f1s\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\n\n# Linear Regression\n\n# load the dataset\ndf_raw = pd.read_csv('data/FuelConsumption.csv')\n\n# displaying dataframe\ndf_raw.head()\n\n# show dataframe description\ndf_raw.describe()\n\n# take some important feature\ndf = df_raw[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]\ndf.head()\n\n# checking dataframe\n# df.info()\ndf.isnull().sum()\n\n# separate between feature and target\nX = df.iloc[:,:-1]\nY = df.iloc[:,-1]\n\n# splitting data into data train and test\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3 ,random_state = 42)\n\n# making Linear model\nLinearReg = LinearRegression()\nLinearReg.fit(x_train,y_train)\n\n# predict the data\nypred = LinearReg.predict(x_test)\n\n# calculate the mae, mse, R2\nLinearRegression_MAE = mae(y_test,ypred)\nLinearRegression_MSE = mse(y_test,ypred)\nLinearRegression_R2 = r2_score(y_test,ypred)\n\nReport = [[LinearRegression_MAE,LinearRegression_MSE,LinearRegression_R2]] \npd.DataFrame(Report , columns=['MAE','MSE','R2'], index = [\"Linear Regression\"])\n\nHands On Coding Linear Regression 2\n\n# Impor library yang diperlukan\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Data contoh\nX = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Variabel independen (misalnya, waktu)\ny = np.array([2, 4, 5, 4, 5])  # Variabel dependen (misalnya, hasil pengukuran)\n\n# Membuat model regresi linear\nmodel = LinearRegression()\n\n# Melatih model dengan data\nmodel.fit(X, y)\n\n# Membuat prediksi dengan model\ny_pred = model.predict(X)\n\n# Plot data asli dan garis regresi\nplt.scatter(X, y, label='Data Asli')\nplt.plot(X, y_pred, color='red', linewidth=2, label='Garis Regresi')\nplt.xlabel('Variabel Independen')\nplt.ylabel('Variabel Dependen')\nplt.legend()\nplt.show()\n\nHands On Coding Logistic Regression\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import accuracy_score as asc\nfrom sklearn.metrics import jaccard_score as js\nfrom sklearn.metrics import f1_score as f1s\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\n\n# Logistic Regression\ndf_raw = pd.read_csv('data/Weather_data.csv')\n\n# print(df_raw.head(10))\n\n# Separate the unique value to new column\ndf = pd.get_dummies(data=df_raw, columns=['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])\n\n# Replace No and Yes to 0 and 1\ndf.replace(['No', 'Yes'], [0,1], inplace=True)\n\n# Drop Date Column\ndf.drop('Date', axis = 1 , inplace = True)\n\n# Separate dataframe to Feature and Target\nX = df.drop(columns = 'RainTomorrow',axis = 1)\nY = df['RainTomorrow']\n\n# Split data to data train and test\nx_train , x_test , y_train,y_test = train_test_split(X,Y, test_size = 0.2 , random_state = 42)\n\n# make Logistic Model\nLR = LogisticRegression(solver = 'liblinear')\nLR.fit(x_train,y_train)\n\n# predict data using model\nypred = LR.predict(x_test)\npredict_proba = LR.predict_proba(x_test)\n\n# Calculate the accuracy , jaccard, F1 , log loss and confusion matrix\nLR_Accuracy_Score = asc(y_test,ypred)\nLR_JaccardIndex = js(y_test,ypred)\nLR_F1_Score = f1s(y_test,ypred)\nLR_Log_Loss = log_loss(y_test,predict_proba)\nLR_Confusion_Matrix = confusion_matrix(y_test,ypred)\n\nReport = [[LR_Accuracy_Score,LR_JaccardIndex,LR_F1_Score,LR_Log_Loss,LR_Confusion_Matrix]] \npd.DataFrame(Report , columns=['Accuracy','Jaccard','F1','Log Loss','CM'], index = [\"Linear Regression\"])\nHands On Coding Logistic Regression 2\n\n# Impor library yang diperlukan\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Memuat dataset Iris\niris = load_iris()\nX = iris.data[:, 2:3]  # Menggunakan satu fitur (panjang kelopak bunga)\ny = (iris.target == 2).astype(int)  # Menggunakan kelas \"versicolor\" sebagai kelas positif (1), yang lain sebagai negatif (0)\n\n# Membuat model regresi logistik\nmodel = LogisticRegression()\n\n# Melatih model dengan data\nmodel.fit(X, y)\n\n# Plot data asli\nplt.scatter(X, y, c='b', marker='o', label='Data Asli')\n\n# Membuat kurva regresi logistik\nX_test = np.linspace(1, 7, 300).reshape(-1, 1)\ny_prob = model.predict_proba(X_test)[:, 1]  # Probabilitas kelas 1\nplt.plot(X_test, y_prob, color='green', linewidth=2, label='Kurva Regresi Logistik')\n\nplt.xlabel('Panjang Kelopak Bunga (cm)')\nplt.ylabel('Probabilitas Kelas Positif')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "project_regresi.html#deployment",
    "href": "project_regresi.html#deployment",
    "title": "Studi Kasus 2",
    "section": "Deployment",
    "text": "Deployment\n\nimport streamlit as st\nimport pandas as pd\nimport sqlite3\nimport pickle\nimport time\nimport sklearn\nfrom sklearn.ensemble import RandomForestClassifier\n\n# load file with pickle and read binaruy \nhotelFacilities = pickle.load(open('hotelFacilities.pkl','rb'))\nroomFacilities = pickle.load(open('roomFacilities.pkl','rb'))\nnearestPoint = pickle.load(open('pointInterests.pkl','rb'))\ncolOri = pickle.load(open('col.pkl','rb'))\n\n# load model from pikcle \nxgbModel = pickle.load(open('xgbModel.pkl','rb'))\nsvrModel = pickle.load(open('svmModel.pkl','rb'))\nrfModel = pickle.load(open('RFModel.pkl','rb'))\n\n\n\n# mengatur konfigurasi halaman\nst.set_page_config(\n    page_title = \"Estimasi Hotel Yogyakarta\",\n    page_icon = ':hotel' #nama emoji\n)\n\n# st.write dapat digunakan menampilkan test,dataframe,visualisasi\nst.title('Yogyakarta Hotel Price Estimation')\nst.write('for mor ingo about blablabal')\n\n# st.sidebar dapat digunakan untuk membuat sidebar\nst.sidebar.header(\"User Input Features\")\n\n# Input User untuk memasukkan elemen input use dalam sidebar dilakukan\n# dengan st.sidebar.slider,st.sidebar.selectbox dll\n\ndef user_input_features():\n    starRating = st.sidebar.slider('Star Rating',0,5,3) #(label,minvalues,maxvalues,initial values)\n    builtYear = st.sidebar.slider('Built Year',1900,2023,1960)\n    size = st.sidebar.slider(\"Room Size (m2)\",2.0,100.0,50.0,0.1,format='%0.1f')#(label,minvalues,maxvalues,initial values,jarak increment,format spesifik)\n    occupancy = st.sidebar.slider('Occupancy',1,5,3)\n    childAge = st.sidebar.slider('Child Age',0,18,9)\n    childOccupancy = st.sidebar.slider('Child Occupancy',0,5,2)\n    breakfast = st.sidebar.checkbox('Breakfast Include')\n    wifi = st.sidebar.checkbox(\"Wifi Include\")\n    refund = st.sidebar.checkbox('Refund')\n    livingRoom = st.sidebar.checkbox('Living Room')\n    hotelFacilitie = st.sidebar.multiselect('Hotel Facilities',(hotelFacilities))\n    roomFacilitie = st.sidebar.multiselect('Room Faciclities', (roomFacilities))\n    pointInterest = st.sidebar.multiselect('Point of Interest',(nearestPoint))\n\n    # handle checkbox\n    breakfast = 1 if breakfast else 0\n    wifi = 1 if wifi else 0\n    refund = 1 if refund else 0\n    livingRoom = 1 if livingRoom else 0\n\n    # handle MultiSelect\n    hotelFacilitie = ','.join(hotelFacilitie)\n    roomFacilitie = ','.join(roomFacilitie)\n    pointInterest = ','.join(pointInterest)\n\n    data = {'starRating': starRating,\n            'builtYear': builtYear,\n            'size': size,\n            'baseOccupancy': occupancy,\n            'maxChildAge': childAge,\n            'maxChildOccupancy': childOccupancy,\n            'isBreakfastIncluded': breakfast,\n            'isWifiIncluded': wifi,\n            'isRefundable': refund,\n            'hasLivingRoom': livingRoom,\n            'hotelFacilities': hotelFacilitie,\n            'roomFacilities': roomFacilitie,\n            'nearestPoint': pointInterest\n            }\n    \n    features = pd.DataFrame(data,index=[0])\n    return features\n\n\n\ndf = user_input_features()\nst.header(\"User Input Features\")\nst.write(df)\n# handling input user\n# buat fungsi untuk membuat dataframe dengan nilai 0 dan 1\n\ndef create_df(dfOri,df_name,df,prefix):\n    value = prefix+dfOri[df_name][0]\n    for i in range(0,len(df.columns)):\n        column_name = df.columns[i]\n        if column_name in value:\n            df.loc[0,column_name] = 1\n        else:\n            df.loc[0,column_name] = 0\n\n    return df\n\n# buat dataframe kosong untuk hotelfacilities,roomFacilitis,nearestPoint \n# dengan nama kolom dari hotelFacilities, roomFacilities, nearestPoint\nroomFacilities_df = pd.DataFrame(columns=roomFacilities)\nhotelFacilities_df = pd.DataFrame(columns=hotelFacilities)\nnearestPoint_df = pd.DataFrame(columns=nearestPoint)\n\ncreate_df(df,'roomFacilities',roomFacilities_df, 'Room_')\ncreate_df(df,'hotelFacilities',hotelFacilities_df,'Hotel_')\ncreate_df(df,'nearestPoint',nearestPoint_df,'Point_')\n\n\n# menghapus kolom hotelFacilities,room facilities,nearestpoint\n# lalu gantikan dengan df yang values roomfasilities, hotelfacilities dan nearest point sudah diganti 0 dan 1\n# lalu gabungkan \ndf = df.drop(['hotelFacilities','roomFacilities','nearestPoint'],axis = 1)\ndf = pd.concat([df,hotelFacilities_df,roomFacilities_df,nearestPoint_df],axis=1)\n\n# change all column data type to unit8 kecuali kolom pertama\ndf= df.astype({col: 'float64' for col in df.columns[:2]})\ndf= df.astype({col: 'uint8' for col in df.columns[2:]})\n\n# mengecek dataframe\n# apakah dataframe sesuai yang digunakan saat traning\n# check df columgn order with model column order using colOri , \n# jika tidak sama print kolom yang salah\n# colOri merupakan kolom pada data training yang di export menggunakan pickle\n\ncolOri = colOri[1:]\nif df.columns.tolist() == colOri.all():\n    st.info(\"Column order is correct.\")\nelse:\n    mismatched_columns = [(idx, df_col, model_col) for idx, (df_col, model_col) in enumerate(zip(df.columns.tolist(), colOri)) if df_col != model_col]\n\n    if len(mismatched_columns) &gt; 0:\n        st.warning(\"The order of the columns is not the same as the model. Mismatched columns:\")\n        for idx, df_col, model_col in mismatched_columns:\n            st.write(f\"At index {idx}: DataFrame column '{df_col}' - Model column '{model_col}'\")\n\n\n\n\nst.write('press button below to predict : ')\nmodel = st.selectbox('Select Model',('XGBoost','Random Forest','SVR'))\n\nif model == 'XGBoost' and st.button('Predict'):\n    # create progres bar widget with initial progress is 0%\n    bar = st.progress(0)\n    # create an empty container or space\n    status_text = st.empty()\n    for i in range(1,101):\n        # create a text to showing a percentage process\n        status_text.text(\"%i%% complete\" %i)\n        # give bar progress values\n        bar.progress(i)\n        # give bar progress time to execute the values\n        time.sleep(0.01)\n\n    #formatting the prediction\n    prediction = xgbModel.predict(df)\n    # \"{:\": This is the start of the format specifier.\n    # \",\": This specifies that a comma should be used as a thousands separator. In many countries, a comma is used to separate thousands in large numbers, making them easier to read.\n    # \"2f\": This specifies how to format the floating-point number. In this case, it's using 2 decimal places (i.e., it will show two digits after the decimal point).\n    formatString = \"Rp{:,.2f}\"\n    #  change the format of prediction variable to float\n    prediction = float(prediction[0])\n    formatted_prediction = formatString.format(prediction)\n    time.sleep(0.08)\n\n    # print the prediction\n    st.subheader('Prediction')\n    st.metric('Price (IDR)',formatted_prediction)\n\n\nelif model == 'Random Forest' and st.button('Predict'):\n    bar = st.progress(0)\n    status_text = st.empty()\n    for i in range(1, 101):\n        status_text.text(\"%i%% Complete\" % i)\n        bar.progress(i)\n        time.sleep(0.01)\n\n    # Formatting the prediction\n    prediction = rfModel.predict(df)\n    formaString = \"Rp{:,.2f}\"\n    prediction = float(prediction[0])\n    formatted_prediction = formaString.format(prediction)\n    time.sleep(0.08)\n\n    # print the prediction\n    st.subheader('Prediction')\n    st.metric('Price (IDR)', formatted_prediction)\n\n    # empty the progress bar and status text\n    time.sleep(0.08)\n    bar.empty()\n    status_text.empty()\n\nelif model == 'SVR' and st.button('Predict'):\n    bar = st.progress(0)\n    status_text = st.empty()\n    for i in range(1, 101):\n        status_text.text(\"%i%% Complete\" % i)\n        bar.progress(i)\n        time.sleep(0.01)\n\n    # Formatting the prediction\n    prediction = svrModel.predict(df)\n    \n    formaString = \"Rp{:,.2f}\"\n    prediction = float(prediction[0])\n    formatted_prediction = formaString.format(prediction)\n    # prediction = rfModel.predict(df)\n    time.sleep(0.08)\n\n    # print the prediction\n    st.subheader('Prediction')\n    st.metric('Price (IDR)', formatted_prediction)\n\n    # empty the progress bar and status text\n    time.sleep(0.08)\n    bar.empty()\n    status_text.empty()\n        \n\nHasil akhir\nUnutuk contoh hasil akhir dapat dilihat pada link berikut: Sreamlit hotel Yogyakarta"
  }
]